
@book{hundepool_handbook_2024,
	edition = {2nd Edition},
	title = {Handbook on {Statistical} {Disclosure} {Control}},
	shorttitle = {Handbook {SDC}},
	url = {https://sdctools.github.io/HandbookSDC/},
	language = {en},
	publisher = {ESSNet SDC},
	author = {Hundepool, Anco and Domingo-Ferre, Josep and Franconi, Luisa and Giessing, Sarah and Lenz, Rainer and Longhurst, Jane and Schulte Nordholt, Eric and Seri, Giovanni and De Wolf, Peter-Paul and Tent, Reinhard and Młodak, Andrzej and Gussenbauer, Johannes and Wilak, Kamil},
	month = nov,
	year = {2024},
	keywords = {SDC - general views},
}

@inproceedings{fraser_proposed_2005,
	title = {A {Proposed} {Method} for {Confidentialising} {Tabular} {Output} to {Protect} against {Differencing}},
	url = {https://d1wqtxts1xzle7.cloudfront.net/15820760/10.1.1.107.259-libre.pdf?1391811225=&response-content-disposition=inline%3B+filename%3DMONA_Microdata_ON_Line_access_at_Statist.pdf&Expires=1689600265&Signature=NnTLlsbC9cp8p-VgiaRT4MDqmTKS8qejrC1KaNWNmDRV5VJRQJ2PFMIePAFfNzwGZYWxsDhz68VOgQ~GwKzeGd3px0DPU-blOhRIaf0sAPRVSlQ4zyFXyWMN6pwg8FLi4Jeq6vds4kL0GbvaOv9cPg09kELRjyaOiin1ekzdAco4YV-HLzZbGXhvEkMnYgdhBvQg1ScK~W7zW-rsxrY8DkENrj3O3ACZMgs0p-ipidRNob2SwCzPTHXl-ZTsCjwhyW19-rnGJp0wfFiqLGPwN4i7jkYNO9nx-mKvrVlSLe~qqSjW-CZ4nI0epMcYj~LLOOLdotEoZjHNYGcJ3n27BA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA#page=285},
	abstract = {The differencing problem puts increased demands on a system of tabular confidentiality. Methods currently
in use at the Australian Bureau of Statistics and many other national statistical offices only target small cell values for
treatment, and allow large cells values to be released without any perturbation to protect confidentiality. Such methods
are vulnerable to differencing attacks which can derive unprotected small cell values as the difference of two unprotected
large cell values. This paper proposes a cell perturbation method for confidentialising Australian population Census tables
to protect against differencing attacks and any other attempts at identification. The method is a two stage process. At the
first stage a perturbation is added to all cells of all tables, including the independent perturbation of table marginals. The
perturbation is set to zero for a pre-determined set of key output (e.g. age by sex population counts). This perturbation
process produces a non-additive protected table. At the second stage additivity is restored. Record keys are assigned to
the microdata and are used to produce consistent perturbations at the first stage of the process, although consistency is
lost when additivity is restored.},
	language = {en},
	booktitle = {Monographs of {Official} {Statistics}: {Work} {Session} on {Statistical} {Data} {Confidentiality}},
	author = {Fraser, B. and Wooton, J.},
	year = {2005},
	keywords = {CKM, SDC - frequency tables, SDC - tools, diclosure control},
	pages = {299--302},
}

@misc{noauthor_handbook_nodate,
	title = {Handbook on {Statistical} {Disclosure} {Control}},
	url = {https://sdctools.github.io/HandbookSDC/},
	urldate = {2025-01-22},
}

@article{salazar-gonzalez_controlled_2005,
	title = {The {Controlled} {Rounding} {Implementation}},
	url = {https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2005/wp.36.e.pdf},
	abstract = {Rounding methods are common techniques in many statistical oﬃces to protect disclosive information when publishing data in tabular form. Classical versions of these methods do not consider protection levels while searching patterns with minimum information loss, and therefore typically the so-called auditing phase is required to check the protection of the proposed patterns. This paper presents a mathematical model for the whole problem of ﬁnding a protected pattern with minimum loss of information, and describe an algorithm to solve it. The base scheme is a branch-and-bound search. If time enough is allowed, the algorithm stops with an optimal solution. Otherwise, an heuristic approach aims at ﬁnding a feasible zero-restricted solution. On complicated or infeasible tables where ﬁnding a zero-restricted feasible solution cannot be found in a reasonable time, the algorithm generates a non zero-restricted solution, here referred to as rapid table. This paper presents a summary of ﬁndings from some computational experiments.},
	language = {en},
	journal = {Joint UNECE/Eurostat work session on statistical data confidentiality},
	author = {Salazar-Gonzalez, Juan-Jose and Bycroft, Christine and Staggemeier, Andrea Toniolo},
	month = nov,
	year = {2005},
}

@article{lamarche_fideli_2021,
	title = {Fidéli, l’intégration des sources fiscales dans les données sociales},
	issn = {2107-0903},
	url = {https://www.insee.fr/fr/information/5398683?sommaire=5398695},
	language = {FR},
	number = {6},
	journal = {Courrier des Statistiques},
	author = {Lamarche, Pierre and Lollivier, Stéfan},
	year = {2021},
	pages = {28--46},
}

@article{noauthor_notitle_nodate,
}

@misc{noauthor_fideli_nodate,
	title = {Fidéli, l’intégration des sources fiscales dans les données sociales − {Courrier} des statistiques {N6} - 2021 {\textbar} {Insee}},
	url = {https://www.insee.fr/fr/information/5398683?sommaire=5398695},
	urldate = {2024-12-20},
}

@article{nissim_is_2018,
	title = {Is privacy \textit{privacy} ?},
	volume = {376},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0358},
	doi = {10.1098/rsta.2017.0358},
	abstract = {This position paper observes how different technical and normative conceptions of privacy have evolved in parallel and describes the practical challenges that these divergent approaches pose. Notably, past technologies relied on intuitive, heuristic understandings of privacy that have since been shown not to satisfy expectations for privacy protection. With computations ubiquitously integrated in almost every aspect of our lives, it is increasingly important to ensure that privacy technologies provide protection that is in line with relevant social norms and normative expectations. Similarly, it is also important to examine social norms and normative expectations with respect to the evolving scientific study of privacy. To this end, we argue for a rigorous analysis of the mapping from normative to technical concepts of privacy and vice versa. We review the landscape of normative and technical definitions of privacy and discuss specific examples of gaps between definitions that are relevant in the context of privacy in statistical computation. We then identify opportunities for overcoming their differences in the design of new approaches to protecting privacy in accordance with both technical and normative standards. 
            This article is part of a discussion meeting issue ‘The growing ubiquity of algorithms in society: implications, impacts and innovations’.},
	language = {en},
	number = {2128},
	urldate = {2024-11-04},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Nissim, Kobbi and Wood, Alexandra},
	month = sep,
	year = {2018},
	keywords = {Differential Privacy},
	pages = {20170358},
}

@misc{noauthor_gda_nodate,
	title = {{GDA} {Score} – {General} {Data} {Anonymity}},
	url = {https://www.gda-score.org/},
	language = {en-US},
	urldate = {2024-10-18},
	keywords = {risk-utility trade-off},
}

@book{cowan_hands-differential_2024,
	address = {Beijing Boston Farnham Sebastopol Tokyo},
	edition = {First edition},
	title = {Hands-on differential privacy: introduction to the theory and practice using {OpenDP}},
	isbn = {978-1-4920-9774-7},
	shorttitle = {Hands-on differential privacy},
	abstract = {Many organizations today analyze and share large, sensitive datasets about individuals. Whether these datasets cover healthcare details, financial records, or exam scores, it's become more difficult for organizations to protect an individual's information through deidentification, anonymization, and other traditional statistical disclosure limitation techniques. This practical book explains how differential privacy (DP) can help. Authors Ethan Cowan, Michael Shoemate, and Mayana Pereira and explain how these techniques enable data scientists, researchers, and programmers to run statistical analyses that hide the contribution of any single individual. You'll dive into basic DP concepts and understand how to use open source tools to create differentially private statistics, explore how to assess the utility/privacy trade-offs, and learn how to integrate differential privacy into workflows. With this book, you'll learn: How DP guarantees privacy when other data anonymization methods don't What preserving individual privacy in a dataset entails How to apply DP in several real-world scenarios and datasets Potential privacy attack methods, including what it means to perform a reidentification attack How to use the OpenDP library in privacy-preserving data releases How to interpret guarantees provided by specific DP data releases},
	language = {eng},
	publisher = {O'Reilly},
	author = {Cowan, Ethan and Shoemate, Michael and Pereira, Mayana},
	year = {2024},
}

@misc{noauthor_proton_2024,
	title = {Proton {Mail}},
	url = {https://mail.proton.me/},
	abstract = {Proton Mail is based in Switzerland and uses advanced encryption to keep your data safe. Apps available for Android, iOS, and desktop devices.},
	language = {fr-FR},
	urldate = {2024-10-18},
	month = oct,
	year = {2024},
}

@incollection{domingo-ferrer_secondary_2024,
	address = {Cham},
	title = {Secondary {Cell} {Suppression} by {Gaussian} {Elimination}: {An} {Algorithm} {Suitable} for {Handling} {Issues} with {Zeros} and {Singletons}},
	volume = {14915},
	isbn = {978-3-031-69650-3 978-3-031-69651-0},
	shorttitle = {Secondary {Cell} {Suppression} by {Gaussian} {Elimination}},
	url = {https://link.springer.com/10.1007/978-3-031-69651-0_6},
	abstract = {To protect tabular data through cell suppression, efficient algorithms are essential. Gaussian elimination can be used for secondary cell suppression to prevent exact disclosure. A beneficial feature of this method is that all tables created from the same microdata can be handled simultaneously. This paper presents a solution to the issue where suppressed zeros in frequency tables cannot protect each other. In magnitude tables, it outlines how the algorithm can be tailored to provide protection against singleton contributors using their own data for disclosure.},
	language = {en},
	urldate = {2024-10-14},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer Nature Switzerland},
	author = {Langsrud, Øyvind},
	editor = {Domingo-Ferrer, Josep and Önen, Melek},
	year = {2024},
	doi = {10.1007/978-3-031-69651-0_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {87--101},
}

@inproceedings{green_statbarn_2024,
	address = {Cham},
	title = {The statbarn: {A} {New} {Model} for {Output} {Statistical} {Disclosure} {Control}},
	isbn = {978-3-031-69651-0},
	shorttitle = {The statbarn},
	doi = {10.1007/978-3-031-69651-0_19},
	abstract = {A major success for research this century has been the growth of secure facilities allowing research access to detailed sensitive personal data. This has also raised awareness of the problem of output disclosure risk, where statistics may inadvertently breach the confidentiality of data subjects, a risk that grows with the detail in the data.},
	language = {en},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer Nature Switzerland},
	author = {Green, Elizabeth and Ritche, Felix and White, Paul},
	editor = {Domingo-Ferrer, Josep and Önen, Melek},
	year = {2024},
	pages = {284--293},
}

@article{ritchie_training_nodate,
	title = {Training research output checkers},
	abstract = {National statistics institutes (NSIs) and other organisations increasingly allow users access to confidential microdata within strictly controlled environments. These are then checked for disclosure risk on leaving the environment by the researchers and/or by NSI staff.},
	language = {en},
	author = {Ritchie, Felix},
}

@article{ritchie_sacro_2023,
	title = {The {SACRO} guide to statistical output checking},
	url = {https://zenodo.org/records/10054629},
	abstract = {This document was developed as an output of the project SACRO (Semi-Automatic checking of Research Outputs). This work is funded by UK research and Innovation [Grant Number MC\_PC\_23006] as part of Phase 1 of the DARE UK (Data and Analytics Research Environments UK) programme, delivered in partnership with Health Data Research UK (HDR UK) and Administrative Data Research UK (ADR UK).
This document has three sections, plus references:
Part I discusses the operational and statistical theory underlying all aspects of output statistical disclosure control (OSDC). It is intended for those who want a deeper understanding of the concepts underlying practical and efficient OSDC.
Part II is the practical manual, intended to be a reference for users and output checkers. It lists the rules (or rules-of-thumb) to be followed for all possible outputs, organised by class. For each class of statistics, the manual presents a quick summary, an example, a discussion of the key risks, a rationale for any rules applied, and guidelines on how to evaluate the output if an exception is requested. A detailed statistical explanation of the risk assessment is not given, but other works are referenced for those interested. A separate chapter discusses graphs.
Part III provides FAQs on both statistics and outputs, plus some guidance for output checkers on how to respond to queries},
	language = {eng},
	urldate = {2024-10-10},
	author = {Ritchie, Felix and Green, Elizabeth and Smith, Jim and Tilbrook, Amy and White, Paul},
	month = oct,
	year = {2023},
	note = {Publisher: Zenodo},
	keywords = {disclosure control, privacy, trusted research environments},
}

@misc{preen_multi-language_2022,
	title = {A multi-language toolkit for supporting automated checking of research outputs},
	url = {https://arxiv.org/abs/2212.02935v2},
	abstract = {This article presents the automatic checking of research outputs package acro, which assists researchers and data governance teams by automatically applying best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. acro distinguishes between: research output that is safe to publish; output that requires further analysis; and output that cannot be published because it creates substantial risk of disclosing private data. This is achieved through the use of a lightweight Python wrapper that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This adds functionality to (i) identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply disclosure mitigation strategies where required; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow. The major analytical programming languages used by researchers are supported: Python, R, and Stata. The acro code and documentation are available under an MIT license at https://github.com/AI-SDC/ACRO},
	language = {en},
	urldate = {2024-10-10},
	journal = {arXiv.org},
	author = {Preen, Richard J. and Albashir, Maha and Davy, Simon and Smith, Jim},
	month = dec,
	year = {2022},
}

@misc{preen_multi-language_2024,
	title = {A multi-language toolkit for supporting automated checking of research outputs},
	url = {http://arxiv.org/abs/2212.02935},
	abstract = {This article presents the automatic checking of research outputs package acro, which assists researchers and data governance teams by automatically applying best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. acro distinguishes between: research output that is safe to publish; output that requires further analysis; and output that cannot be published because it creates substantial risk of disclosing private data. This is achieved through the use of a lightweight Python wrapper that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This adds functionality to (i) identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply disclosure mitigation strategies where required; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow. The major analytical programming languages used by researchers are supported: Python, R, and Stata. The acro code and documentation are available under an MIT license at https://github.com/AI-SDC/ACRO},
	language = {en},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Preen, Richard J. and Albashir, Maha and Davy, Simon and Smith, Jim},
	month = sep,
	year = {2024},
	note = {arXiv:2212.02935 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Information Retrieval, Computer Science - Software Engineering, Statistics - Applications, Statistics - Methodology},
}

@article{preen_multi-language_2022-1,
	title = {A multi-language toolkit for supporting automated checking of research outputs},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2212.02935},
	doi = {10.48550/ARXIV.2212.02935},
	abstract = {This article presents the automatic checking of research outputs package acro, which assists researchers and data governance teams by automatically applying best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. acro distinguishes between: research output that is safe to publish; output that requires further analysis; and output that cannot be published because it creates substantial risk of disclosing private data. This is achieved through the use of a lightweight Python wrapper that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This adds functionality to (i) identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply disclosure mitigation strategies where required; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow. The major analytical programming languages used by researchers are supported: Python, R, and Stata. The acro code and documentation are available under an MIT license at https://github.com/AI-SDC/ACRO},
	urldate = {2024-10-10},
	author = {Preen, Richard J. and Albashir, Maha and Davy, Simon and Smith, Jim},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Applications (stat.AP), Cryptography and Security (cs.CR), FOS: Computer and information sciences, Information Retrieval (cs.IR), Methodology (stat.ME), Software Engineering (cs.SE)},
}

@inproceedings{smith_sacro_2023,
	title = {{SACRO}: {Semi}-{Automated} {Checking} {Of} {Research} {Outputs}},
	shorttitle = {{SACRO}},
	url = {https://www.semanticscholar.org/paper/SACRO%3A-Semi-Automated-Checking-Of-Research-Outputs-Smith-Preen/cb9122dbcd92214f61d12b26874cfb907cb831b2},
	abstract = {Output checking can require significant resources, acting as a barrier to scaling up the research use of confidential data. We report on a project, SACRO, that is developing a general-purpose, semi-automatic output checking systems that works across the range of restricted research environments. SACRO is designed to • Automate checking of most common statistics, using best-practice principles-based modelling.},
	urldate = {2024-10-10},
	author = {Smith, Jim Q. and Preen, R. and Albashir, Maha and Ritchie, F. and Green, Elizabeth and Davy, S. and Stokes, Pete and Bacon, S.},
	year = {2023},
}

@misc{noauthor_httpssecuredatagrouporgwp-contentuploads201910sdc-handbook-v10pdf_nodate,
	title = {https://securedatagroup.org/wp-content/uploads/2019/10/sdc-handbook-v1.0.pdf},
	url = {https://securedatagroup.org/wp-content/uploads/2019/10/sdc-handbook-v1.0.pdf},
	urldate = {2024-10-10},
}

@book{welpton_sdc_2019,
	title = {{SDC} {Handbook}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://figshare.com/articles/SDC_Handbook/9958520},
	abstract = {A number of Safe Settings operate throughout the UK. This includes government agencies such as the Office for National Statistics Secure Research Service, HMRC Datalab, UK Data Service Secure Lab. Charities such as Cancer Research UK and The Health Foundation have their own Safe Settings.{\textless}br{\textgreater}These are secure environments in which confidential data can be analysed. Data are not released, but the statistics generated from analysis are. These statistics are subject to a process known as Statistical Disclosure Control (SDC) which aims to prevent confidential data being passed out of the secure environment.{\textless}br{\textgreater}This Handbook provides guidelines for undertaking SDC.},
	language = {en},
	urldate = {2024-10-10},
	publisher = {figshare},
	author = {Welpton, Richard},
	year = {2019},
	doi = {10.6084/M9.FIGSHARE.9958520},
	note = {Artwork Size: 0 Bytes
Pages: 0 Bytes},
	keywords = {80402 Data Encryption, 80604 Database Management, FOS: Computer and information sciences},
}

@misc{giomi_staticeanonymeter_2024,
	title = {statice/anonymeter},
	url = {https://github.com/statice/anonymeter},
	abstract = {A Unified Framework for Quantifying Privacy Risk in Synthetic Data according to the GDPR},
	urldate = {2024-10-04},
	publisher = {Statice},
	author = {Giomi, Matteo and eicca, Mikhail},
	month = sep,
	year = {2024},
	note = {original-date: 2022-11-17T12:12:36Z},
	keywords = {Synthetic data, gdpr, privacy, risk-utility trade-off, synthetic-data},
}

@book{raab_privacy_2024,
	title = {Privacy risk from synthetic data: practical proposals},
	shorttitle = {Privacy risk from synthetic data},
	abstract = {This paper proposes and compares measures of identity and attribute disclosure risk for synthetic data. Data custodians can use the methods proposed here to inform the decision as to whether to release synthetic versions of confidential data. Different measures are evaluated on two data sets. Insight into the measures is obtained by examining the details of the records identified as posing a disclosure risk. This leads to methods to identify, and possibly exclude, apparently risky records where the identification or attribution would be expected by someone with background knowledge of the data. The methods described are available as part of the {\textbackslash}textbf\{synthpop\} package for {\textbackslash}textbf\{R\}.},
	author = {Raab, Gillian},
	month = sep,
	year = {2024},
	doi = {10.48550/arXiv.2409.04257},
	keywords = {Synthetic data, risk-utility trade-off},
}

@misc{noauthor_httpsuneceorgsitesdefaultfiles2023-08sdc2023_s3_1_switzerland_fondeville_dpdf_nodate,
	title = {https://unece.org/sites/default/files/2023-08/{SDC2023}\_S3\_1\_Switzerland\_Fondeville\_D.pdf},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S3_1_Switzerland_Fondeville_D.pdf},
	urldate = {2024-10-04},
}

@book{united_nations_economic_commission_for_europe_conference_2022,
	title = {Conference of {European} {Statisticians}: {Road} {Map} on {Statistics} for {Sustainable} {Development} {Goals} - {Second} {Edition}},
	isbn = {978-92-1-001079-5},
	shorttitle = {Conference of {European} {Statisticians}},
	url = {https://www.un-ilibrary.org/content/books/9789210010795},
	abstract = {The past few years have seen an explosion of the volume of geo-referenced data, a trend that can be observed in the world of official statistics: large scale imputation, generalizing survey results to the whole population, is made more and more common thanks to the efficiency and the flexibility of new machine learning algorithms. Official agencies are now capable of providing realistic estimates of population characteristics at lower than ever aggregation levels, but communicating survey results at always finer geographical scales strongly increases privacy risks. Thus, in order to maintain trust between populations and their administrations, official statistical offices must ensure highest levels of confidentiality.},
	language = {en},
	urldate = {2024-10-04},
	publisher = {United Nations},
	author = {{United Nations Economic Commission for Europe}},
	month = mar,
	year = {2022},
	doi = {10.18356/9789210010795},
}

@article{dwork_firm_2011,
	title = {A firm foundation for private data analysis},
	volume = {54},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1866739.1866758},
	doi = {10.1145/1866739.1866758},
	abstract = {What does it mean to preserve privacy?},
	language = {en},
	number = {1},
	urldate = {2024-09-02},
	journal = {Communications of the ACM},
	author = {Dwork, Cynthia},
	month = jan,
	year = {2011},
	keywords = {Differential Privacy},
	pages = {86--95},
}

@article{benschop_statistical_nodate,
	title = {Statistical {Disclosure} {Control}: {A} {Practice} {Guide}},
	language = {en},
	author = {Benschop, Thijs and Machingauta, Cathrine and Welch, Matthew},
	keywords = {SDC - general views, SDC - tools},
}

@techreport{de_wolf_public_2015,
	type = {Specific {Grant} {Agreement}},
	title = {Public {Use} {Files} of {EU}-{SILC} and {EU}-{LFS} data},
	url = {https://www.ksh.hu/statszemle_archive/2015/2015_11-12/2015_11-12_1140_2.pdf},
	institution = {Eurostat},
	author = {De Wolf, Peter-Paul},
	year = {2015},
}

@misc{noauthor_programming_nodate,
	title = {Programming {Differential} {Privacy} — {Programming} {Differential} {Privacy}},
	url = {https://programming-dp.com/cover.html},
	urldate = {2024-07-17},
}

@misc{steed_quantifying_2024,
	title = {Quantifying {Privacy} {Risks} of {Public} {Statistics} to {Residents} of {Subsidized} {Housing}},
	url = {http://arxiv.org/abs/2407.04776},
	abstract = {As the U.S. Census Bureau implements its controversial new disclosure avoidance system, researchers and policymakers debate the necessity of new privacy protections for public statistics. With experiments on both published statistics and synthetic data, we explore a particular privacy concern: respondents in subsidized housing may deliberately not mention unauthorized children and other household members for fear of being evicted. By combining public statistics from the Decennial Census and the Department of Housing and Urban Development, we demonstrate a simple, inexpensive reconstruction attack that could identify subsidized households living in violation of occupancy guidelines in 2010. Experiments on synthetic data suggest that a random swapping mechanism similar to the Census Bureau’s 2010 disclosure avoidance measures does not significantly reduce the precision of this attack, while a differentially private mechanism similar to the 2020 disclosure avoidance system does. Our results provide a valuable example for policymakers seeking a trustworthy, accurate census.},
	language = {en},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Steed, Ryan and Qing, Diana and Wu, Zhiwei Steven},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04776 [cs]},
	keywords = {Computer Science - Computers and Society, Differential Privacy, Re-identification risk, SDC - attack scenario, US Census},
}

@misc{xu_modeling_2019,
	title = {Modeling {Tabular} data using {Conditional} {GAN}},
	url = {http://arxiv.org/abs/1907.00503},
	doi = {10.48550/arXiv.1907.00503},
	abstract = {Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design TGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. TGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
	month = oct,
	year = {2019},
	note = {arXiv:1907.00503 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dalenius_privacy_1977,
	title = {Privacy transformations for statistical information systems},
	volume = {1},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/0378375877900076},
	doi = {10.1016/0378-3758(77)90007-6},
	number = {1},
	urldate = {2024-06-26},
	journal = {Journal of Statistical Planning and Inference},
	author = {Dalenius, Tore},
	month = feb,
	year = {1977},
	pages = {73--86},
}

@article{dwork_difficulties_2010,
	title = {On the {Difficulties} of {Disclosure} {Prevention} in {Statistical} {Databases} or {The} {Case} for {Differential} {Privacy}},
	volume = {2},
	copyright = {http://creativecommons.org/licenses/by-sa/4.0},
	issn = {2575-8527},
	url = {https://journalprivacyconfidentiality.org/index.php/jpc/article/view/585},
	doi = {10.29012/jpc.v2i1.585},
	abstract = {In 1977 Tore Dalenius articulated a desideratum for statistical databases: nothing about

an individual should be learnable from the database that cannot be learned without access to the

database. We give a general impossibility result showing that a natural formalization of Dalenius’

goal cannot be achieved if the database is useful. The key obstacle is the side information that

may be available to an adversary. Our results hold under very general conditions regarding the

database, the notion of privacy violation, and the notion of utility.

Contrary to intuition, a variant of the result threatens the privacy even of someone not in

the database. This state of affairs motivated the notion of differential privacy [15, 16], a strong

ad omnia privacy which, intuitively, captures the increased risk to one’s privacy incurred by

participating in a database.},
	number = {1},
	urldate = {2024-06-20},
	journal = {Journal of Privacy and Confidentiality},
	author = {Dwork, Cynthia and Naor, Moni},
	month = sep,
	year = {2010},
	keywords = {Differential Privacy, SDC - general views, risk-utility trade-off},
}

@article{gussenbauer_simulation_2024,
	title = {Simulation of {Calibrated} {Complex} {Synthetic} {Population} {Data} with {XGBoost}},
	volume = {17},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/17/6/249},
	doi = {10.3390/a17060249},
	abstract = {Syntheticdata generation methods are used to transform the original data into privacy-compliant synthetic copies (twin data). With our proposed approach, synthetic data can be simulated in the same size as the input data or in any size, and in the case of finite populations, even the entire population can be simulated. The proposed XGBoost-based method is compared with known model-based approaches to generate synthetic data using a complex survey data set. The XGBoost method shows strong performance, especially with synthetic categorical variables, and outperforms other tested methods. Furthermore, the structure and relationship between variables are well preserved. The tuning of the parameters is performed automatically by a modified k-fold cross-validation. If exact population margins are known, e.g., cross-tabulated population counts on age class, gender and region, the synthetic data must be calibrated to those known population margins. For this purpose, we have implemented a simulated annealing algorithm that is able to use multiple population margins simultaneously to post-calibrate a synthetic population. The algorithm is, thus, able to calibrate simulated population data containing cluster and individual information, e.g., about persons in households, at both person and household level. Furthermore, the algorithm is efficiently implemented so that the adjustment of populations with many millions or more persons is possible.},
	language = {en},
	number = {6},
	urldate = {2024-06-20},
	journal = {Algorithms},
	author = {Gussenbauer, Johannes and Templ, Matthias and Fritzmann, Siro and Kowarik, Alexander},
	month = jun,
	year = {2024},
	pages = {249},
}

@inproceedings{giesing_concepts_2019,
	address = {the Hague},
	title = {Concepts for generalising tools implementing the cell key method to the case of continuous variables},
	url = {https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2019/mtg1/SDC2019_S2_Germany_Giessing_Tent_AD.pdf},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Gießing, Sarah and Tent, Reinhard},
	month = oct,
	year = {2019},
	keywords = {CKM, SDC - frequency tables, SDC - tools, risk-utility trade-off},
}

@incollection{domingo-ferrer_when_2022,
	address = {Cham},
	title = {When {Machine} {Learning} {Models} {Leak}: {An} {Exploration} of {Synthetic} {Training} {Data}},
	volume = {13463},
	isbn = {978-3-031-13944-4 978-3-031-13945-1},
	shorttitle = {When {Machine} {Learning} {Models} {Leak}},
	url = {https://link.springer.com/10.1007/978-3-031-13945-1_20},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Slokom, Manel and de Wolf, Peter-Paul and Larson, Martha},
	editor = {Domingo-Ferrer, Josep and Laurent, Maryline},
	year = {2022},
	doi = {10.1007/978-3-031-13945-1_20},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {SDC - attack scenario, Synthetic data},
	pages = {283--296},
}

@article{abowd_confidentiality_2023,
	title = {Confidentiality {Protection} in the 2020 {US} {Census} of {Population} and {Housing}},
	volume = {10},
	url = {https://discovery.researcher.life/article/confidentiality-protection-in-the-2020-us-census-of-population-and-housing/8002f491676b31c0949518aa83296071},
	doi = {10.1146/annurev-statistics-010422-034226},
	abstract = {In an era where external data and computational capabilities far exceed statistical agencies’ own resources and capabilities, they face the renewed challenge of protecting the confidentiality of underlying microdata when publishing statistics in very granular form and ensuring that these granular data are used for statistical purposes only. Conventional statistical disclosure limitation methods are too fragile to address this new challenge. This article discusses the deployment of a differential privacy framework for the 2020 US Census that was customized to protect confidentiality, particularly the most detailed geographic and demographic categories, and deliver controlled accuracy across the full geographic hierarchy.},
	language = {en},
	urldate = {2023-03-17},
	journal = {Annual Review of Statistics and Its Application},
	author = {Abowd, John M. and Hawes, Michael B.},
	month = mar,
	year = {2023},
	keywords = {Differential Privacy, US Census},
}

@misc{swiss_federal_statistical_office_dscc-admin-chlomas_2024,
	title = {dscc-admin-ch/lomas},
	copyright = {MIT},
	url = {https://github.com/dscc-admin-ch/lomas},
	urldate = {2024-06-19},
	publisher = {dscc-admin-ch},
	author = {Swiss Federal Statistical Office},
	month = jun,
	year = {2024},
	note = {original-date: 2024-03-27T09:37:03Z},
	keywords = {Differential Privacy, SDC - tools},
}

@misc{swiss_federal_statistical_office_swiss_2024,
	title = {The {Swiss} {FSO} innovates with {OpenDP} to protect citizen privacy},
	url = {https://www.bfs.admin.ch/bfs/en/home/dscc/blog/2024-02-opendp.html},
	abstract = {While the Swiss Federal Statistical Office (FSO) recognizes the potential of data, concerns for citizen privacy limit its potential. Through the collaboration between its Data Science Competence Center (DSCC) and the OpenDP project, solutions are being developed to protect privacy while still ensuring utility.},
	language = {en},
	urldate = {2024-06-19},
	author = {Swiss Federal Statistical Office},
	month = jun,
	year = {2024},
	keywords = {Differential Privacy, SDC - tools},
}

@article{langsrud_information_2019,
	title = {Information preserving regression-based tools for statistical disclosure control},
	volume = {29},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-018-9848-9},
	doi = {10.1007/s11222-018-9848-9},
	language = {en},
	number = {5},
	urldate = {2024-06-17},
	journal = {Statistics and Computing},
	author = {Langsrud, Øyvind},
	month = sep,
	year = {2019},
	pages = {965--976},
}

@inproceedings{chen_xgboost_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2024-06-11},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	keywords = {large-scale machine learning},
	pages = {785--794},
}

@book{duncan_statistical_2011,
	address = {New York Heidelberg},
	series = {Statistics for social and behavioral sciences},
	title = {Statistical confidentiality: principles and practice},
	isbn = {978-1-4419-7801-1 978-1-4614-2837-4},
	shorttitle = {Statistical confidentiality},
	language = {eng},
	publisher = {Springer},
	author = {Duncan, George T. and Elliot, Mark and Salazar-González, Juan-José},
	year = {2011},
	keywords = {SDC - general views},
}

@inproceedings{branchu_donnees_2018,
	address = {Paris},
	title = {Données carroyées et confidentialité},
	url = {http://www.jms-insee.fr/2018/S23_3_ACTE_BRANCHU_JMS2018.pdf},
	language = {fr},
	booktitle = {{JMS}},
	author = {Branchu, Marc and Costemalle, Vianney and Fontaine, Maëlle},
	year = {2018},
}

@phdthesis{hunter_efficient_1978,
	title = {Efficient computation and data structures for graphics.},
	url = {https://www.proquest.com/openview/c8ce862e3dff9df650361e767ccc0274/1?pq-origsite=gscholar&cbl=18750&diss=y},
	language = {en},
	urldate = {2024-03-20},
	school = {Princeton University},
	author = {Hunter, Gregory Michael},
	year = {1978},
}

@inproceedings{samet_spatial_1995,
	title = {Spatial {Data} {Structures}},
	url = {https://www.semanticscholar.org/paper/Spatial-Data-Structures-Samet/145f299c1a7954f836b93822fab63bbeb271cdd9},
	abstract = {An overview is presented of the use of spatial data structures in spatial databases. The focus is on hierarchical data structures, including a number of variants of quadtrees, which sort the data with respect to the space occupied by it. Such techniques are known as spatial indexing methods. Hierarchical data structures are based on the principle of recursive decomposition. They are attractive because they are compact and depending on the nature of the data they save space as well as time and also facilitate operations such as search. Examples are given of the use of these data structures in the representation of di erent data types such as regions, points, rectangles, lines, and volumes.},
	urldate = {2024-03-20},
	author = {Samet, H.},
	month = jan,
	year = {1995},
}

@article{samet_quadtree_1984,
	title = {The {Quadtree} and {Related} {Hierarchical} {Data} {Structures}},
	volume = {16},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/356924.356930},
	doi = {10.1145/356924.356930},
	number = {2},
	urldate = {2024-03-20},
	journal = {ACM Computing Surveys},
	author = {Samet, Hanan},
	year = {1984},
	pages = {187--260},
}

@inproceedings{sune_statistical_2017,
	address = {Bruxelles},
	title = {Statistical disclosure control on visualising geocoded population data using quadtrees},
	url = {https://www.idescat.cat/serveis/biblioteca/docs/bib/pec/paae2017/b3713abstract.pdf},
	language = {en},
	booktitle = {{NTTS} 2017},
	author = {Suñé, Eduard and Ibáñez, Daniel},
	year = {2017},
}

@article{lagonigro_quadtree_nodate,
	title = {A quadtree approach based on {European} geographic grids: reconciling data privacy and accuracy},
	abstract = {Methods to preserve conﬁdentiality when publishing geographic information conﬂict with the need to publish accurate data. The goal of this paper is to create a European geographic grid framework to disseminate statistical data over maps. We propose a methodology based on quadtree hierarchical geographic data structures. We create a varying size grid adapted to local area densities. High populated zones are disaggregated in small squares to allow dissemination of accurate data. Alternatively, information on low populated zones is published in big squares to avoid identiﬁcation of individual data. The methodology has been applied to the 2014 population register data in Catalonia.},
	language = {en},
	author = {Lagonigro, Raymond and Oller, Ramon and Martori, Joan Carles},
}

@article{behnisch_using_2013,
	title = {Using quadtree representations in building stock visualization and analysis},
	volume = {67},
	copyright = {Copyright (c)},
	issn = {2702-5985},
	url = {https://www.erdkunde.uni-bonn.de/article/view/2726},
	doi = {10.3112/erdkunde.2013.02.04},
	abstract = {Many public facilities, institutions, and private domains in Germany are still limited to using administrative zones when visualizing densities of statistical data. When dealing with geospatial information the following question is becoming increasingly more important: „How much data (persons/buildings) must be aggregated to consider regulations of data protection?”. This article presents a quadtree representation to provide building data management on a small scale. In recent years, German buildings were fully geocoded as a building polygon. Regarding statistical and cartographic analysis this allows the use of grids of arbitrary size and any administrative units. The city of Hamburg and several surrounding communities (Hamburg conurbation) are used as an example to demonstrate the principle of a quadtree. It is also possible to apply the general model to other sensitive data and to other regions (e.g. cities or rural areas). This includes the possibility of using regular grids in any projection useful for European cooperation (INSPIRE). The building representation, using varying cell sizes is not to be understood as a pure visualization as it serves as a new structure on a small scale to carry out cell-based analyses in the future.},
	language = {en},
	number = {2},
	urldate = {2024-03-20},
	journal = {ERDKUNDE},
	author = {Behnisch, Martin and Meinel, Gotthard and Tramsen, Sebastian and Diesselmann, Markus},
	month = jun,
	year = {2013},
	note = {Number: 2},
	keywords = {GIS, Germany, building stock, cell-based analysis, data protection, quadtree, thematic cartography},
	pages = {151--166},
}

@article{lagonigro_aquadtree_2020,
	title = {{AQuadtree}: an {R} {Package} for {Quadtree} {Anonymization} of {Point} {Data}},
	volume = {12},
	issn = {2073-4859},
	shorttitle = {{AQuadtree}},
	url = {https://journal.r-project.org/archive/2020/RJ-2021-013/index.html},
	doi = {10.32614/RJ-2021-013},
	abstract = {The demand for precise data for analytical purposes grows rapidly among the research community and decision makers as more geographic information is being collected. Laws protecting data privacy are being enforced to prevent data disclosure. Statistical institutes and agencies need methods to preserve conﬁdentiality while maintaining accuracy when disclosing geographic data. In this paper we present the AQuadtree package, a software intended to produce and deal with oﬃcial spatial data making data privacy and accuracy compatible. The lack of speciﬁc methods in R to anonymize spatial data motivated the development of this package, providing an automatic aggregation tool to anonymize point data. We propose a methodology based on hierarchical geographic data structures to create a varying size grid adapted to local area population densities. This article gives insights and hints for implementation and usage. We hope this new tool may be helpful for statistical oﬃces and users of oﬃcial spatial data.},
	language = {en},
	number = {2},
	urldate = {2024-03-20},
	journal = {The R Journal},
	author = {Lagonigro, Raymond and Oller, Ramon and Martori, Carles, Joan},
	year = {2020},
	pages = {209},
}

@article{armstrong_geographic_2005,
	title = {Geographic {Information} {Technologies} and {Personal} {Privacy}},
	volume = {40},
	issn = {0317-7173, 1911-9925},
	url = {https://utpjournals.press/doi/10.3138/RU65-81R3-0W75-8V21},
	doi = {10.3138/RU65-81R3-0W75-8V21},
	abstract = {Concepts of privacy are fluid. They change according to historical contingencies and are mediated by technology. Geospatial technologies are now altering the way privacy is being considered. Remote sensing technologies can be used to observe, or infer, the locations of individuals from space, from remotely piloted aircraft, and from fixed terrestrial observation points. Other geospatial technologies can be used to track movements and to recover individual-level information from maps. These changes are welcomed by some, since they provide a certain level of public safety (e.g., E-911). In other cases, however, a lack of awareness about the sinister aspects of surveillance may lead to complacency. Where personal privacy is eroded, individuals should be aware of the limitations of technology and the degree to which it may be applied to monitor their activities.},
	language = {en},
	number = {4},
	urldate = {2024-03-19},
	journal = {Cartographica: The International Journal for Geographic Information and Geovisualization},
	author = {Armstrong, Marc P. and Ruggles, Amy J.},
	month = dec,
	year = {2005},
	pages = {63--73},
}

@article{lagonigro_aquadtree_2020-1,
	title = {{AQuadtree}: an {R} {Package} for {Quadtree} {Anonymization} of {Point} {Data}},
	volume = {12},
	issn = {2073-4859},
	shorttitle = {{AQuadtree}},
	url = {https://journal.r-project.org/archive/2020/RJ-2021-013/index.html},
	doi = {10.32614/RJ-2021-013},
	abstract = {The demand for precise data for analytical purposes grows rapidly among the research community and decision makers as more geographic information is being collected. Laws protecting data privacy are being enforced to prevent data disclosure. Statistical institutes and agencies need methods to preserve conﬁdentiality while maintaining accuracy when disclosing geographic data. In this paper we present the AQuadtree package, a software intended to produce and deal with oﬃcial spatial data making data privacy and accuracy compatible. The lack of speciﬁc methods in R to anonymize spatial data motivated the development of this package, providing an automatic aggregation tool to anonymize point data. We propose a methodology based on hierarchical geographic data structures to create a varying size grid adapted to local area population densities. This article gives insights and hints for implementation and usage. We hope this new tool may be helpful for statistical oﬃces and users of oﬃcial spatial data.},
	language = {en},
	number = {2},
	urldate = {2024-03-18},
	journal = {The R Journal},
	author = {Lagonigro, Raymond and Oller, Ramon and Martori, Carles, Joan},
	year = {2020},
	pages = {209},
}

@article{lagonigro_quadtree_2017,
	title = {A quadtree approach based on {European} geographic grids: reconciling data privacy and accuracy},
	issn = {2013-8830},
	shorttitle = {A quadtree approach based on {European} geographic grids},
	url = {https://doi.org/10.2436/20.8080.02.55},
	doi = {10.2436/20.8080.02.55},
	abstract = {Methods to preserve confidentiality when publishing geographic information conflict with the need to publish accurate data. The goal of this paper is to create a European geographic grid frame- work to disseminate statistical data over maps. We propose a methodology based on quadtree hierarchical geographic data structures. We create a varying size grid adapted to local area densities. High populated zones are disaggregated in small squares to allow dissemination of accurate data. Alternatively, information on low populated zones is published in big squares to avoid identification of individual data. The methodology has been applied to the 2014 population register data in Catalonia},
	language = {eng},
	number = {41},
	urldate = {2024-03-18},
	journal = {SORT. Statistics and Operations Research Transactions},
	author = {Lagonigro, Raymond and Oller, Ramon and Martori, Joan Carles},
	year = {2017},
	pages = {139--158},
}

@inproceedings{noauthor_notitle_nodate-1,
}

@article{reiter_estimating_2005,
	title = {Estimating {Risks} of {Identification} {Disclosure} in {Microdata}},
	volume = {100},
	issn = {0162-1459, 1537-274X},
	url = {http://www2.stat.duke.edu/~jerry/Papers/jasa05.pdf},
	doi = {10.1198/016214505000000619},
	abstract = {When statistical agencies release microdata to the public, malicious users (intruders) may be able to link records in the released data to records in external databases. Releasing data in ways that fail to prevent such identifications may discredit the agency or, for some data, constitute a breach of law. To limit disclosures, agencies often release altered versions of the data; however, there usually remain risks of identification. This article applies and extends the framework developed by Duncan and Lambert for computing probabilities of identification for sampled units. It describes methods tailored specifically to data altered by recoding and topcoding variables, data swapping, or adding random noise (and combinations of these common data alteration techniques) that agencies can use to assess threats from intruders who possess information on relationships among variables and the methods of data alteration. Using data from the Current Population Survey, the article illustrates a step-by-step process for evaluating identification disclosure risks for competing releases under varying assumptions of intruders' knowledge. Risk measures are presented for individual units and for entire datasets.},
	language = {en},
	number = {472},
	urldate = {2024-03-05},
	journal = {Journal of the American Statistical Association},
	author = {Reiter, Jerome P},
	month = dec,
	year = {2005},
	keywords = {Re-identification risk, risk-utility trade-off},
	pages = {1103--1112},
}

@article{zhou_smoothing_2010,
	title = {A smoothing approach for masking spatial data},
	volume = {4},
	issn = {1932-6157},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-4/issue-3/A-smoothing-approach-for-masking-spatial-data/10.1214/09-AOAS325.full},
	doi = {10.1214/09-AOAS325},
	abstract = {Individual-level health data are often not publicly available due to confidentiality ; masked data are released instead. Therefore, it is important to evaluate the utility of using the masked data in statistical analyses such as regression. In this paper we propose a data masking method which is based on spatial smoothing techniques. The proposed method allows for selecting both the form and the degree of masking, thus resulting in a large degree of flexibility. We investigate the utility of the masked data sets in terms of the mean square error (MSE) of regression parameter estimates when fitting a Generalized Linear Model (GLM) to the masked data. We also show that incorporating prior knowledge on the spatial pattern of the exposure into the data masking may reduce the bias and MSE of the parameter estimates. By evaluating both utility and disclosure risk as functions of the form and the degree of masking, our method produces a risk-utility profile which can facilitate the selection of masking parameters. We apply the method to a study of racial disparities in mortality rates using data on more than 4 million Medicare enrollees residing in 2095 zip codes in the Northeast region of the United States.},
	number = {3},
	urldate = {2024-03-05},
	journal = {The Annals of Applied Statistics},
	author = {Zhou, Yijie and Dominici, Francesca and Louis, Thomas A.},
	month = sep,
	year = {2010},
	keywords = {SDC - geo referenced data},
}

@article{lee_evaluation_2019,
	title = {An evaluation of kernel smoothing to protect the confidentiality of individual locations},
	volume = {23},
	issn = {1226-5934, 2161-6779},
	url = {https://www.tandfonline.com/doi/full/10.1080/12265934.2018.1482778},
	doi = {10.1080/12265934.2018.1482778},
	abstract = {ABSTRACT With advances in spatial data management technologies, accurate geographic information about individual patients increasingly has become available. Researchers should protect the privacy of patients, which includes their locational information, in public health data analyses. Protecting privacy involves a trade-off between information loss and disclosure risk. Estimation of a kernel density surface commonly has been used to mask confidential point locations. However, the literature lacks an extensive discussion of reverse transformations from a kernel density estimation surface to points, and evaluations of recovered points compared to their original point counterparts. This paper presents a method to recover relatively precise point locations from a kernel density estimation surface using geometric centres of clusters, and evaluates recovered points in terms of protecting locational privacy and maintaining locational accuracy. An application illustrates this method utilizing late-stage colorectal cancer points in the Pensacola metropolitan statistical area, Florida that examines various kernel density estimation surfaces with different bandwidths and cell sizes.},
	language = {en},
	number = {3},
	urldate = {2024-03-05},
	journal = {International Journal of Urban Sciences},
	author = {Lee, Monghyeon and Chun, Yongwan and Griffith, Daniel A.},
	month = jul,
	year = {2019},
	pages = {335--351},
}

@article{lee_evaluation_2019-1,
	title = {An evaluation of kernel smoothing to protect the confidentiality of individual locations},
	volume = {23},
	issn = {1226-5934, 2161-6779},
	url = {https://www.tandfonline.com/doi/full/10.1080/12265934.2018.1482778},
	doi = {10.1080/12265934.2018.1482778},
	abstract = {ABSTRACT With advances in spatial data management technologies, accurate geographic information about individual patients increasingly has become available. Researchers should protect the privacy of patients, which includes their locational information, in public health data analyses. Protecting privacy involves a trade-off between information loss and disclosure risk. Estimation of a kernel density surface commonly has been used to mask confidential point locations. However, the literature lacks an extensive discussion of reverse transformations from a kernel density estimation surface to points, and evaluations of recovered points compared to their original point counterparts. This paper presents a method to recover relatively precise point locations from a kernel density estimation surface using geometric centres of clusters, and evaluates recovered points in terms of protecting locational privacy and maintaining locational accuracy. An application illustrates this method utilizing late-stage colorectal cancer points in the Pensacola metropolitan statistical area, Florida that examines various kernel density estimation surfaces with different bandwidths and cell sizes.},
	language = {en},
	number = {3},
	urldate = {2024-03-05},
	journal = {International Journal of Urban Sciences},
	author = {Lee, Monghyeon and Chun, Yongwan and Griffith, Daniel A.},
	month = jul,
	year = {2019},
	keywords = {SDC - attack scenario, SDC - geo referenced data},
	pages = {335--351},
}

@article{wang_how_2019,
	title = {How {Is} the {Confidentiality} of {Crime} {Locations} {Affected} by {Parameters} in {Kernel} {Density} {Estimation}?},
	volume = {8},
	issn = {2220-9964},
	url = {https://www.mdpi.com/2220-9964/8/12/544},
	doi = {10.3390/ijgi8120544},
	abstract = {Kernel density estimation (KDE) is widely adopted to show the overall crime distribution and at the same time obscure exact crime locations due to the confidentiality of crime data in many countries. However, the confidential level of crime locational information in the KDE map has not been systematically investigated. This study aims to examine whether a kernel density map could be reverse-transformed to its original map with discrete crime locations. Using the Epanecknikov kernel function, a default setting in ArcGIS for density mapping, the transformation from a density map to a point map was conducted with various combinations of parameters to examine its impact on the deconvolution process (density to point location). Results indicate that if the bandwidth parameter (search radius) in the original convolution process (point to density) was known, the original point map could be fully recovered by a deconvolution process. Conversely, when the parameter was unknown, the deconvolution process would be unable to restore the original point map. Experiments on four different point maps—a random point distribution, a simulated monocentric point distribution, a simulated polycentric point distribution, and a real crime location map—show consistent results. Therefore, it can be concluded that the point location of crime events cannot be restored from crime density maps as long as parameters such as the search radius parameter in the density mapping process remain confidential.},
	language = {en},
	number = {12},
	urldate = {2024-03-05},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Wang, Zengli and Liu, Lin and Zhou, Hanlin and Lan, Minxuan},
	month = nov,
	year = {2019},
	keywords = {SDC - attack scenario, SDC - geo referenced data},
	pages = {544},
}

@incollection{domingo-ferrer_epsilon-differential_2020,
	address = {Cham},
	title = {\$\${\textbackslash}epsilon\$\$-{Differential} {Privacy} for {Microdata} {Releases} {Does} {Not} {Guarantee} {Confidentiality} ({Let} {Alone} {Utility})},
	volume = {12276},
	isbn = {978-3-030-57520-5 978-3-030-57521-2},
	url = {http://link.springer.com/10.1007/978-3-030-57521-2_2},
	abstract = {Differential privacy (DP) is a privacy model that was
designed for interactive queries to databases. Its use has then been
extended to other data release formats, including microdata. In this
paper we show that setting a certain epsilon in DP does not determine the confidentiality
offered by DP microdata, let alone their utility. Confidentiality
refers to the difficulty of correctly matching original and anonymized
data, and utility refers to anonymized data preserving the correlation
structure of original data. Specifically, we present two methods for generating
epsilon-differentially private microdata. One of them creates DP synthetic
microdata from noise-added covariances. The other relies on adding noise
to the cumulative distribution function. We present empirical work that
compares the two new methods with DP microdata generation via prior
microaggregation. The comparison is in terms of several confidentiality
and utility metrics. Our experimental results indicate that different
methods to enforce epsilon-DP lead to very different utility and confidentiality
levels. Both confidentiality and utility seem rather dependent on the
amount of permutation performed by the particular SDC method used
to enforce DP. Thus suggests that DP is not a good privacy model for
microdata releases.},
	language = {en},
	urldate = {2023-05-09},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Muralidhar, Krishnamurty and Domingo-Ferrer, Josep and Martínez, Sergio},
	editor = {Domingo-Ferrer, Josep and Muralidhar, Krishnamurty},
	year = {2020},
	doi = {10.1007/978-3-030-57521-2_2},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Differential Privacy, SDC - general views, Synthetic data, risk-utility trade-off},
	pages = {21--31},
}

@article{noauthor_notitle_nodate-2,
}

@inproceedings{costemalle_identification_2018,
	address = {Paris},
	title = {Identification des problèmes de différenciation géographique},
	url = {https://www.jms-insee.fr/2018/S23_2_ACTE_COSTEMALLE_JMS2018.pdf},
	language = {fr-FR},
	booktitle = {Journées de {Méthodologie} {Statistique}},
	author = {Costemalle, Vianney},
	month = jun,
	year = {2018},
	keywords = {différenciation géographique, geo differencing},
}

@article{duke-williams_can_1998,
	title = {Can {Census} {Offices} publish statistics for more than one small area geography? {An} analysis of the differencing problem in statistical disclosure},
	volume = {12},
	issn = {1365-8816, 1362-3087},
	shorttitle = {Can {Census} {Offices} publish statistics for more than one small area geography?},
	url = {http://www.tandfonline.com/doi/abs/10.1080/136588198241680},
	doi = {10.1080/136588198241680},
	language = {en},
	number = {6},
	urldate = {2023-10-26},
	journal = {International Journal of Geographical Information Science},
	author = {Duke-Williams, Oliver and Rees, Philip},
	month = sep,
	year = {1998},
	keywords = {différenciation géographique, geo differencing},
	pages = {579--605},
}

@misc{australian_bureau_of_statistics_abs_ssf_nodate,
	title = {{SSF} {Guidance} {Material} – {Protecting} {Privacy} for {Geospatially} {Enabled} {Statistics}: {Geographic} {Differencing}},
	url = {https://www.abs.gov.au/statistics/statistical-geography/statistical-spatial-framework-ssf/statistical-spatial-framework-guidance-material/ssf-guidance-material-protecting-privacy-geospatially-enabled-statistics-geographic-differencing},
	abstract = {When individuals or organisations provide information that is private¹ , this information must be managed as sensitive information and confidentialisation methods must be used to protect that private information. The Statistical Spatial Framework (SSF)² identifies the importance of protecting private information that is geospatially enabled. These protections must be applied to private information stored in unit record datasets (where data is in its raw form) and to statistical information that is released from these datasets. Maintaining privacy is of critical importance whenever any data is released from a socio-economic dataset; however, when the data being released includes point-based locations (i.e. coordinates) or regional³ breakdowns there are some specific issues that must be considered.},
	language = {en},
	editor = {{Australian Bureau of Statistics (ABS)}},
}

@incollection{nguyen_publication_nodate,
	title = {Publication de données : dispose-t-on de techniques d’anonymisation suffisamment robustes ?},
	url = {https://benjamin-nguyen.fr/papers/bergerLevrault23.pdf},
	author = {Nguyen, Benjamin},
	keywords = {Differential Privacy, SDC - attack scenario, SDC - general views, k-anonymity, risk-utility trade-off},
}

@incollection{noauthor_notitle_nodate-3,
}

@article{abowd_2020_2022,
	title = {The 2020 {Census} {Disclosure} {Avoidance} {System} {TopDown} {Algorithm}},
	url = {https://hdsr.mitpress.mit.edu/pub/7evz361i},
	doi = {10.1162/99608f92.529e3cb9},
	language = {en},
	number = {Special Issue 2},
	urldate = {2024-01-17},
	journal = {Harvard Data Science Review},
	author = {Abowd, John and Ashmead, Robert and Cumings-Menon, Ryan and Garfinkel, Simson and Heineck, Micah and Heiss, Christine and Johns, Robert and Kifer, Daniel and Leclerc, Philip and Machanavajjhala, Ashwin and Moran, Brett and Sexton, William and Spence, Matthew and Zhuravlev, Pavel},
	month = jun,
	year = {2022},
	keywords = {Differential Privacy, US Census},
}

@article{cumings-menon_disclosure_2023,
	title = {Disclosure {Avoidance} for the 2020 {Census} {Demographic} and {Housing} {Characteristics} {File}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2312.10863},
	doi = {10.48550/ARXIV.2312.10863},
	abstract = {In "The 2020 Census Disclosure Avoidance System TopDown Algorithm," Abowd et al. (2022) describe the concepts and methods used by the Disclosure Avoidance System (DAS) to produce formally private output in support of the 2020 Census data product releases, with a particular focus on the DAS implementation that was used to create the 2020 Census Redistricting Data (P.L. 94-171) Summary File. In this paper we describe the updates to the DAS that were required to release the Demographic and Housing Characteristics (DHC) File, which provides more granular tables than other data products, such as the Redistricting Data Summary File. We also describe the final configuration parameters used for the production DHC DAS implementation, as well as subsequent experimental data products to facilitate development of tools that provide confidence intervals for confidential 2020 Census tabulations.},
	urldate = {2024-01-17},
	author = {Cumings-Menon, Ryan and Ashmead, Robert and Kifer, Daniel and Leclerc, Philip and Spence, Matthew and Zhuravlev, Pavel and Abowd, John M.},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation (stat.CO), Cryptography and Security (cs.CR), Differential Privacy, FOS: Computer and information sciences, US Census},
}

@article{abowd_2010_2023,
	title = {The 2010 {Census} {Confidentiality} {Protections} {Failed}, {Here}'s {How} and {Why}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2312.11283},
	doi = {10.48550/ARXIV.2312.11283},
	abstract = {Using only 34 published tables, we reconstruct five variables (census block, sex, age, race, and ethnicity) in the confidential 2010 Census person records. Using the 38-bin age variable tabulated at the census block level, at most 20.1\% of reconstructed records can differ from their confidential source on even a single value for these five variables. Using only published data, an attacker can verify that all records in 70\% of all census blocks (97 million people) are perfectly reconstructed. The tabular publications in Summary File 1 thus have prohibited disclosure risk similar to the unreleased confidential microdata. Reidentification studies confirm that an attacker can, within blocks with perfect reconstruction accuracy, correctly infer the actual census response on race and ethnicity for 3.4 million vulnerable population uniques (persons with nonmodal characteristics) with 95\% accuracy, the same precision as the confidential data achieve and far greater than statistical baselines. The flaw in the 2010 Census framework was the assumption that aggregation prevented accurate microdata reconstruction, justifying weaker disclosure limitation methods than were applied to 2010 Census public microdata. The framework used for 2020 Census publications defends against attacks that are based on reconstruction, as we also demonstrate here. Finally, we show that alternatives to the 2020 Census Disclosure Avoidance System with similar accuracy (enhanced swapping) also fail to protect confidentiality, and those that partially defend against reconstruction attacks (incomplete suppression implementations) destroy the primary statutory use case: data for redistricting all legislatures in the country in compliance with the 1965 Voting Rights Act.},
	urldate = {2024-01-17},
	author = {Abowd, John M. and Adams, Tamara and Ashmead, Robert and Darais, David and Dey, Sourya and Garfinkel, Simson L. and Goldschlag, Nathan and Kifer, Daniel and Leclerc, Philip and Lew, Ethan and Moore, Scott and Rodríguez, Rolando A. and Tadros, Ramy N. and Vilhuber, Lars},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Applications (stat.AP), Cryptography and Security (cs.CR), Differential Privacy, Econometrics (econ.EM), FOS: Computer and information sciences, FOS: Economics and business, SDC - attack scenario, US Census},
}

@article{kenny_use_2021,
	title = {The use of differential privacy for census data and its impact on redistricting: {The} case of the 2020 {U}.{S}. {Census}},
	volume = {7},
	issn = {2375-2548},
	shorttitle = {The use of differential privacy for census data and its impact on redistricting},
	url = {https://www.science.org/doi/10.1126/sciadv.abk3283},
	doi = {10.1126/sciadv.abk3283},
	abstract = {New Census privacy protections may introduce both bias and noise into redistricting and voting rights analysis.
          , 
            Census statistics play a key role in public policy decisions and social science research. However, given the risk of revealing individual information, many statistical agencies are considering disclosure control methods based on differential privacy, which add noise to tabulated data. Unlike other applications of differential privacy, however, census statistics must be postprocessed after noise injection to be usable. We study the impact of the U.S. Census Bureau’s latest disclosure avoidance system (DAS) on a major application of census statistics, the redrawing of electoral districts. We find that the DAS systematically undercounts the population in mixed-race and mixed-partisan precincts, yielding unpredictable racial and partisan biases. While the DAS leads to a likely violation of the “One Person, One Vote” standard as currently interpreted, it does not prevent accurate predictions of an individual’s race and ethnicity. Our findings underscore the difficulty of balancing accuracy and respondent privacy in the Census.},
	language = {en},
	number = {41},
	urldate = {2023-12-07},
	journal = {Science Advances},
	author = {Kenny, Christopher T. and Kuriwaki, Shiro and McCartan, Cory and Rosenman, Evan T. R. and Simko, Tyler and Imai, Kosuke},
	month = oct,
	year = {2021},
	keywords = {Differential Privacy, Re-identification risk, US Census, risk-utility trade-off},
	pages = {eabk3283},
}

@inproceedings{narayanan_robust_2008,
	address = {Oakland, CA, USA},
	title = {Robust {De}-anonymization of {Large} {Sparse} {Datasets}},
	isbn = {978-0-7695-3168-7},
	url = {http://ieeexplore.ieee.org/document/4531148/},
	doi = {10.1109/SP.2008.33},
	abstract = {We present a new class of statistical de-
anonymization attacks against high-dimensional
micro-data, such as individual preferences, recommen-
dations, transaction records and so on. Our techniques
are robust to perturbation in the data and tolerate some
mistakes in the adversary’s background knowledge.
We apply our de-anonymization methodology to the
Netflix Prize dataset, which contains anonymous movie
ratings of 500,000 subscribers of Netflix, the world’s
largest online movie rental service. We demonstrate
that an adversary who knows only a little bit about
an individual subscriber can easily identify this sub-
scriber’s record in the dataset. Using the Internet
Movie Database as the source of background knowl-
edge, we successfully identified the Netflix records of
known users, uncovering their apparent political pref-
erences and other potentially sensitive information},
	urldate = {2023-12-06},
	booktitle = {2008 {IEEE} {Symposium} on {Security} and {Privacy} (sp 2008)},
	publisher = {IEEE},
	author = {Narayanan, Arvind and Shmatikov, Vitaly},
	month = may,
	year = {2008},
	note = {ISSN: 1081-6011},
	keywords = {Re-identification risk, risk-utility trade-off},
	pages = {111--125},
}

@misc{bergeat_gestion_2016,
	title = {La gestion de la confidentialité pour les données individuelles},
	url = {https://www.insee.fr/fr/statistiques/2535625},
	language = {fr},
	publisher = {Insee},
	author = {Bergeat, Maxime},
	month = dec,
	year = {2016},
	keywords = {SDC - general views, SDC - microdata, risk-utility trade-off},
}

@misc{bergeat_gestion_2016-1,
	title = {La gestion de la confidentialité pour les données individuelles},
	url = {https://www.insee.fr/fr/statistiques/2535625},
	language = {fr},
	publisher = {Insee},
	author = {Bergeat, Maxime},
	month = dec,
	year = {2016},
	keywords = {SDC - general views, SDC - microdata, risk-utility trade-off},
}

@inproceedings{sarcevic_fingerprinting_2021,
	address = {Poland},
	title = {Fingerprinting relational data},
	url = {https://unece.org/sites/default/files/2021-12/SDC2021_Day1_%C5%A0ar%C4%8Devi%C4%87_AD.pdf},
	abstract = {Fingerprinting is a method of embedding a traceable mark into digital data to verify the owner and identify the
recipient of a released copy of a data set. This is crucial when releasing data to third parties, especially if it
involves a fee, or if the data is of sensitive nature, due to which further sharing and leaks should be discouraged
and deterred from.
Fingerprints are achieved by introducing modifications to the data that encode the owner's and recipient's
identifiers. Therefore, a robust fingerprint is required to achieve successful ownership protection while
affecting the data as little as possible. We focus our research on a few challenges in the domain of
fingerprinting relational data. We (i) propose a framework for evaluation and analysing fingerprinting methods
for relational data with regards to risks relating to the removal of the mark and data utility, (ii) analyse the
trade-off between fingerprint robustness and data utility and (iii) address the problem of fingerprinting
categorical data as a use-case with a smaller bandwidth for imperceptible modifications and propose a
correlation-preserving technique for categorical relational data.},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Sarcevic, Tanja},
	month = dec,
	year = {2021},
}

@inproceedings{guillo_protection_2023,
	address = {Bruxelles},
	title = {Protection of linked tables with a suppressive approach. {Method} and {Use} {Cases}},
	language = {en},
	booktitle = {{NTTS} 2023},
	author = {Guillo, Clément and Jamme, Julien and Rastout, Nathanaël},
	month = mar,
	year = {2023},
}

@article{rocher_estimating_2019,
	title = {Estimating the success of re-identifications in incomplete datasets using generative models},
	volume = {10},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-10933-3},
	doi = {10.1038/s41467-019-10933-3},
	abstract = {Abstract
            While rich medical, behavioral, and socio-demographic data are key to modern data-driven research, their collection and use raise legitimate privacy concerns. Anonymizing datasets through de-identification and sampling before sharing them has been the main tool used to address those concerns. We here propose a generative copula-based method that can accurately estimate the likelihood of a specific person to be correctly re-identified, even in a heavily incomplete dataset. On 210 populations, our method obtains AUC scores for predicting individual uniqueness ranging from 0.84 to 0.97, with low false-discovery rate. Using our model, we find that 99.98\% of Americans would be correctly re-identified in any dataset using 15 demographic attributes. Our results suggest that even heavily sampled anonymized datasets are unlikely to satisfy the modern standards for anonymization set forth by GDPR and seriously challenge the technical and legal adequacy of the de-identification release-and-forget model.},
	language = {en},
	number = {1},
	urldate = {2023-11-16},
	journal = {Nature Communications},
	author = {Rocher, Luc and Hendrickx, Julien M. and De Montjoye, Yves-Alexandre},
	month = jul,
	year = {2019},
	pages = {3069},
}

@book{dwork_algorithmic_2014,
	address = {Boston, MA Delft},
	series = {Foundations and trends in theoretical computer science},
	title = {The algorithmic foundations of differential privacy},
	isbn = {978-1-60198-818-8},
	url = {https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf},
	language = {eng},
	number = {9,3/4},
	publisher = {Now},
	author = {Dwork, Cynthia and Roth, Aaron},
	year = {2014},
}

@book{drechsler_synthetic_2011,
	address = {New York},
	series = {Lecture notes in statistics},
	title = {Synthetic datasets for statistical disclosure control: theory and implementation},
	isbn = {978-1-4614-0325-8},
	shorttitle = {Synthetic datasets for statistical disclosure control},
	number = {201},
	publisher = {Springer},
	author = {Drechsler, Jörg},
	year = {2011},
	note = {OCLC: ocn733239576},
	keywords = {Access control, Confidential communications, Data protection, Disclosure of information, Electronic records, Multiple imputation (Statistics), Social sciences, Statistical methods, Statistics, Synthetic data},
}

@inproceedings{gussenbauer_spatial_2023,
	address = {Wiesbaden},
	title = {Spatial {SDC} experiments and evaluations – multiple countries comparison},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S3_4_Austria_Gussenbauer_D.pdf},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Güssenbauer, Johannes and Jamme, Julien and De Jonge, Edwin and De Wolf, Peter-Paul and Möhler, Martin},
	month = sep,
	year = {2023},
	keywords = {SDC - geo referenced data, SDC - tools, risk-utility trade-off},
}

@inproceedings{ricciato_kantorovich-wasserstein_2023,
	address = {Brussels},
	title = {Kantorovich-{Wasserstein} {Distances} for {Spatial} {Statistics}: {The} {Spatial}-{KWD} library},
	language = {en},
	booktitle = {{NTTS} 2023},
	author = {Ricciato, Fabio},
	month = sep,
	year = {2023},
}

@inproceedings{thompson_methodology_2013,
	address = {Ottawa},
	title = {Methodology for the {Automatic} {Confidentialisation} of {Statistical} {Outputs} from {Remote} {Servers} at the {Australian} {Bureau} of {Statistics}},
	url = {https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_1_ABS.pdf},
	booktitle = {Joint {UNECE}/{Eurostat} work session on statistical data confidentiality},
	author = {Thompson, Gwenda and Broadfoot, Stephen and Elazar, Daniel},
	month = oct,
	year = {2013},
	keywords = {CKM, SDC - tools, risk-utility trade-off},
}

@article{chipperfield_australian_2016,
	title = {The {Australian} {Bureau} of {Statistics} and releasing frequency tables via a remote server},
	volume = {32},
	issn = {18747655, 18759254},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SJI-160969},
	doi = {10.3233/SJI-160969},
	number = {1},
	urldate = {2023-11-09},
	journal = {Statistical Journal of the IAOS},
	author = {Chipperfield, James and Gow, Daniel and Loong, Bronwyn},
	month = feb,
	year = {2016},
	keywords = {ABS - Table Builder, Differential Privacy, SDC - frequency tables, SDC - tools, risk-utility trade-off},
	pages = {53--64},
}

@article{elliot_scenarios_1999,
	title = {Scenarios of attack: the data intruder's perspective on statistical disclosure risk},
	volume = {14},
	url = {https://www.researchgate.net/publication/343963431_Scenarios_of_attack_the_data_intruder's_perspective_on_statistical_disclosure_risk},
	abstract = {The release of valuable microdata files from sources such as the census of population, is constrained by fears of individual identification and hence disclosure of information. Much work has been done on analysing the risks of disclosure once an attempt has been made. However, the complexity of the social, psychological, and political factors that affect the probability of an attempt being made in the first place has meant that little work has been done on the overall likelihood of a disclosure attempt. Such an analysis is a crucial first step in a project to investigate the risk of statistical disclosure because the psychological components of an attempt (e.g. goals and motives) will influence the type of attack made, the strategy used, and the probability of identification or disclosure given an attempt.},
	journal = {Netherlands Official Statistics},
	author = {Elliot, Mark and Dale, Angela},
	year = {1999},
	keywords = {SDC - attack scenario, SDC - general views, risk-utility trade-off},
	pages = {6--10},
}

@article{redor_confidentialite_2023,
	title = {Confidentialité des données statistiques : un enjeu majeur pour le service statistique public},
	url = {https://www.insee.fr/fr/information/7635823?sommaire=7635842},
	number = {9},
	journal = {Courrier des Statistiques},
	author = {Redor, Patrick},
	year = {2023},
	keywords = {SDC - general views},
	pages = {46--64},
}

@misc{asghar_averaging_2019,
	title = {Averaging {Attacks} on {Bounded} {Noise}-based {Disclosure} {Control} {Algorithms}},
	url = {http://arxiv.org/abs/1902.06414},
	abstract = {We describe and evaluate an attack that reconstructs the histogram of any target attribute of a sensitive dataset which can only be queried through a specific class of real-world privacy-preserving algorithms which we call bounded perturbation algorithms. A defining property of such an algorithm is that it perturbs answers to the queries by adding zero-mean noise distributed within a bounded (possibly undisclosed) range. Other key properties of the algorithm include only allowing restricted queries (enforced via an online interface), suppressing answers to queries which are only satisfied by a small group of individuals (e.g., by returning a zero as an answer), and adding the same perturbation to two queries which are satisfied by the same set of individuals (to thwart differencing or averaging attacks). A real-world example of such an algorithm is the one deployed by the Australian Bureau of Statistics' (ABS) online tool called TableBuilder, which allows users to create tables, graphs and maps of Australian census data [30]. We assume an attacker (say, a curious analyst) who is given oracle access to the algorithm via an interface. We describe two attacks on the algorithm. Both attacks are based on carefully constructing (different) queries that evaluate to the same answer. The first attack finds the hidden perturbation parameter \$r\$ (if it is assumed not to be public knowledge). The second attack removes the noise to obtain the original answer of some (counting) query of choice. We also show how to use this attack to find the number of individuals in the dataset with a target attribute value \$a\$ of any attribute \$A\$, and then for all attribute values \$a\_i {\textbackslash}in A\$. Our attacks are a practical illustration of the (informal) fundamental law of information recovery which states that ``overly accurate estimates of too many statistics completely destroys privacy'' [9, 15].},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Asghar, Hassan Jameel and Kaafar, Dali},
	month = nov,
	year = {2019},
	note = {arXiv:1902.06414 [cs]},
	keywords = {CKM, Computer Science - Cryptography and Security, SDC - attack scenario, SDC - averaging attack, risk-utility trade-off},
}

@article{cox_risk-utility_2011,
	title = {Risk-{Utility} {Paradigms} for {Statistical} {Disclosure} {Limitation}: {How} to {Think}, {But} {Not} {How} to {Act}: {Risk}-{Utility} {Paradigms} for {SDL}},
	volume = {79},
	issn = {03067734},
	shorttitle = {Risk-{Utility} {Paradigms} for {Statistical} {Disclosure} {Limitation}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1751-5823.2011.00140.x},
	doi = {10.1111/j.1751-5823.2011.00140.x},
	language = {en},
	number = {2},
	urldate = {2023-11-08},
	journal = {International Statistical Review},
	author = {Cox, Lawrence H. and Karr, Alan F. and Kinney, Satkartar K.},
	month = aug,
	year = {2011},
	keywords = {SDC - general views, risk-utility trade-off},
	pages = {160--183},
}

@inproceedings{bach_case_2023,
	address = {Wiesbaden},
	title = {The case of bounds in noisy protection methods: {Selected} risk and utility perspectives from official population statistics},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S4_8_Eurostat_Bach_D.pdf},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Bach, Fabian},
	month = sep,
	year = {2023},
	keywords = {Differential Privacy, SDC - general views, risk-utility trade-off},
}

@inproceedings{branchu_donnees_2018-1,
	address = {Paris},
	title = {Données carroyées et confidentialité},
	url = {http://www.jms-insee.fr/2018/S23_3_ACTE_BRANCHU_JMS2018.pdf},
	author = {Branchu, Marc and Coostemalle, Vianney and Fontaine, Maëlle},
	year = {2018},
	keywords = {SDC - geo referenced data, SDC - tools},
}

@article{golle_revisiting_2006,
	title = {Revisiting the {Uniqueness} of {Simple} {Demographics} in the {US} {Population}},
	url = {https://crypto.stanford.edu/~pgolle/papers/census.pdf},
	journal = {WPES},
	author = {Golle, Philippe},
	month = oct,
	year = {2006},
}

@article{sweeney_simple_2000,
	title = {Simple {Demographics} {Often} {Identify} {People} {Uniquely}.},
	volume = {3},
	language = {en},
	journal = {Carnegie Mellon University, Data Privacy Working Paper},
	author = {Sweeney, Latanya},
	year = {2000},
	keywords = {Re-identification risk},
}

@article{armstrong_geographically_1999,
	title = {{GEOGRAPHICALLY} {MASKING} {HEALTH} {DATA} {TOPRESERVE} {CONFIDENTIALITY}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/(SICI)1097-0258(19990315)18:5%3C497::AID-SIM45%3E3.0.CO;2-%23},
	abstract = {The conventional approach to preserving the confidentiality of health records aggregates all records withina geographical area that has a population large enough to ensure prevention of disclosure. Though thisapproach normally protects the privacy of individuals, the use of such aggregated data limits the types ofresearch one can conduct and makes it impossible to address many important health problems. In this paperwe discuss the design and implementation of geographical masks that not only preserve the security ofindividual health records, but also support the investigation of questions that can be answered only withsome knowledge about the location of health events. We describe several alternative methods of maskingindividual-level data, evaluate their performance, and discuss both the degree to which we can analysemasked data validly as well as the relative security of each approach, should anyone attempt to recover theidentity of an individual from the masked data. We conclude that the geographical masks we describe, whenappropriately used, protect the confidentiality of health records while permitting many important geo-graphically-based analyses, but that further research is needed to determine how the power of tests forclustering or the strength of other associative relationships are adversely a§ected by the characteristics ofdi§erent masks.},
	language = {en},
	number = {18},
	journal = {Statistics in Medicine},
	author = {Armstrong, Marc P. and Rushton, Gerard and Zimmerman, Dale L.},
	year = {1999},
	pages = {497--525},
}

@article{young_geographically_2009,
	title = {Geographically intelligent disclosure control for flexible aggregation of census data},
	volume = {23},
	issn = {1365-8816, 1362-3087},
	url = {http://www.tandfonline.com/doi/abs/10.1080/13658810801949835},
	doi = {10.1080/13658810801949835},
	language = {en},
	number = {4},
	urldate = {2023-10-26},
	journal = {International Journal of Geographical Information Science},
	author = {Young, Caroline and Martin, David and Skinner, Chris},
	month = apr,
	year = {2009},
	pages = {457--482},
}

@article{duke-williams_can_1998-1,
	title = {Can {Census} {Offices} publish statistics for more than one small area geography? {An} analysis of the differencing problem in statistical disclosure},
	volume = {12},
	issn = {1365-8816, 1362-3087},
	shorttitle = {Can {Census} {Offices} publish statistics for more than one small area geography?},
	url = {http://www.tandfonline.com/doi/abs/10.1080/136588198241680},
	doi = {10.1080/136588198241680},
	language = {en},
	number = {6},
	urldate = {2023-10-26},
	journal = {International Journal of Geographical Information Science},
	author = {Duke-Williams, Oliver and Rees, Philip},
	month = sep,
	year = {1998},
	keywords = {différenciation géographique},
	pages = {579--605},
}

@book{loonis_handbook_2018,
	address = {Paris},
	series = {Insee {Méthodes}},
	title = {Handbook of {Spatial} {Analysis}},
	isbn = {978-2-11-139686-9},
	url = {https://www.insee.fr/en/information/3635545},
	language = {en},
	number = {131},
	publisher = {Insee},
	author = {Loonis, Vincent (dir.) and Bellefon (de), Marie-Pierre (coord.)},
	month = oct,
	year = {2018},
}

@misc{jamme_about_2023,
	address = {Brussels},
	type = {Diaporama},
	title = {About protecting multiple linked tables with a suppressive approach: an illustration with the {ICT} survey.},
	language = {en},
	author = {Jamme, Julien and Rastout, Nathanaël},
	month = jul,
	year = {2023},
}

@techreport{guillo_expose_2023,
	address = {Paris},
	type = {{GT} carroyage {RP} - {Rapport} d'aide à la prise de décision},
	title = {Exposé des enjeux et méthodes proposées pour la gestion de la confidentialité et la protection des données carroyées diffusées dans le cadre du {Census} 2021},
	language = {fr},
	institution = {Insee},
	author = {Guillo, Clément and Jamme, Julien},
	month = feb,
	year = {2023},
}

@misc{guillo_protection_2023-1,
	address = {Paris},
	type = {Diaporama},
	title = {Protection des données carroyées du {Census} 2021},
	language = {fr},
	author = {Guillo, Clément and Jamme, Julien},
	month = oct,
	year = {2023},
}

@article{predhumeau_synthetic_2023,
	title = {A synthetic population for agent-based modelling in {Canada}},
	volume = {10},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-023-02030-4},
	doi = {10.1038/s41597-023-02030-4},
	abstract = {Abstract
            In order to anticipate the impact of local public policies, a synthetic population reflecting the characteristics of the local population provides a valuable test bed. While synthetic population datasets are now available for several countries, there is no open-source synthetic population for Canada. We propose an open-source synthetic population of individuals and households at a fine geographical level for Canada for the years 2021, 2023 and 2030. Based on 2016 census data and population projections, the synthetic individuals have detailed socio-demographic attributes, including age, sex, income, education level, employment status and geographic locations, and are related into households. A comparison of the 2021 synthetic population with 2021 census data over various geographical areas validates the reliability of the synthetic dataset. Users can extract populations from the dataset for specific zones, to explore ‘what if’ scenarios on present and future populations. They can extend the dataset using local survey data to add new characteristics to individuals. Users can also run the code to generate populations for years up to 2042.},
	language = {en},
	number = {1},
	urldate = {2023-10-20},
	journal = {Scientific Data},
	author = {Prédhumeau, Manon and Manley, Ed},
	month = mar,
	year = {2023},
	keywords = {Synthetic data},
	pages = {148},
}

@inproceedings{rigaud_checking_2023,
	address = {Wiesbaden},
	title = {Checking {Data} {Outputs} from {Research} {Works}: a {Mixed} {Method} with {AI} and {Human} {Control}},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S5_5_CASD_Rigaud_D.pdf},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Rigaud, Titouan},
	month = sep,
	year = {2023},
}

@inproceedings{welpton_smoothing_2023,
	address = {Wiesbaden},
	title = {Smoothing the way for secure data access using synthetic data},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S2_2_ADR%20UK_Oliver_D_0.pdf},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Welpton, Richard and Oliver, Emily},
	month = sep,
	year = {2023},
}

@inproceedings{benschop_differential_2023,
	address = {Wiesbaden},
	title = {Differential privacy for microdata},
	url = {https://unece.org/sites/default/files/2023-09/SDC2023_S4_7_WB_Benschop_D_0.pdf},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Benschop, Thijs},
	month = sep,
	year = {2023},
	keywords = {Differential Privacy, SDC - microdata, k-anonymity, risk-utility trade-off},
}

@inproceedings{ito_potential_2023,
	address = {Wiesbaden},
	title = {The {Potential} of {Differential} {Privacy} {Applied} to {Detailed} {Statistical} {Tables} {Created} {Using} {Microdata} from the {Japanese} {Population} {Census}},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S3_6_Chuo%20Univ_Ito_D.pdf},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Ito, Shinsuke and Terada, Masayuki and Kato, Shunsuke},
	month = sep,
	year = {2023},
	keywords = {Differential Privacy, risk-utility trade-off},
}

@inproceedings{de_fondeville_protecting_2023,
	address = {Wiesbaden},
	title = {Protecting {High}-{Resolution} {Poverty} {Statistics} against {Disclosure} using {Differential} {Privacy}},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S3_1_Switzerland_Fondeville_D.pdf},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {de Fondeville, Raphaël and Shoemate, Michael and Zhang, Wanrong and Vadhan, Salil},
	month = sep,
	year = {2023},
	keywords = {Differential Privacy, SDC - geo referenced data, risk-utility trade-off},
}

@inproceedings{elliot_samples_2023,
	address = {Wiesbaden},
	title = {Do samples of synthetic microdata population replicate the relationship between samples taken from an original population and that population?},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S4_5_UnivManchester_Elliot_D.pdf},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Elliot, Mark and Little, Claire and Allmendinger, Richard},
	month = sep,
	year = {2023},
	keywords = {Synthetic data, risk-utility trade-off},
}

@inproceedings{noauthor_overview_nodate,
	title = {Overview of the {AnigeD} {Project} and {Potentials} of {Dataset} {Synthetization} for {Official} {Statistics} and {Research}},
}

@inproceedings{steffen_overview_2023,
	address = {Wiesbaden},
	title = {An overview of data protection strategies for individual-level geocoded data},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S2_1_Germany%20IAB_Steffen_D_0.pdf},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Steffen, Maike and Körner, Konstantin and Drechsler, Jörg},
	month = sep,
	year = {2023},
	keywords = {SDC - geo referenced data},
}

@inproceedings{volker_assessing_2023,
	address = {Wiesbaden},
	title = {{ASSESSING} {THE} {UTILITY} {OF} {SYNTHETIC} {DATA}: {A} {DENSITY} {RATIO} {PERSPECTIVE}},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S4_1_Utrecht%20Univ_Volker_D.pdf},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Volker, Thom Benjamin and De Wolf, Peter-Paul and van Kesteren, Erik-Jan},
	month = sep,
	year = {2023},
	keywords = {Synthetic data, risk-utility trade-off},
}

@inproceedings{solatorio_generating_2023,
	address = {Wiesbaden},
	title = {Generating {Synthetic} {Microdata} and {Assessing} {Statistical} {Disclosure} {Risk} {Measures}},
	url = {https://unece.org/sites/default/files/2023-08/SDC2023_S4_4_WB_Solatorio_D.pdf},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Solatorio, Aivin and Dupriez, Olivier},
	month = sep,
	year = {2023},
	keywords = {Re-identification risk, SDC - tools, Synthetic data, risk-utility trade-off},
}

@article{sugiyama_density-ratio_2012,
	title = {Density-ratio matching under the {Bregman} divergence: a unified framework of density-ratio estimation},
	volume = {64},
	issn = {0020-3157, 1572-9052},
	shorttitle = {Density-ratio matching under the {Bregman} divergence},
	url = {http://link.springer.com/10.1007/s10463-011-0343-8},
	doi = {10.1007/s10463-011-0343-8},
	language = {en},
	number = {5},
	urldate = {2023-10-12},
	journal = {Annals of the Institute of Statistical Mathematics},
	author = {Sugiyama, Masashi and Suzuki, Taiji and Kanamori, Takafumi},
	month = oct,
	year = {2012},
	pages = {1009--1044},
}

@inproceedings{zwick_overview_2023,
	address = {Wiesbaden},
	title = {Overview of the {AnigeD} {Project} and {Potentials} of {Dataset} {Synthetization} for {Official} {Statistics} and {Research}},
	url = {https://unece.org/sites/default/files/2023-09/SDC2023_S2_3_Germany_Garcia%20Ritz_D.pdf},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Zwick, Markus and Mühlhan, Jannek and Garcia Ritz, Yannik},
	month = sep,
	year = {2023},
	keywords = {Synthetic data},
}

@misc{bergeat_panorama_2014,
	address = {Paris},
	type = {Diaporama},
	title = {Un panorama de la protection des fichiers de données individuelles},
	author = {Bergeat, Maxime},
	month = jun,
	year = {2014},
}

@incollection{buron_confidentialite_2018,
	address = {Paris},
	edition = {Insee/Eurostat},
	series = {Insee {Méthodes}},
	title = {Confidentialité des données spatiales},
	isbn = {978-2-11-139677-7},
	url = {https://www.insee.fr/fr/information/3635442},
	language = {fr},
	number = {131},
	booktitle = {Manuel d'analyse spatiale},
	author = {Buron, Maël-Luc and Fontaine, Maëlle},
	month = oct,
	year = {2018},
	keywords = {SDC - geo referenced data},
	pages = {361--388},
}

@inproceedings{lefevre_mondrian_2006,
	address = {Atlanta, GA, USA},
	title = {Mondrian {Multidimensional} {K}-{Anonymity}},
	isbn = {978-0-7695-2570-9},
	url = {http://ieeexplore.ieee.org/document/1617393/},
	doi = {10.1109/ICDE.2006.101},
	urldate = {2023-10-11},
	booktitle = {22nd {International} {Conference} on {Data} {Engineering} ({ICDE}'06)},
	publisher = {IEEE},
	author = {LeFevre, K. and DeWitt, D.J. and Ramakrishnan, R.},
	year = {2006},
	keywords = {Re-identification risk, SDC - tools, k-anonymity, risk-utility trade-off},
	pages = {25--25},
}

@book{loonis_manuel_2018,
	address = {Paris},
	series = {Insee {Méthodes}},
	title = {Manuel d'{Analyse} {Spatiale}},
	isbn = {978-2-11-139677-7},
	url = {https://www.insee.fr/fr/information/3635442},
	language = {fr},
	number = {131},
	publisher = {Insee},
	author = {Loonis, Vincent (dir.) and Bellefon (de), Marie-Pierre (coord.)},
	month = oct,
	year = {2018},
}

@techreport{bonnans_diffusion_2022,
	address = {Paris},
	type = {Rapport de l'{IG}},
	title = {La diffusion sur mesure à l'{Insee}},
	url = {https://intranet.insee.fr/jcms/21829491_DBFileDocument/20230704-rapport-ig-diffusion-sur-mesure-insee},
	language = {fr},
	number = {2022\_72/DG75-B001},
	institution = {Insee},
	author = {Bonnans, Dominique and Quellec, Jean-Michel},
	month = dec,
	year = {2022},
}

@techreport{guedes_strategie_2023,
	address = {Paris},
	type = {Rapport de l'{IG}},
	title = {Stratégie de diffusion à l'infra-communal},
	url = {https://intranet.insee.fr/jcms/22528636_DBFileDocument/mi-2022-9-strategie-de-diffusion-infra-communal-rapport-ig-19-07-2023},
	language = {fr},
	number = {2023\_46/DG75-B001},
	institution = {Insee},
	author = {Guédès, Dominique and Quellec, Jean-Michel},
	month = jul,
	year = {2023},
}

@article{dalenius_data-swapping_1982,
	title = {Data-swapping: {A} technique for disclosure control},
	volume = {6},
	issn = {03783758},
	shorttitle = {Data-swapping},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0378375882900581},
	doi = {10.1016/0378-3758(82)90058-1},
	language = {en},
	number = {1},
	urldate = {2023-07-21},
	journal = {Journal of Statistical Planning and Inference},
	author = {Dalenius, Tore and Reiss, Steven P.},
	year = {1982},
	pages = {73--85},
}

@incollection{domingo-ferrer_calculation_2020,
	address = {Cham},
	title = {Calculation of {Risk} {Probabilities} for the {Cell} {Key} {Method}},
	volume = {12276},
	isbn = {978-3-030-57520-5 978-3-030-57521-2},
	url = {http://link.springer.com/10.1007/978-3-030-57521-2_11},
	language = {en},
	urldate = {2023-07-17},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Enderle, Tobias and Giessing, Sarah and Tent, Reinhard},
	editor = {Domingo-Ferrer, Josep and Muralidhar, Krishnamurty},
	year = {2020},
	doi = {10.1007/978-3-030-57521-2_11},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {151--165},
}

@incollection{domingo-ferrer_designing_2018,
	address = {Cham},
	title = {Designing {Confidentiality} on the {Fly} {Methodology} – {Three} {Aspects}},
	volume = {11126},
	isbn = {978-3-319-99770-4 978-3-319-99771-1},
	url = {http://link.springer.com/10.1007/978-3-319-99771-1_3},
	urldate = {2023-07-17},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Enderle, Tobias and Giessing, Sarah and Tent, Reinhard},
	editor = {Domingo-Ferrer, Josep and Montes, Francisco},
	year = {2018},
	doi = {10.1007/978-3-319-99771-1_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {28--42},
}

@incollection{semecurbe_spatial_2018,
	address = {Paris},
	edition = {Insee Methodes},
	title = {Spatial {Smoothing}},
	isbn = {978-2-11-139677-7},
	url = {https://www.insee.fr/en/statistiques/fichier/3635545/imet131-l-chapitre-8.pdf},
	abstract = {chapter 8},
	language = {en},
	booktitle = {Handbook of {Spatial} {Analysis}},
	author = {Sémécurbe, François and Genebes, Laure and Renaud, Auriane},
	month = oct,
	year = {2018},
}

@incollection{buron_confidentiality_nodate,
	address = {Paris},
	edition = {Insee Methodes},
	title = {Confidentiality of spatial data},
	isbn = {978-2-11-139677-7},
	url = {https://www.insee.fr/en/statistiques/fichier/3635545/imet131-r-chapitre-14.pdf},
	abstract = {chapter 14},
	language = {10/2018},
	booktitle = {Handbook of {Spatial} {Analysis}},
	author = {Buron, Maël and Fontaine, Maëlle},
	pages = {349--373},
}

@inproceedings{de_wolf_location_2017,
	title = {Location related risk and utility},
	url = {https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2017/3_LocationRiskUtility.pdf},
	abstract = {Cartographic maps have many practical uses and can be an attractive alter-
native for disseminating statistics with spatial characteristics. However, a detailed map
may disclose private data of individual units of a population. Traditionally, a disclosure
risk measure is related to the (distribution of) individual units. When publishing official
statistics on a map, the location is the identifying variable and thus information about
units is linked with locations. In this paper we try to formulate disclosure risk measures
in terms of locations: when is it safe to publish information linked to a certain location? The other side of the coin is obviously information loss associated with applying some disclosure control method(s). We also formulate some utility measures that may be used to assess the usability of disclosure control methods, specifically targeted at geo-coded
information, displayed on a map},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {De Wolf, Peter-Paul and De Jonge, Edwin},
	year = {2017},
	keywords = {Géographie, disclosure control, risk-utility trade-off},
}

@misc{ricciato_estimation_2021,
	title = {On the estimation of spatial density from mobile network operator data},
	url = {http://arxiv.org/abs/2009.05410},
	abstract = {We tackle the problem of estimating the spatial distribution of mobile phones from Mobile Network Operator (MNO) data, namely Call Detail Record (CDR) or signalling data. The process of transforming MNO data to a density map requires geolocating radio cells to determine their spatial footprint. Traditional geolocation solutions rely on Voronoi tessellations and approximate cell footprints by mutually disjoint regions. Recently, some pioneering work started to consider more elaborate geolocation methods with partially overlapping (non-disjoint) cell footprints coupled with a probabilistic model for phone-to-cell association. Estimating the spatial density in such a probabilistic setup is currently an open research problem and is the focus of the present work. We start by reviewing three different estimation methods proposed in literature and provide novel analytical insights that unveil some key aspects of their mutual relationships and properties. Furthermore, we develop a novel estimation approach for which a closed-form solution can be given. Numerical results based on semi-synthetic data are presented to assess the relative accuracy of each method. Our results indicate that the estimators based on overlapping cells have the potential to improve spatial accuracy over traditional approaches based on Voronoi tessellations.},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Ricciato, Fabio and Coluccia, Angelo},
	month = nov,
	year = {2021},
	note = {arXiv:2009.05410 [eess]},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
}

@misc{mckenna_disclosure_2018,
	title = {Disclosure {Avoidance} {Techniques} {Used} for the 1970 through 2010 {Decennial} {Censuses} of {Population} and {Housing}},
	url = {https://www2.census.gov/ces/wp/2018/CES-WP-18-47.pdf},
	abstract = {The U.S. Census Bureau conducts the decennial censuses under Title 13 of the U. S. Code with
the Section 9 mandate to not “use the information furnished under the provisions of this title
for any purpose other than the statistical purposes for which it is supplied; or make any
publication whereby the data furnished by any particular establishment or individual under this
title can be identified; or permit anyone other than the sworn officers and employees of the
Department or bureau or agency thereof to examine the individual reports (13 U.S.C. § 9
(2007)).” The Census Bureau applies disclosure avoidance techniques to its publicly released
statistical products in order to protect the confidentiality of its respondents and their data},
	language = {en},
	publisher = {US Census Bureau},
	author = {McKenna, Laura},
	month = oct,
	year = {2018},
	keywords = {SDC - tools, US Census, disclosure control},
}

@article{barth-jones_re-identification_2012,
	title = {The '{Re}-{Identification}' of {Governor} {William} {Weld}'s {Medical} {Information}: {A} {Critical} {Re}-{Examination} of {Health} {Data} {Identification} {Risks} and {Privacy} {Protections}, {Then} and {Now}},
	issn = {1556-5068},
	shorttitle = {The '{Re}-{Identification}' of {Governor} {William} {Weld}'s {Medical} {Information}},
	url = {http://www.ssrn.com/abstract=2076397},
	doi = {10.2139/ssrn.2076397},
	language = {en},
	urldate = {2023-06-21},
	journal = {SSRN Electronic Journal},
	author = {Barth-Jones, Daniel C.},
	year = {2012},
	keywords = {Re-identification risk},
}

@misc{abowd_declaration_2021,
	title = {Declaration of {John} {Abowd}, {State} of {Alabama} v. {United} {States} {Department} of {Commerce}. {Case} {No}. 3:21-{CV}-211-{RAH}-{ECM}-{KCN}},
	url = {https://vhdshf2oms2wcnsvk7sdv3so.blob.core.windows.net/thearp-media/documents/Declaration_of_John_M._Abowd.pdf},
	author = {Abowd, John M.},
	month = apr,
	year = {2021},
	keywords = {Differential Privacy, Re-identification risk, US Census},
}

@incollection{domingo-ferrer_safely_2018,
	address = {Cham},
	title = {Safely {Plotting} {Continuous} {Variables} on a {Map}},
	volume = {11126},
	isbn = {978-3-319-99770-4 978-3-319-99771-1},
	url = {http://link.springer.com/10.1007/978-3-319-99771-1_23},
	urldate = {2023-06-06},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {De Wolf, Peter-Paul and De Jonge, Edwin},
	editor = {Domingo-Ferrer, Josep and Montes, Francisco},
	year = {2018},
	doi = {10.1007/978-3-319-99771-1_23},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {SDC - geo referenced data, SDC - tools},
	pages = {347--359},
}

@article{bach_differential_2022,
	title = {Differential {Privacy} and {Noisy} {Confidentiality} {Concepts} for {European} {Population} {Statistics}},
	volume = {10},
	issn = {2325-0984, 2325-0992},
	url = {https://academic.oup.com/jssam/article/10/3/642/6484459},
	doi = {10.1093/jssam/smab044},
	abstract = {Abstract
            The article discusses various approaches to statistical disclosure control based on random noise that are currently being discussed for official population statistics and censuses. A particular focus is on a stringent delineation between different concepts influencing the discussion: we separate clearly between risk measures, noise distributions, and output mechanisms—putting these concepts into scope and into relation with each other. The article also remarks on utility and risk aspects of some specific output mechanisms and parameter setups, with special attention on static outputs that are rather typical in official population statistics. In particular, it is argued that unbounded noise distributions, such as plain Laplace, may jeopardize key unique census features without a clear need from a risk perspective. On the other hand, bounded noise distributions, such as the truncated Laplace or the cell key method, can contribute effectively to safeguarding these unique census features while controlling disclosure risks in census-like outputs. Finally, the article analyses some typical attack scenarios to constrain generic noise parameter ranges that suggest a good risk/utility compromise for the 2021 EU census output scenario. The analysis also shows that strictly differentially private mechanisms would be severely constrained in this scenario.},
	language = {en},
	number = {3},
	urldate = {2023-06-05},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Bach, Fabian},
	month = jun,
	year = {2022},
	pages = {642--687},
}

@techreport{longhurst_statistical_nodate,
	title = {Statistical {Disclosure} {Control} for the 2011 {UK} {Census}},
	url = {https://www.ons.gov.uk/file?uri=/census/2011census/howourcensusworks/howwetookthe2011census/howweplannedfordatadelivery/protectingconfidentialitywithstatisticaldisclosurecontrol/statistical-disclosure-control-for-the-2011-uk-census_tcm77-189747.pdf},
	abstract = {Présentation de la démarche utilisée par l'ONS UK pour protéger les données du Census. Les auteurs présentent en particulier le Target Record Swapping utilisé.},
	language = {en},
	institution = {Office for National Statistics (ONS - UK)},
	author = {Longhurst, Jane and Tromans, Nicola and Young, Caroline},
	pages = {26},
}

@article{dalenius_data-swapping_1982-1,
	title = {Data-swapping: {A} technique for disclosure control},
	volume = {6},
	issn = {03783758},
	shorttitle = {Data-swapping},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0378375882900581},
	doi = {10.1016/0378-3758(82)90058-1},
	language = {en},
	number = {1},
	urldate = {2023-05-30},
	journal = {Journal of Statistical Planning and Inference},
	author = {Dalenius, Tore and Reiss, Steven P.},
	year = {1982},
	keywords = {SDC - tools},
	pages = {73--85},
}

@inproceedings{gouweleeuw_post_1997,
	title = {Post randomisation for statistical disclosure control: {Theory} and implementation},
	shorttitle = {Post randomisation for statistical disclosure control},
	url = {https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/post-randomisation-for-statistical-disclosure-control-theory-and-implementation.pdf},
	abstract = {This article introduces the Post RAndomisation Method (PRAM) as a method for disclosure protection of the categorical variables in a microdata ®le. Applying PRAM means that for each record in a microdata ®le the score on one or more categorical variables is changed (independently of the other records) according to a predetermined probability mechanism. Since the original data ®le is perturbed, it will be dif®cult for an intruder to identify records as corresponding to certain individuals in the population. The records in the original ®le are thus protected, which is the main goal of applying PRAM. On the other hand, since the probability mechanism that is used when applying PRAM is known, characteristics of the (latent) true data can be estimated from the perturbed data ®le. Hence it is still possible to perform all kinds of statistical analyses after PRAM has been applied. Originally we developed PRAM as the categorical variable analogon of noise addition to continuous variables; see e.g., Fuller (1993), Hwang (1986), and Kim and Winkler (1995). Only after we had developed most of the theory did we become aware of the obvious relationship of our method with the randomised response technique applied in survey sampling; see e.g., Warner (1965, 1971) and Chaudhuri and Mukerjee (1988). This method is employed in the case of highly sensitive questions to which the respondent is not likely to respond truthfully in a face-to-face setting. By embedding the question in The Post RAndomisation Method (PRAM) is a perturbative method for disclosure protection of categorical variables. Applying PRAM means that for each record in a microdata ®le the score on a number of variables is changed according to a speci®ed probability mechanism. This article considers the effect of PRAM on both the safety of the data and the statistical quality of the data. When applying PRAM in practice, a number of decisions have to be made, as for example to which variables and in what way to apply PRAM. These issues are brie ̄y discussed in this article. As an example, the result of an investigation performed at Statistics Netherlands into the possibility of protecting the Dutch National Travel Survey using PRAM is presented.},
	urldate = {2023-05-30},
	author = {Gouweleeuw, J. and Kooiman, P. and Willenborg, L. and De-wolf, P. P.},
	year = {1997},
	keywords = {SDC - tools},
}

@article{elhassan_transfer_2008,
	title = {Transfer of pectoralis major for the treatment of irreparable tears of subscapularis: does it work?},
	volume = {90},
	issn = {2044-5377},
	shorttitle = {Transfer of pectoralis major for the treatment of irreparable tears of subscapularis},
	doi = {10.1302/0301-620X.90B8.20659},
	abstract = {Transfer of pectoralis major has evolved as the most favoured option for the management of the difficult problem of irreparable tears of subscapularis. We describe our experience with this technique in 30 patients divided into three groups. Group I comprised 11 patients with a failed procedure for instability of the shoulder, group II included eight with a failed shoulder replacement and group III, 11 with a massive tear of the rotator cuff. All underwent transfer of the sternal head of pectoralis major to restore the function of subscapularis. At the latest follow-up pain had improved in seven of the 11 patients in groups I and III, but in only one of eight in group II. The subjective shoulder score improved in seven patients in group I, in one in group II and in six in group III. The mean Constant score improved from 40.9 points (28 to 50) in group I, 32.9 (17 to 47) in group II and 28.7 (20 to 42) in group III pre-operatively to 60.8 (28 to 89), 41.9 (24 to 73) and 52.3 (24 to 78), respectively. Failure of the tendon transfer was highest in group II and was associated with pre-operative anterior subluxation of the humeral head. We conclude that in patients with irreparable rupture of subscapularis after shoulder replacement there is a high risk of failure of transfer of pectoralis major, particularly if there is pre-operative anterior subluxation of the humeral head.},
	language = {eng},
	number = {8},
	journal = {The Journal of Bone and Joint Surgery. British Volume},
	author = {Elhassan, B. and Ozbaydar, M. and Massimini, D. and Diller, D. and Higgins, L. and Warner, J. J. P.},
	month = aug,
	year = {2008},
	pmid = {18669963},
	keywords = {Adolescent, Adult, Analysis of Variance, Arthroplasty, Replacement, Female, Follow-Up Studies, Humans, Joint Instability, Male, Middle Aged, Pectoralis Muscles, Range of Motion, Articular, Recovery of Function, Rotator Cuff, Rotator Cuff Injuries, Shoulder, Shoulder Injuries, Shoulder Joint, Tendon Transfer, Trauma Severity Indices, Treatment Outcome},
	pages = {1059--1065},
}

@misc{cohen_attacks_2022,
	title = {Attacks on {Deidentification}'s {Defenses}},
	url = {http://arxiv.org/abs/2202.13470},
	abstract = {Quasi-identifier-based deidentification techniques (QI-deidentification) are widely used in practice, including \$k\$-anonymity, \${\textbackslash}ell\$-diversity, and \$t\$-closeness. We present three new attacks on QI-deidentification: two theoretical attacks and one practical attack on a real dataset. In contrast to prior work, our theoretical attacks work even if every attribute is a quasi-identifier. Hence, they apply to \$k\$-anonymity, \${\textbackslash}ell\$-diversity, \$t\$-closeness, and most other QI-deidentification techniques. First, we introduce a new class of privacy attacks called downcoding attacks, and prove that every QI-deidentification scheme is vulnerable to downcoding attacks if it is minimal and hierarchical. Second, we convert the downcoding attacks into powerful predicate singling-out (PSO) attacks, which were recently proposed as a way to demonstrate that a privacy mechanism fails to legally anonymize under Europe's General Data Protection Regulation. Third, we use LinkedIn.com to reidentify 3 students in a \$k\$-anonymized dataset published by EdX (and show thousands are potentially vulnerable), undermining EdX's claimed compliance with the Family Educational Rights and Privacy Act. The significance of this work is both scientific and political. Our theoretical attacks demonstrate that QI-deidentification may offer no protection even if every attribute is treated as a quasi-identifier. Our practical attack demonstrates that even deidentification experts acting in accordance with strict privacy regulations fail to prevent real-world reidentification. Together, they rebut a foundational tenet of QI-deidentification and challenge the actual arguments made to justify the continued use of \$k\$-anonymity and other QI-deidentification techniques.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Cohen, Aloni},
	month = feb,
	year = {2022},
	note = {arXiv:2202.13470 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security, Re-identification risk},
}

@inproceedings{endres_synthetic_2022,
	address = {Budapest Hungary},
	title = {Synthetic {Data} {Generation}: {A} {Comparative} {Study}},
	isbn = {978-1-4503-9709-4},
	shorttitle = {Synthetic {Data} {Generation}},
	url = {https://dl.acm.org/doi/10.1145/3548785.3548793},
	doi = {10.1145/3548785.3548793},
	abstract = {Generating synthetic data similar to realistic data is a crucial task
in data augmentation and data production. Due to the preservation
of authentic data distribution, synthetic data provide concealment
of sensitive information and therefore enable Big Data acquisition
for model training without facing privacy challenges. Nevertheless,
the obstacles arise starting with acquiring real-world open-source
data to effectively synthesizing new samples as genuine as possible.
In this paper, a comparative study is conducted by considering the
efficacy of different generative models like Generative Adversarial
Network (GAN), Variational Autoencoder (VAE), Synthetic Minority
Oversampling Technique (SMOTE), Data Synthesizer (DS), Synthetic
Data Vault with Gaussian Copula (SDV-G), Conditional Generative
Adversarial Networks (SDV-GAN), and SynthPop Non-Parametric
(SP-NP) approach to synthesize data with regard to various datasets.
We used the pairwise correlation and Synthetic Data (SD) metrics
as utility measures respectively between real data and generated
data for evaluation. Accordingly, this paper investigates the effects
of various data generation models, and the processing time of every
model is included as one of the evaluation metrics.},
	language = {en},
	urldate = {2023-05-10},
	booktitle = {International {Database} {Engineered} {Applications} {Symposium}},
	publisher = {ACM},
	author = {Endres, Markus and Mannarapotta Venugopal, Asha and Tran, Tung Son},
	month = aug,
	year = {2022},
	keywords = {Deep learning, Synthetic data},
	pages = {94--102},
}

@article{garfinkel_differential_2022,
	title = {Differential {Privacy} and the 2020 {US} {Census}},
	url = {https://mit-serc.pubpub.org/pub/differential-privacy-2020-us-census},
	doi = {10.21428/2c646de5.7ec6ab93},
	language = {en},
	number = {Winter 2022},
	urldate = {2023-05-09},
	journal = {MIT Case Studies in Social and Ethical Responsibilities of Computing},
	author = {Garfinkel, Simson},
	month = jan,
	year = {2022},
	keywords = {Differential Privacy, US Census},
}

@article{bowen_comparative_2020,
	title = {Comparative {Study} of {Differentially} {Private} {Data} {Synthesis} {Methods}},
	volume = {35},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-35/issue-2/Comparative-Study-of-Differentially-Private-Data-Synthesis-Methods/10.1214/19-STS742.full},
	doi = {10.1214/19-STS742},
	number = {2},
	urldate = {2023-05-09},
	journal = {Statistical Science},
	author = {Bowen, Claire McKay and Liu, Fang},
	month = may,
	year = {2020},
	keywords = {Differential Privacy, Synthetic data, risk-utility trade-off},
}

@incollection{domingo-ferrer_utility_2022,
	address = {Cham},
	title = {Utility and {Disclosure} {Risk} for {Differentially} {Private} {Synthetic} {Categorical} {Data}},
	volume = {13463},
	isbn = {978-3-031-13944-4 978-3-031-13945-1},
	url = {https://link.springer.com/10.1007/978-3-031-13945-1_18},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Raab, Gillian M.},
	editor = {Domingo-Ferrer, Josep and Laurent, Maryline},
	year = {2022},
	doi = {10.1007/978-3-031-13945-1_18},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Synthetic data, US Census, risk-utility trade-off},
	pages = {250--265},
}

@incollection{domingo-ferrer_synthetic_2022,
	address = {Cham},
	title = {Synthetic {Individual} {Income} {Tax} {Data}: {Methodology}, {Utility}, and {Privacy} {Implications}},
	volume = {13463},
	isbn = {978-3-031-13944-4 978-3-031-13945-1},
	shorttitle = {Synthetic {Individual} {Income} {Tax} {Data}},
	url = {https://link.springer.com/10.1007/978-3-031-13945-1_14},
	language = {en},
	urldate = {2023-05-09},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Bowen, Claire McKay and Bryant, Victoria and Burman, Leonard and Czajka, John and Khitatrakun, Surachai and MacDonald, Graham and McClelland, Robert and Mucciolo, Livia and Pickens, Madeline and Ueyama, Kyle and Williams, Aaron R. and Wissoker, Doug and Zwiefel, Noah},
	editor = {Domingo-Ferrer, Josep and Laurent, Maryline},
	year = {2022},
	doi = {10.1007/978-3-031-13945-1_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {191--204},
}

@article{rubin_statistical_1993,
	title = {Statistical {Disclosure} {Limitation}},
	volume = {9},
	url = {https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/discussion-statistical-disclosure-limitation2.pdf},
	language = {en},
	number = {2},
	journal = {Journal of Official Statistics},
	author = {Rubin, Donald},
	year = {1993},
	keywords = {SDC - general views, Synthetic data},
	pages = {461--468},
}

@article{reiter_releasing_2005,
	title = {Releasing {Multiply} {Imputed}, {Synthetic} {Public} use {Microdata}: {An} {Illustration} and {Empirical} {Study}},
	volume = {168},
	issn = {0964-1998, 1467-985X},
	shorttitle = {Releasing {Multiply} {Imputed}, {Synthetic} {Public} use {Microdata}},
	url = {https://academic.oup.com/jrsssa/article/168/1/185/7084130},
	doi = {10.1111/j.1467-985X.2004.00343.x},
	abstract = {Summary
            The paper presents an illustration and empirical study of releasing multiply imputed, fully synthetic public use microdata. Simulations based on data from the US Current Population Survey are used to evaluate the potential validity of inferences based on fully synthetic data for a variety of descriptive and analytic estimands, to assess the degree of protection of confidentiality that is afforded by fully synthetic data and to illustrate the specification of synthetic data imputation models. Benefits and limitations of releasing fully synthetic data sets are discussed.},
	language = {en},
	number = {1},
	urldate = {2023-05-09},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Reiter, Jerome P.},
	month = jan,
	year = {2005},
	keywords = {Synthetic data},
	pages = {185--205},
}

@article{shlomo_statistical_2007,
	title = {Statistical {Disclosure} {Control} {Methods} for {Census} {Frequency} {Tables}},
	volume = {75},
	issn = {0306-7734, 1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1751-5823.2007.00010.x},
	doi = {10.1111/j.1751-5823.2007.00010.x},
	language = {en},
	number = {2},
	urldate = {2023-05-03},
	journal = {International Statistical Review},
	author = {Shlomo, Natalie},
	month = aug,
	year = {2007},
	keywords = {SDC - frequency tables, SDC - general views, risk-utility trade-off},
	pages = {199--217},
}

@article{savage_synthetic_2023,
	title = {Synthetic data could be better than real data},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/d41586-023-01445-8},
	doi = {10.1038/d41586-023-01445-8},
	language = {en},
	urldate = {2023-05-03},
	journal = {Nature},
	author = {Savage, Neil},
	month = apr,
	year = {2023},
	keywords = {Synthetic data},
	pages = {d41586--023--01445--8},
}

@inproceedings{endres_synthetic_2022-1,
	address = {Budapest Hungary},
	title = {Synthetic {Data} {Generation}: {A} {Comparative} {Study}},
	isbn = {978-1-4503-9709-4},
	shorttitle = {Synthetic {Data} {Generation}},
	url = {https://dl.acm.org/doi/10.1145/3548785.3548793},
	doi = {10.1145/3548785.3548793},
	language = {en},
	urldate = {2023-04-04},
	booktitle = {International {Database} {Engineered} {Applications} {Symposium}},
	publisher = {ACM},
	author = {Endres, Markus and Mannarapotta Venugopal, Asha and Tran, Tung Son},
	month = aug,
	year = {2022},
	pages = {94--102},
}

@inproceedings{muralidhar_database_2021,
	address = {Poland},
	title = {Database reconstruction is very difficult in practice.},
	abstract = {The U.S. Census Bureau has motivated the use of differential privacy to protect the outputs of the 2020
Decennial Census by highlighting the dangers of reconstruction attacks (see Garfinkel, Abowd and Martindale
(2019) "Understanding database reconstruction attacks on public data", Communications of the ACM,
62(3):46-53). We examine in detail the running example in that paper and we conclude it reveals quite the
opposite: database reconstruction appears to be very difficult even for very small databases if classical
statistical disclosure control techniques are properly applied (e.g. complementary cell suppression). In contrast,
the use of differential privacy entails a very large utility loss even when the parameter epsilon is chosen to be as
large as 10 (in which case practically no privacy is achieved},
	language = {en},
	booktitle = {{UNECE} - {Expert} {Meeting} on {Statistical} {Data} {Confidentiality}},
	author = {Muralidhar, Krishnamurty},
	month = dec,
	year = {2021},
}

@article{dwork_calibrating_2017,
	title = {Calibrating {Noise} to {Sensitivity} in {Private} {Data} {Analysis}},
	volume = {7},
	issn = {2575-8527},
	url = {https://journalprivacyconfidentiality.org/index.php/jpc/article/view/405},
	doi = {10.29012/jpc.v7i3.405},
	abstract = {We continue a line of research initiated in Dinur and Nissim (2003); Dwork and Nissim (2004); and Blum et al. (2005) on privacy-preserving statistical databases. 
Consider a trusted server that holds a database of sensitive information. Given a query function \$f\$ mapping databases to reals, the so-called \{{\textbackslash}em true answer\} is the result of applying \$f\$ to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user. 
Previous work focused on the case of noisy sums, in which \$f = {\textbackslash}sum\_i g(x\_i)\$, where \$x\_i\$ denotes the \$i\$th row of the database and \$g\$ maps database rows to \$[0,1]\$. We extend the study to general functions \$f\$, proving that privacy can be preserved by calibrating the standard deviation of the noise according to the \{{\textbackslash}em sensitivity\} of the function \$f\$. Roughly speaking, this is the amount that any single argument to \$f\$ can change its output. The new analysis shows that for several particular applications substantially less noise is needed than was previously understood to be the case. 
The first step is a very clean definition of privacy---now known as differential privacy---and measure of its loss. We also provide a set of tools for designing and combining differentially private algorithms, permitting the construction of complex differentially private analytical tools from simple differentially private primitives. 
Finally, we obtain separation results showing the increased value of interactive statistical release mechanisms over non-interactive ones.},
	number = {3},
	urldate = {2023-04-04},
	journal = {Journal of Privacy and Confidentiality},
	author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
	month = may,
	year = {2017},
	keywords = {Differential Privacy},
	pages = {17--51},
}

@article{dwork_differential_2019,
	title = {Differential {Privacy} in {Practice}: {Expose} your {Epsilons}!},
	volume = {9},
	issn = {2575-8527},
	shorttitle = {Differential {Privacy} in {Practice}},
	url = {https://journalprivacyconfidentiality.org/index.php/jpc/article/view/689},
	doi = {10.29012/jpc.689},
	abstract = {Differential privacy is at a turning point. Implementations have been successfully leveraged in private industry, the public sector, and academia in a wide variety of applications, allowing scientists, engineers, and researchers the ability to learn about populations of interest without specifically learning about these individuals. Because differential privacy allows us to quantify cumulative privacy loss, these differentially private systems will, for the first time, allow us to measure and compare the total privacy loss due to these personal data-intensive activities. Appropriately leveraged, this could be a watershed moment for privacy. 
Like other technologies and techniques that allow for a range of instantiations, implementation details matter. When meaningfully implemented, differential privacy supports deep data-driven insights with minimal worst-case privacy loss. When not meaningfully implemented, differential privacy delivers privacy mostly in name. Using differential privacy to maximize learning while providing a meaningful degree of privacy requires judicious choices with respect to the privacy parameter epsilon, among other factors. However, there is little understanding of what is the optimal value of epsilon for a given system or classes of systems/purposes/data etc. or how to go about figuring it out. 
To understand current differential privacy implementations and how organizations make these key choices in practice, we conducted interviews with practitioners to learn from their experiences of implementing differential privacy. We found no clear consensus on how to choose epsilon, nor is there agreement on how to approach this and other key implementation decisions. Given the importance of these implementation details there is a need for shared learning amongst the differential privacy community. To serve these purposes, we propose the creation of the Epsilon Registry—a publicly available communal body of knowledge about differential privacy implementations that can be used by various stakeholders to drive the identification and adoption of judicious differentially private implementations.},
	number = {2},
	urldate = {2023-04-04},
	journal = {Journal of Privacy and Confidentiality},
	author = {Dwork, Cynthia and Kohli, Nitin and Mulligan, Deirdre},
	month = oct,
	year = {2019},
	keywords = {Differential Privacy},
}

@article{dwork_firm_2011,
	title = {A firm foundation for private data analysis},
	volume = {54},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1866739.1866758},
	doi = {10.1145/1866739.1866758},
	abstract = {What does it mean to preserve privacy?},
	language = {en},
	number = {1},
	urldate = {2023-04-04},
	journal = {Communications of the ACM},
	author = {Dwork, Cynthia},
	month = jan,
	year = {2011},
	keywords = {Differential Privacy},
	pages = {86--95},
}

@article{ruggles_role_2022,
	title = {The {Role} of {Chance} in the {Census} {Bureau} {Database} {Reconstruction} {Experiment}},
	volume = {41},
	issn = {0167-5923, 1573-7829},
	url = {https://link.springer.com/10.1007/s11113-021-09674-3},
	doi = {10.1007/s11113-021-09674-3},
	abstract = {Abstract
            The Census Bureau plans a new approach to disclosure control for the 2020 census that will add noise to every statistic the agency produces for places below the state level. The Bureau argues the new approach is needed because the confidentiality of census responses is threatened by “database reconstruction,” a technique for inferring individual-level responses from tabular data. The Census Bureau constructed hypothetical individual-level census responses from public 2010 tabular data and matched them to internal census records and to outside sources. The Census Bureau did not compare these results to a null model to demonstrate that their success in matching would not be expected by chance. This is analogous to conducting a clinical trial without a control group. We implement a simple simulation to assess how many matches would be expected by chance. We demonstrate that most matches reported by the Census Bureau experiment would be expected randomly. To extend the metaphor of the clinical trial, the treatment and the placebo produced similar outcomes. The database reconstruction experiment therefore fails to demonstrate a credible threat to confidentiality.},
	language = {en},
	number = {3},
	urldate = {2023-03-30},
	journal = {Population Research and Policy Review},
	author = {Ruggles, Steven and Van Riper, David},
	month = jun,
	year = {2022},
	keywords = {Differential Privacy, Re-identification risk, US Census},
	pages = {781--788},
}

@article{bach_differential_2022-1,
	title = {Differential {Privacy} and {Noisy} {Confidentiality} {Concepts} for {European} {Population} {Statistics}},
	volume = {10},
	issn = {2325-0984, 2325-0992},
	url = {https://academic.oup.com/jssam/article/10/3/642/6484459},
	doi = {10.1093/jssam/smab044},
	abstract = {Abstract
            The article discusses various approaches to statistical disclosure control based on random noise that are currently being discussed for official population statistics and censuses. A particular focus is on a stringent delineation between different concepts influencing the discussion: we separate clearly between risk measures, noise distributions, and output mechanisms—putting these concepts into scope and into relation with each other. The article also remarks on utility and risk aspects of some specific output mechanisms and parameter setups, with special attention on static outputs that are rather typical in official population statistics. In particular, it is argued that unbounded noise distributions, such as plain Laplace, may jeopardize key unique census features without a clear need from a risk perspective. On the other hand, bounded noise distributions, such as the truncated Laplace or the cell key method, can contribute effectively to safeguarding these unique census features while controlling disclosure risks in census-like outputs. Finally, the article analyses some typical attack scenarios to constrain generic noise parameter ranges that suggest a good risk/utility compromise for the 2021 EU census output scenario. The analysis also shows that strictly differentially private mechanisms would be severely constrained in this scenario.},
	language = {en},
	number = {3},
	urldate = {2023-03-30},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Bach, Fabian},
	month = jun,
	year = {2022},
	keywords = {Differential Privacy, risk-utility trade-off},
	pages = {642--687},
}

@inproceedings{dinur_revealing_2003,
	address = {San Diego California},
	title = {Revealing information while preserving privacy},
	isbn = {978-1-58113-670-8},
	url = {https://dl.acm.org/doi/10.1145/773153.773173},
	doi = {10.1145/773153.773173},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the twenty-second {ACM} {SIGMOD}-{SIGACT}-{SIGART} symposium on {Principles} of database systems},
	publisher = {ACM},
	author = {Dinur, Irit and Nissim, Kobbi},
	month = jun,
	year = {2003},
	keywords = {Differential Privacy, Re-identification risk},
	pages = {202--210},
}

@book{european_commission_statistical_office_of_the_european_union_automatic_2021,
	address = {LU},
	title = {Automatic {Checking} of {Research} {Outputs} ({ACRO}): a tool for dynamic disclosure checks : 2021 edition.},
	shorttitle = {Automatic {Checking} of {Research} {Outputs} ({ACRO})},
	url = {https://data.europa.eu/doi/10.2785/75954},
	abstract = {Checking research outputs for disclosure risk is commonly carried out by statistical agencies and other managers of secure facilities. This can be a time-consuming task, requiring skilled staff. This paper discusses the development of an automatic tool for the statistical disclosure control (SDC) of research outputs. The purpose of the tool (ACRO, for Automatic Checking of Research Outputs) is
to distinguish between research output that is safe to publish, output that requires further analysis and output that cannot be published because of substantial disclosure risk.
In this document we review the problem and design goals, and describe the implementation of the tool as a proof-of-concept. The prototype tool is only able to address simple problems; but as these constitute both the biggest risk and the most time-consuming outputs to check, there is significant potential to reduce resource costs for data holders.
We also reflect on the broader lessons learned about the desirability of an automatic tool; in particular, the need for acceptance of the “good-enough-ness” quality of a tool. That is, that the tool reduces the burden to the researcher and/or to the data holder who has to certify that the output is safe to publish. There are clear dividing lines between the statistical problems where the cost-benefit argument supports automation, and where it does not.},
	language = {eng},
	urldate = {2023-03-21},
	publisher = {Publications Office},
	author = {{European Commission. Statistical Office of the European Union.}},
	year = {2021},
}

@incollection{kanade_data_2004,
	address = {Berlin, Heidelberg},
	title = {Data {Swapping}: {Variations} on a {Theme} by {Dalenius} and {Reiss}},
	volume = {3050},
	isbn = {978-3-540-22118-0 978-3-540-25955-8},
	shorttitle = {Data {Swapping}},
	url = {http://link.springer.com/10.1007/978-3-540-25955-8_2},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fienberg, Stephen E. and McIntyre, Julie},
	editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Domingo-Ferrer, Josep and Torra, Vicenç},
	year = {2004},
	doi = {10.1007/978-3-540-25955-8_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {14--29},
}

@incollection{domingo-ferrer_how_2008,
	address = {Berlin, Heidelberg},
	title = {How {Protective} {Are} {Synthetic} {Data}?},
	volume = {5262},
	isbn = {978-3-540-87470-6 978-3-540-87471-3},
	url = {http://link.springer.com/10.1007/978-3-540-87471-3_20},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Abowd, John M. and Vilhuber, Lars},
	editor = {Domingo-Ferrer, Josep and Saygın, Yücel},
	year = {2008},
	doi = {10.1007/978-3-540-87471-3_20},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	keywords = {Synthetic data},
	pages = {239--246},
}

@article{bolon-canedo_review_2013,
	title = {A review of feature selection methods on synthetic data},
	volume = {34},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-012-0487-8},
	doi = {10.1007/s10115-012-0487-8},
	language = {en},
	number = {3},
	urldate = {2023-03-21},
	journal = {Knowledge and Information Systems},
	author = {Bolón-Canedo, Verónica and Sánchez-Maroño, Noelia and Alonso-Betanzos, Amparo},
	month = mar,
	year = {2013},
	keywords = {Synthetic data},
	pages = {483--519},
}

@article{nowok_synthpop_2016,
	title = {\textbf{synthpop} : {Bespoke} {Creation} of {Synthetic} {Data} in \textit{{R}}},
	volume = {74},
	issn = {1548-7660},
	shorttitle = {\textbf{synthpop}},
	url = {http://www.jstatsoft.org/v74/i11/},
	doi = {10.18637/jss.v074.i11},
	language = {en},
	number = {11},
	urldate = {2023-03-21},
	journal = {Journal of Statistical Software},
	author = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
	year = {2016},
	keywords = {SDC - tools, Synthetic data},
}

@article{nowok_synthpop_2016-1,
	title = {\textbf{synthpop} : {Bespoke} {Creation} of {Synthetic} {Data} in \textit{{R}}},
	volume = {74},
	issn = {1548-7660},
	shorttitle = {\textbf{synthpop}},
	url = {http://www.jstatsoft.org/v74/i11/},
	doi = {10.18637/jss.v074.i11},
	language = {en},
	number = {11},
	urldate = {2023-03-21},
	journal = {Journal of Statistical Software},
	author = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
	year = {2016},
}

@misc{holohan_kepsilon-anonymity_2017,
	title = {(\$k\$,\${\textbackslash}epsilon\$)-{Anonymity}: \$k\$-{Anonymity} with \${\textbackslash}epsilon\$-{Differential} {Privacy}},
	shorttitle = {(\$k\$,\${\textbackslash}epsilon\$)-{Anonymity}},
	url = {http://arxiv.org/abs/1710.01615},
	abstract = {The explosion in volume and variety of data offers enormous potential for research and commercial use. Increased availability of personal data is of particular interest in enabling highly customised services tuned to individual needs. Preserving the privacy of individuals against reidentification attacks in this fast-moving ecosystem poses significant challenges for a one-size fits all approach to anonymisation. In this paper we present (\$k\$,\${\textbackslash}epsilon\$)-anonymisation, an approach that combines the \$k\$-anonymisation and \${\textbackslash}epsilon\$-differential privacy models into a single coherent framework, providing privacy guarantees at least as strong as those offered by the individual models. Linking risks of less than 5{\textbackslash}\% are observed in experimental results, even with modest values of \$k\$ and \${\textbackslash}epsilon\$. Our approach is shown to address well-known limitations of \$k\$-anonymity and \${\textbackslash}epsilon\$-differential privacy and is validated in an extensive experimental campaign using openly available datasets.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Holohan, Naoise and Antonatos, Spiros and Braghin, Stefano and Mac Aonghusa, Pól},
	month = oct,
	year = {2017},
	note = {arXiv:1710.01615 [cs, math]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Databases, Differential Privacy, Mathematics - Probability},
}

@misc{domingo-ferrer_limits_2020,
	title = {The {Limits} of {Differential} {Privacy} (and its {Misuse} in {Data} {Release} and {Machine} {Learning})},
	url = {http://arxiv.org/abs/2011.02352},
	abstract = {Differential privacy (DP) is a neat privacy definition that can co-exist with certain well-defined data uses in the context of interactive queries. However, DP is neither a silver bullet for all privacy problems nor a replacement for all previous privacy models. In fact, extreme care should be exercised when trying to extend its use beyond the setting it was designed for. This paper reviews the limitations of DP and its misuse for individual data collection, individual data release, and machine learning.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Domingo-Ferrer, Josep and Sánchez, David and Blanco-Justicia, Alberto},
	month = nov,
	year = {2020},
	note = {arXiv:2011.02352 [cs]},
	keywords = {Computer Science - Cryptography and Security, Differential Privacy},
}

@misc{muralidhar_database_2023,
	title = {Database {Reconstruction} {Is} {Not} {So} {Easy} and {Is} {Different} from {Reidentification}},
	url = {http://arxiv.org/abs/2301.10213},
	abstract = {In recent years, it has been claimed that releasing accurate statistical information on a database is likely to allow its complete reconstruction. Differential privacy has been suggested as the appropriate methodology to prevent these attacks. These claims have recently been taken very seriously by the U.S. Census Bureau and led them to adopt differential privacy for releasing U.S. Census data. This in turn has caused consternation among users of the Census data due to the lack of accuracy of the protected outputs. It has also brought legal action against the U.S. Department of Commerce. In this paper, we trace the origins of the claim that releasing information on a database automatically makes it vulnerable to being exposed by reconstruction attacks and we show that this claim is, in fact, incorrect. We also show that reconstruction can be averted by properly using traditional statistical disclosure control (SDC) techniques. We further show that the geographic level at which exact counts are released is even more relevant to protection than the actual SDC method employed. Finally, we caution against confusing reconstruction and reidentification: using the quality of reconstruction as a metric of reidentification results in exaggerated reidentification risk figures.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Muralidhar, Krishnamurty and Domingo-Ferrer, Josep},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10213 [cs]},
	keywords = {68P27 Privacy of data, Computer Science - Cryptography and Security, Computer Science - Databases, G.3, H.2, Re-identification risk, US Census},
}

@article{templ_statistical_2015,
	title = {Statistical {Disclosure} {Control} for {Micro}-{Data} {Using} the \textit{{R}} {Package} \textbf{{sdcMicro}}},
	volume = {67},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v67/i04/},
	doi = {10.18637/jss.v067.i04},
	language = {en},
	number = {4},
	urldate = {2023-03-21},
	journal = {Journal of Statistical Software},
	author = {Templ, Matthias and Kowarik, Alexander and Meindl, Bernhard},
	year = {2015},
	keywords = {SDC - tools},
}

@article{noauthor_statistical_2018,
	title = {Statistical {Disclosure} {Limitation}: {New} {Directions} and {Challenges}},
	volume = {8},
	url = {https://discovery.researcher.life/article/statistical-disclosure-limitation-new-directions-and-challenges/7f1672244ffe3905a178e746e1a538d6},
	doi = {10.29012/jpc.v8i1},
	abstract = {An overview of traditional types of data dissemination at statistical agencies is provided including definitions of disclosure risks, the quantification of disclosure risk and data utility and common statistical disclosure limitation (SDL) methods. However, with technological advancements and the increasing push by governments for openand accessible data, new forms of data dissemination are currently being explored. We focus on web-based applications such as flexible table builders and remote analysis servers, synthetic data and remote access. Many of these applications introduce new challenges for statistical agencies as they are gradually relinquishing some of their control on what data is released. There is now more recognition of the need for perturbative methods to protect the confidentiality of data subjects. These new forms of data dissemination are changing the landscape of how disclosure risks are conceptualized and the types of SDL methods that need to be applied to protect thedata. In particular, inferential disclosure is the main disclosure risk of concern and encompasses the traditional types of disclosure risks based on identity and attribute disclosures. These challenges have led to statisticians exploring the computer science definition of differential privacy and privacy- by-design applications. We explore how differential privacy can be a useful addition to the current SDL framework within statistical agencies.},
	language = {en},
	urldate = {2023-03-17},
	journal = {Journal of Privacy and Confidentiality},
	month = dec,
	year = {2018},
	keywords = {SDC - general views},
}

@incollection{domingo-ferrer_membership_2022,
	address = {Cham},
	title = {Membership {Inference} {Attack} {Against} {Principal} {Component} {Analysis}},
	volume = {13463},
	isbn = {978-3-031-13944-4 978-3-031-13945-1},
	url = {https://link.springer.com/10.1007/978-3-031-13945-1_19},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Zari, Oualid and Parra-Arnau, Javier and Ünsal, Ayşe and Strufe, Thorsten and Önen, Melek},
	editor = {Domingo-Ferrer, Josep and Laurent, Maryline},
	year = {2022},
	doi = {10.1007/978-3-031-13945-1_19},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {risk-utility trade-off},
	pages = {269--282},
}

@article{sweeney_k-anonymity_2002,
	title = {k-{ANONYMITY}: {A} {MODEL} {FOR} {PROTECTING} {PRIVACY}},
	volume = {10},
	issn = {0218-4885, 1793-6411},
	shorttitle = {k-{ANONYMITY}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218488502001648},
	doi = {10.1142/S0218488502001648},
	abstract = {Consider a data holder, such as a hospital or a bank, that has a privately held collection of person-specific, field structured data. Suppose the data holder wants to share a version of the data with researchers. How can a data holder release a version of its private data with scientific guarantees that the individuals who are the subjects of the data cannot be re-identified while the data remain practically useful? The solution provided in this paper includes a formal protection model named k-anonymity and a set of accompanying policies for deployment. A release provides k-anonymity protection if the information for each person contained in the release cannot be distinguished from at least k-1 individuals whose information also appears in the release. This paper also examines re-identification attacks that can be realized on releases that adhere to k-anonymity unless accompanying policies are respected. The k-anonymity protection model is important because it forms the basis on which the real-world systems known as Datafly, μ-Argus and k-Similar provide guarantees of privacy protection.},
	language = {en},
	number = {05},
	urldate = {2023-03-16},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Sweeney, Latanya},
	month = oct,
	year = {2002},
	keywords = {risk-utility trade-off},
	pages = {557--570},
}

@article{snoke_pmse_2018,
	title = {{pMSE} {Mechanism}: {Differentially} {Private} {Synthetic} {Data} with {Maximal} {Distributional} {Similarity}},
	shorttitle = {{pMSE} {Mechanism}},
	url = {http://arxiv.org/abs/1805.09392},
	abstract = {We propose a method for the release of differentially private synthetic datasets. In many contexts, data contain sensitive values which cannot be released in their original form in order to protect individuals' privacy. Synthetic data is a protection method that releases alternative values in place of the original ones, and differential privacy (DP) is a formal guarantee for quantifying the privacy loss. We propose a method that maximizes the distributional similarity of the synthetic data relative to the original data using a measure known as the pMSE, while guaranteeing epsilon-differential privacy. Additionally, we relax common DP assumptions concerning the distribution and boundedness of the original data. We prove theoretical results for the privacy guarantee and provide simulations for the empirical failure rate of the theoretical results under typical computational limitations. We also give simulations for the accuracy of linear regression coefficients generated from the synthetic data compared with the accuracy of non-differentially private synthetic data and other differentially private methods. Additionally, our theoretical results extend a prior result for the sensitivity of the Gini Index to include continuous predictors.},
	urldate = {2021-10-13},
	journal = {arXiv:1805.09392 [stat]},
	author = {Snoke, Joshua and Slavković, Aleksandra},
	month = may,
	year = {2018},
	note = {arXiv: 1805.09392},
	keywords = {Differential Privacy, Synthetic data},
}

@article{slavkovic_statistical_2023,
	title = {Statistical {Data} {Privacy}: {A} {Song} of {Privacy} and {Utility}},
	volume = {10},
	url = {https://discovery.researcher.life/article/statistical-data-privacy-a-song-of-privacy-and-utility/487edf2b96b23a97b7f4c97cc6d4de99},
	doi = {10.1146/annurev-statistics-033121-112921},
	abstract = {To quantify trade-offs between increasing demand for open data sharing and concerns about sensitive information disclosure, statistical data privacy (SDP) methodology analyzes data release mechanisms that sanitize outputs based on confidential data. Two dominant frameworks exist: statistical disclosure control (SDC) and the more recent differential privacy (DP). Despite framing differences, both SDC and DP share the same statistical problems at their core. For inference problems, either we may design optimal release mechanisms and associated estimators that satisfy bounds on disclosure risk measures, or we may adjust existing sanitized output to create new statistically valid and optimal estimators. Regardless of design or adjustment, in evaluating risk and utility, valid statistical inferences from mechanism outputs require uncertainty quantification that accounts for the effect of the sanitization mechanism that introduces bias and/or variance. In this review, we discuss the statistical foundations common to both SDC and DP, highlight major developments in SDP, and present exciting open research problems in private inference.},
	language = {en},
	urldate = {2023-03-17},
	journal = {Annual Review of Statistics and Its Application},
	author = {Slavković, Aleksandra and Seeman, Jeremy},
	month = mar,
	year = {2023},
	keywords = {risk-utility trade-off},
}

@article{skinner_measuring_2022,
	title = {Measuring {Risk} of {Re}-{Identification} in {Microdata}: {State}-of-the {Art} and {New} {Directions}},
	volume = {185},
	url = {https://discovery.researcher.life/article/measuring-risk-of-re-identification-in-microdata-state-of-the-art-and-new-directions/3781cf8f41be398eaa5002a6cdb7c1ac},
	doi = {10.1111/rssa.12902},
	abstract = {Abstract
               We review the influential research carried out by Chris Skinner in the area of statistical disclosure control, and in particular quantifying the risk of re-identification in sample microdata from a random survey drawn from a finite population. We use the sample microdata to infer population parameters when the population is unknown, and estimate the risk of re-identification based on the notion of population uniqueness using probabilistic modelling. We also introduce a new approach to measure the risk of re-identification for a subpopulation in a register that is not representative of the general population, for example a register of cancer patients. In addition, we can use the additional information from the register to measure the risk of re-identification for the sample microdata. This new approach was developed by the two authors and is published here for the first time. We demonstrate this approach in an application study based on UK census data where we can compare the estimated risk measures to the known truth.},
	language = {en},
	urldate = {2023-03-17},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Skinner, Chris and Shlomo, Natalie},
	month = oct,
	year = {2022},
	keywords = {Re-identification risk},
}

@article{skinner_statistical_2012,
	title = {Statistical {Disclosure} {Risk}: {Separating} {Potential} and {Harm}: \textit{{Statistical} {Disclosure} {Risk}}},
	volume = {80},
	issn = {03067734},
	shorttitle = {Statistical {Disclosure} {Risk}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1751-5823.2012.00194.x},
	doi = {10.1111/j.1751-5823.2012.00194.x},
	language = {en},
	number = {3},
	urldate = {2023-03-16},
	journal = {International Statistical Review},
	author = {Skinner, Chris},
	month = dec,
	year = {2012},
	keywords = {SDC - general views},
	pages = {349--368},
}

@article{rinott_confidentiality_2018,
	title = {Confidentiality and {Differential} {Privacy} in the {Dissemination} of {Frequency} {Tables}},
	volume = {33},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-33/issue-3/Confidentiality-and-Differential-Privacy-in-the-Dissemination-of-Frequency-Tables/10.1214/17-STS641.full},
	doi = {10.1214/17-STS641},
	number = {3},
	urldate = {2021-09-03},
	journal = {Statistical Science},
	author = {Rinott, Yosef and O’Keefe, Christine M. and Shlomo, Natalie and Skinner, Chris},
	month = aug,
	year = {2018},
	keywords = {Differential Privacy, Synthetic data},
}

@misc{nguyen_techniques_2020,
	title = {Techniques d'anonymisation tabulaire : concepts et mise en oeuvre},
	shorttitle = {Techniques d'anonymisation tabulaire},
	url = {http://arxiv.org/abs/2001.02650},
	abstract = {In this document, we present a state of the art of anonymization techniques for classical tabular datasets. This article is geared towards a general public having some knowledge of mathematics and computer science, but with no need for specific knowledge in anonymization. The objective of this document it to explain anonymization concepts in order to be able to sanitize a dataset and compute reindentification risk. The document contains a large number of examples to help understand the calculations. ----- Dans ce document, nous pr{\textbackslash}'esentons l'{\textbackslash}'etat de l'art des techniques d'anonymisation pour des bases de donn{\textbackslash}'ees classiques (i.e. des tables), {\textbackslash}`a destination d'un public technique ayant une formation universitaire de base en math{\textbackslash}'ematiques et informatique, mais non sp{\textbackslash}'ecialiste. L'objectif de ce document est d'expliquer les concepts permettant de r{\textbackslash}'ealiser une anonymisation de donn{\textbackslash}'ees tabulaires, et de calculer les risques de r{\textbackslash}'eidentification. Le document est largement compos{\textbackslash}'e d'exemples permettant au lecteur de comprendre comment mettre en oeuvre les calculs.},
	urldate = {2023-03-17},
	publisher = {arXiv},
	author = {Nguyen, Benjamin and Castelluccia, Claude},
	month = jan,
	year = {2020},
	note = {arXiv:2001.02650 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security, Computer Science - Databases, SDC - general views},
}

@article{nguyen_techniques_2014,
	title = {Techniques d'anonymisation},
	volume = {2 (4)},
	url = {https://hal.science/hal-01113412},
	abstract = {L’opposition entre une donnée qui permet d’identifier une personne et
une donnée anonyme n’est pas une opposition absolue. C’est pourquoi il
existe plusieurs méthodes d’anonymisation, plus ou moins efficaces. On
utilise souvent aujourd’hui la « k-anonymisation », la « l-diversité », ou la « confidentialité différentielle », trois techniques dont les principes sont donnés dans cet article. Les différentes techniques sont à juger à la fois sur la sécurité qu’elles procurent, et sur ce  u’elles laissent subsister comme analyses possibles.},
	language = {fr},
	urldate = {2023-03-16},
	journal = {Statistiques et Société},
	author = {Nguyen, Benjamin},
	year = {2014},
	keywords = {SDC - general views},
	pages = {53--60},
}

@incollection{domingo-ferrer_re-examination_2022,
	address = {Cham},
	title = {A {Re}-examination of the {Census} {Bureau} {Reconstruction} and {Reidentification} {Attack}},
	volume = {13463},
	isbn = {978-3-031-13944-4 978-3-031-13945-1},
	url = {https://link.springer.com/10.1007/978-3-031-13945-1_22},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Muralidhar, Krishnamurty},
	editor = {Domingo-Ferrer, Josep and Laurent, Maryline},
	year = {2022},
	doi = {10.1007/978-3-031-13945-1_22},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Re-identification risk, US Census},
	pages = {312--323},
}

@article{mlodak_trade-off_2022,
	title = {The trade-off between the risk of disclosure and data utility in {SDC}: {A} case of data from a survey of accidents at work1},
	volume = {38},
	url = {https://discovery.researcher.life/article/the-trade-off-between-the-risk-of-disclosure-and-data-utility-in-sdc-a-case-of-data-from-a-survey-of-accidents-at-work1/a1480211f40537d5a647bc5fc40984ff},
	doi = {10.3233/sji-220936},
	abstract = {One of the key problems associated with Statistical Disclosure Control is ensuring an optimal trade-off between minimizing the risk of unit identification and maximizing the utility of data to be disseminated (which means minimizing information loss due to the application of SDC methods). In practice, it is usually achieved by defining how much risk can be accepted for any given unit, and then doing the best to modify the data set so that the risk is below the preset threshold while maximising the utility. Moreover, variables from statistical surveys vary not only in terms of their measurement scale but also as regards the role they play in the SDC process. All these aspects should therefore be taken into account when one tries to find this trade-off. In the paper we present a way of assessing whether an optimal trade-off has been achieved. Two main aspects of measuring the risk of disclosure are discussed. The first one is internal risk, i.e. the risk of disclosing confidential information only on the basis on disseminated microdata after the application of SDC (i.e. no attempt of combining data with external information is made); the second one is external risk, when the user has access to an alternative data set containing information that can be linked with statistical data in order to identify a unit. We show that it is possible to measure external risk and information loss while accounting for the measurement scale of variables. In our empirical study we used data from an annual survey of accidents at work for 2017. We compared complex information loss and the risk of disclosure in the original data files and those subjected to SDC using methods implemented in the new working version of the sdcMicro R package. We present the underlying assumptions and results of the SDC process, highlighting the benefits and drawbacks of the tools used in the study, which was conducted in 2020 and 2021 in the Centre for Small Area Estimation at the Statistical Office in Poznań.},
	language = {en},
	urldate = {2023-03-17},
	journal = {Statistical Journal of the IAOS},
	author = {Młodak, Andrzej and Józefowski, Tomasz and Pietrzak, Michał},
	month = dec,
	year = {2022},
	keywords = {risk-utility trade-off},
}

@incollection{domingo-ferrer_note_2022,
	address = {Cham},
	title = {A {Note} on the {Misinterpretation} of the {US} {Census} {Re}-identification {Attack}},
	volume = {13463},
	isbn = {978-3-031-13944-4 978-3-031-13945-1},
	url = {https://link.springer.com/10.1007/978-3-031-13945-1_21},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Francis, Paul},
	editor = {Domingo-Ferrer, Josep and Laurent, Maryline},
	year = {2022},
	doi = {10.1007/978-3-031-13945-1_21},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Re-identification risk, US Census},
	pages = {299--311},
}

@article{goldstein_probabilistic_2020,
	title = {A {Probabilistic} {Procedure} for {Anonymisation}, for {Assessing} the {Risk} of {Re}-identification and for the {Analysis} of {Perturbed} {Data} {Sets}},
	volume = {36},
	issn = {2001-7367},
	url = {https://www.sciendo.com/article/10.2478/jos-2020-0005},
	doi = {10.2478/jos-2020-0005},
	abstract = {Abstract 
            The requirement to anonymise data sets that are to be released for secondary analysis should be balanced by the need to allow their analysis to provide efficient and consistent parameter estimates. The proposal in this article is to integrate the process of anonymisation and data analysis. The first stage uses the addition of random noise with known distributional properties to some or all variables in a released (already pseudonymised) data set, in which the values of some identifying and sensitive variables for data subjects of interest are also available to an external ‘attacker’ who wishes to identify those data subjects in order to interrogate their records in the data set. The second stage of the analysis consists of specifying the model of interest so that parameter estimation accounts for the added noise. Where the characteristics of the noise are made available to the analyst by the data provider, we propose a new method that allows a valid analysis. This is formally a measurement error model and we describe a Bayesian MCMC algorithm that recovers consistent estimates of the true model parameters. A new method for handling categorical data is presented. The article shows how an appropriate noise distribution can be determined.},
	language = {en},
	number = {1},
	urldate = {2021-09-03},
	journal = {Journal of Official Statistics},
	author = {Goldstein, Harvey and Shlomo, Natalie},
	month = mar,
	year = {2020},
	keywords = {Re-identification risk},
	pages = {89--115},
}

@book{hundepool_statistical_2012,
	address = {Chichester, UK},
	series = {Wiley series in survey methodology},
	title = {Statistical disclosure control},
	isbn = {978-1-119-97815-2},
	language = {eng},
	publisher = {Wiley},
	editor = {Hundepool, Anco and Domingo-Ferrer, Josep and Franconi, Luisa and Gießing, Sarah and Schulte Nordholt, Eric and Spicer, Keith and de Wolf, Peter-Paul},
	year = {2012},
	keywords = {SDC - general views},
}

@book{hundepool_handbook_2010,
	edition = {Version 1.2},
	title = {Handbook on {Statistical} {Disclosure} {Control}},
	shorttitle = {Handbook {SDC}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.135.3606&rep=rep1&type=pdf},
	language = {en},
	publisher = {ESSNet SDC},
	author = {Hundepool, Anco and Domingo-Ferre, Josep and Franconi, Luisa and Giessing, Sarah and Lenz, Rainer and Longhurst, Jane and Schulte Nordholt, Eric and Seri, Giovanni and De Wolf, Peter-Paul},
	month = jan,
	year = {2010},
	keywords = {SDC - general views},
}

@article{duncan_enhancing_1991,
	title = {Enhancing {Access} to {Microdata} {While} {Protecting} {Confidentiality}: {Prospects} for the {Future}},
	volume = {6},
	issn = {0883-4237},
	shorttitle = {Enhancing {Access} to {Microdata} {While} {Protecting} {Confidentiality}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-6/issue-3/Enhancing-Access-to-Microdata-While-Protecting-Confidentiality--Prospects-for/10.1214/ss/1177011681.full},
	doi = {10.1214/ss/1177011681},
	number = {3},
	urldate = {2023-03-16},
	journal = {Statistical Science},
	author = {Duncan, George T. and Pearson, Robert W.},
	month = aug,
	year = {1991},
	keywords = {SDC - general views},
}

@article{desai_five_2016,
	title = {Five {Safes}: designing data access for research},
	url = {https://www2.uwe.ac.uk/faculties/BBS/Documents/1601.pdf},
	language = {en},
	number = {1601},
	journal = {Economics Working Paper Series},
	author = {Desai, Tanvi and Ritchie, Felix and Welpton, Richard},
	month = jan,
	year = {2016},
	keywords = {SDC - general views},
	pages = {1--27},
}

@incollection{domingo-ferrer_challenges_2022,
	address = {Cham},
	title = {Challenges in {Measuring} {Utility} for {Fully} {Synthetic} {Data}},
	volume = {13463},
	isbn = {978-3-031-13944-4 978-3-031-13945-1},
	url = {https://link.springer.com/10.1007/978-3-031-13945-1_16},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Drechsler, Jörg},
	editor = {Domingo-Ferrer, Josep and Laurent, Maryline},
	year = {2022},
	doi = {10.1007/978-3-031-13945-1_16},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Synthetic data},
	pages = {220--233},
}

@incollection{domingo-ferrer_synthetic_2022-1,
	address = {Cham},
	title = {Synthetic {Individual} {Income} {Tax} {Data}: {Methodology}, {Utility}, and {Privacy} {Implications}},
	volume = {13463},
	isbn = {978-3-031-13944-4 978-3-031-13945-1},
	shorttitle = {Synthetic {Individual} {Income} {Tax} {Data}},
	url = {https://link.springer.com/10.1007/978-3-031-13945-1_14},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Bowen, Claire McKay and Bryant, Victoria and Burman, Leonard and Czajka, John and Khitatrakun, Surachai and MacDonald, Graham and McClelland, Robert and Mucciolo, Livia and Pickens, Madeline and Ueyama, Kyle and Williams, Aaron R. and Wissoker, Doug and Zwiefel, Noah},
	editor = {Domingo-Ferrer, Josep and Laurent, Maryline},
	year = {2022},
	doi = {10.1007/978-3-031-13945-1_14},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Synthetic data},
	pages = {191--204},
}

@incollection{domingo-ferrer_comparing_2022,
	address = {Cham},
	title = {Comparing the {Utility} and {Disclosure} {Risk} of {Synthetic} {Data} with {Samples} of {Microdata}},
	volume = {13463},
	isbn = {978-3-031-13944-4 978-3-031-13945-1},
	url = {https://link.springer.com/10.1007/978-3-031-13945-1_17},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer International Publishing},
	author = {Little, Claire and Elliot, Mark and Allmendinger, Richard},
	editor = {Domingo-Ferrer, Josep and Laurent, Maryline},
	year = {2022},
	doi = {10.1007/978-3-031-13945-1_17},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Synthetic data},
	pages = {234--249},
}

@misc{solomon_geometry_2017,
	title = {Geometry of data: {Algorithmic} approaches to gerrymandering},
	url = {https://www.youtube.com/watch?v=HJIAhW1FIZ0&t=141s},
	language = {en},
	urldate = {2023-03-17},
	author = {Solomon, Justin},
	month = sep,
	year = {2017},
}

@misc{kun_eartmover_2018,
	title = {Eartmover {Distance}},
	url = {https://jeremykun.com/2018/03/05/earthmover-distance/},
	abstract = {Problem: Compute distance between points with uncertain locations (given by samples, or differing observations, or clusters).},
	language = {en},
	urldate = {2023-03-17},
	journal = {Math and Programming},
	author = {Kun, Jeremy},
	month = may,
	year = {2018},
}

@article{young_geographically_2009,
	title = {Geographically intelligent disclosure control for flexible aggregation of census data},
	volume = {23},
	url = {https://discovery.researcher.life/article/geographically-intelligent-disclosure-control-for-flexible-aggregation-of-census-data/0d8071bd8695399890d3c9e082d23766},
	doi = {10.1080/13658810801949835},
	abstract = {This paper describes a geographically intelligent approach to disclosure control for protecting flexibly aggregated census data. Increased analytical power has stimulated user demand for more detailed information for smaller geographical areas and customized boundaries. Consequently, it is vital that improved methods of statistical disclosure control are developed to protect against the increased disclosure risk. Traditionally methods of statistical disclosure control have been aspatial in nature. Here we present a geographically intelligent approach that takes into account the spatial distribution of risk. We describe empirical work illustrating how the flexibility of this new method, called local density swapping, is an improved alternative to random record swapping in terms of risk–utility.},
	language = {en},
	urldate = {2023-03-17},
	journal = {International Journal of Geographical Information Science},
	author = {Young, Caroline and Martin, David and Skinner, Chris},
	month = apr,
	year = {2009},
}

@book{duncan_statistical_2011,
	address = {New York},
	series = {Statistics for social and behavioral sciences},
	title = {Statistical confidentiality: principles and practice},
	isbn = {978-1-4419-7802-8},
	shorttitle = {Statistical confidentiality},
	language = {eng},
	publisher = {Springer},
	author = {Duncan, George T. and Elliot, Mark and Salazar-González, Juan-José},
	year = {2011},
}

@article{agnew_territorial_2014,
	title = {The territorial trap: {The} geographical assumptions of international relations theory},
	volume = {54},
	issn = {1291-1941},
	shorttitle = {The territorial trap},
	url = {https://www.cairn.info/revue-raisons-politiques-2014-2-page-23.htm},
	abstract = {{\textless}titre{\textgreater}R\&\#233;sum\&\#233;{\textless}/titre{\textgreater}Quand bien m\&\#234;me le pouvoir politique serait territorial, la territorialit\&\#233; n\&\#8217;implique pas n\&\#233;cessairement les pratiques d\&\#8217;exclusion mutuelle totale que lui attribuent les conceptions dominantes de l\&\#8217;\&\#201;tat moderne. Cependant, dans les th\&\#233;ories des relations internationales, lorsqu\&\#8217;il est question de la territorialit\&\#233; d\&\#8217;un \&\#201;tat, la discussion est presque toujours men\&\#233;e dans les termes de la persistance ou de l\&\#8217;obsolescence d\&\#8217;un \&\#201;tat territorial compris comme une entit\&\#233; inchang\&\#233;e et non d\&\#233;pendante des circonstances historico-g\&\#233;ographiques vari\&\#233;es. Cette approche est remise en question par les \&\#233;v\&\#232;nements contemporains. La fin de la Guerre froide, la v\&\#233;locit\&\#233; et la volatilit\&\#233; croissantes de l\&\#8217;\&\#233;conomie mondiale et l\&\#8217;\&\#233;mergence de mouvements politiques hors du cadre des \&\#201;tats territoriaux, sugg\&\#232;rent qu\&\#8217;il faut comprendre la territorialit\&\#233; des \&\#201;tats dans un contexte historique. Les trois pr\&\#233;suppos\&\#233;s g\&\#233;ographiques sur lesquels s\&\#8217;appuie la pens\&\#233;e orthodoxe (les \&\#201;tats comme des unit\&\#233;s fixes d\&\#8217;espace souverain, la polarit\&\#233; int\&\#233;rieur/\&\#233;tranger et les \&\#201;tats comme des \&\#171;\&\#160;conteneurs\&\#160;\&\#187; des soci\&\#233;t\&\#233;s) aboutissent \&\#224; un \&\#171;\&\#160;pi\&\#232;ge territorial\&\#160;\&\#187;.},
	language = {fr},
	number = {2},
	urldate = {2022-01-17},
	journal = {Raisons politiques},
	author = {Agnew, John and Dufoix, Stéphane},
	month = sep,
	year = {2014},
	note = {Bibliographie\_available: 0
Cairndomain: www.cairn.info
Cite Par\_available: 1
Publisher: Presses de Sciences Po},
	pages = {23--51},
}

@article{costemalle_detecting_2019,
	title = {Detecting geographical differencing problems in the context of spatial data dissemination},
	volume = {35},
	issn = {18747655, 18759254},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SJI-190564},
	doi = {10.3233/SJI-190564},
	number = {4},
	urldate = {2021-11-15},
	journal = {Statistical Journal of the IAOS},
	author = {Costemalle, Vianney},
	month = dec,
	year = {2019},
	keywords = {diclosure control, différenciation géographique, geo differencing},
	pages = {559--568},
}

@article{piche_theories_2013,
	title = {Les théories migratoires contemporaines au prisme des textes fondateurs},
	volume = {68},
	issn = {9782733231203},
	url = {https://www.cairn.info/revue-population-2013-1-page-153.htm},
	doi = {10.3917/popu.1301.0153},
	abstract = {L’objectif de cet article est de rendre compte de l’évolution des théories migratoires contemporaines à partir de 20 textes fondateurs et regroupés pour la première fois dans un manuel (Piché V., 2013, Les théories de la migration, Ined). Ils sont à l’origine d’avancées significatives dans l’explication des migrations, leurs causes et leurs effets. On voit ainsi se développer des théories qui, privilégiant d’abord des approches micro-individuelles centrées sur l’analyse coût-bénéfice, vont peu à peu intégrer les facteurs macro-structurels. L’analyse des réseaux migratoires occupe une place centrale aussi bien dans les cadres explicatifs que dans les travaux à propos des effets de la migration sur le développement économique. L’approche en termes de rapports de genre dans les décisions migratoires complète cette analyse. Le cadre analytique proposé ici présente la migration comme un phénomène multifactoriel et multidimensionnel, qui intègre trois dimensions principales : l’origine et la destination ; les niveaux d’analyse micro, méso, macro et global ; les aspects économiques, sociaux et politiques. Plutôt que de s’opposer, chaque approche apporte un éclairage spécifique et toute explication des phénomènes migratoires doit en tenir compte, ainsi que l’élaboration et l’évaluation des politiques migratoires.},
	language = {FR},
	number = {1},
	journal = {Population},
	author = {Piché, Victor},
	year = {2013},
	note = {Place: Paris
Publisher: Ined Éditions},
	pages = {153--178},
}

@misc{tavernier_taux_nodate,
	title = {Le taux de pauvreté serait stable en 2020 : ce que dit cette première estimation et ce qu’elle ne dit pas - {Le} blog de l'{InseeLe} blog de l'{Insee}},
	shorttitle = {Le taux de pauvreté serait stable en 2020},
	url = {https://blog.insee.fr/le-taux-de-pauvrete-serait-stable-en-2020-ce-que-dit-cette-premiere-estimation-et-ce-quelle-ne-dit-pas/},
	abstract = {L’Insee vient de faire paraître sa première estimation du taux de pauvreté pour 2020 : 14,6 \% des personnes seraient en dessous du seuil de pauvreté en France. Ce taux est stable par rapport à 2019. Cette stabilité peut étonner si l’on se réfère au « million de pauvres supplémentaire » dont la presse s’est fait l’écho depuis un an. Pour son estimation, l’Insee utilise depuis quelques années une méthode de microsimulation. Cette méthode présente certaines fragilités, accentuées par le caractère inédit de la crise. Néanmoins, les travaux complémentaires menés par l’Insee sur les données de La Banque postale et sur le recours à l’aide alimentaire conduisent à conclure que la pauvreté s’est sans doute intensifiée mais n’a pas explosé. Au total, l’estimation de stabilité ou quasi-stabilité du taux de pauvreté paraît fiable, avec la réserve usuelle qu’elle ne tient compte que des revenus déclarés. L’écart entre cette mesure et les perceptions tient sans doute au caractère inédit de la crise, qui exacerbe les visions pessimistes : l’opinion sera plus marquée par les mois difficiles que par les compensations sur le reste de l’année, par des situations locales très préoccupantes mais qui ne sont pas généralisées sur tout le territoire. Il demeure qu’un seul indicateur ne peut pas à lui seul rendre compte d’une réalité sociale ou économique comme la pauvreté. Celle-ci n’est pas que monétaire, et son intensité reste à mesurer avec précision.},
	language = {fr-FR},
	urldate = {2021-11-05},
	author = {Tavernier, Jean-Luc},
	note = {Section: Revenus - Pouvoir d'achat - Consommation},
	keywords = {Insee, Pauvreté},
}

@article{elguellab_faits_2021,
	title = {Faits stylisés de l’économie marocaine  sur la période 1998-2018},
	url = {https://www.insee.fr/fr/statistiques/fichier/5759514/3_ST115.pdf},
	abstract = {Même s’il existe plusieurs travaux explorant les faits stylisés dans les pays en développement, y compris le Maroc, leurs conclusions sont devenues caduques en raison des transformations économiques récentes. De plus, ces travaux ne prennent pas en considération l’existence, au sein de l’économie marocaine, de deux secteurs (agricole et non agricole) aux comportements très distincts. L’agrégation de ces deux secteurs s’avère être un facteur de perturbation tant de l’inférence que de la lecture des tendances récentes. Ce travail consiste en une étude des faits stylisés de l’économie marocaine sur la période 1998-2018. Les faits relevés, essentiellement en se basant sur le produit intérieur brut (PIB) non agricole, semblent indiquer que l’économie marocaine conjugue deux tendances : d’une part, des caractéristiques partagées avec les pays en développement (comme la consommation non lisse et la dépendance vis-à-vis de la conjoncture mondiale) et, d’autre part, des comportements spécifiques, notamment la faible
volatilité de ses agrégats globaux et l’importance des chocs de demande.},
	language = {fr},
	number = {115},
	journal = {Statéco},
	author = {Elguellab, Ali and Ezzahid, Elhadj},
	year = {2021},
	pages = {37--52},
}

@article{daubree_mesure_2021,
	title = {La mesure du secteur informel dans un département français d’{Outre}-mer : le cas de {Mayotte}},
	url = {https://www.insee.fr/fr/statistiques/fichier/5759514/5_ST115.pdf},
	abstract = {En 2015, pour faire face au besoin d’informations sur l’activité économique à Mayotte, l’Insee a complété son dispositif d’enquêtes auprès des entreprises par une enquête spéci-fique afin d’appréhender le secteur informel et de pouvoir quantifier le poids de ce secteur dans l’économie mahoraise. Compte-tenu des caractéristiques économiques et sociales de Mayotte, proches de celles d’un pays en développement (PED), l’approche retenue par l’Insee, a été inspirée de la méthodologie des enquêtes 1-2-3 développée par des chercheurs de l’Institut de recherche pour le développement (IRD-DIAL). Ce type d’enquêtes mixtes n’avait, à ce jour, jamais été conduit sur un territoire relevant d’un pays développé. En l’absence de référentiel de qualité permettant de servir de base de sondage aux enquêtes auprès des entreprises, l’Insee a déployé une stratégie et opté pour des choix méthodolo-giques associés à la mesure de l’informel afin de mettre en oeuvre un dispositif de suivi des entreprises à Mayotte. D’une part, l’univers des entreprises formelles, jusque-là partiel-lement couvert a été complété. D’autre part, l’univers des entreprises informelles a été pris en compte pour la première fois : en 2015, deux-tiers des entreprises mahoraises sont informelles, donc inconnues de l’administration fiscale. Elles ne génèrent que 9 \% de la valeur ajoutée de l’ensemble des entreprises. Cette expérience réussie, qui sera renouvelée en 2021 une deuxième fois, met en avant d’autres résultats-clés, qui sont présentés dans cet article.},
	language = {fr},
	number = {115},
	journal = {Statéco},
	author = {Daubrée, Sylvain and Roubaud, François and Torelli, Constance and Zanuso, Claire},
	year = {2021},
	pages = {71--90},
}

@article{malinowski_automated_2020,
	title = {Automated {Production} of a {Land} {Cover}/{Use} {Map} of {Europe} {Based} on {Sentinel}-2 {Imagery}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/12/21/3523},
	doi = {10.3390/rs12213523},
	abstract = {Up-to-date information about the Earth\&rsquo;s surface provided by land cover maps is essential for numerous environmental and land management applications. There is, therefore, a clear need for the continuous and reliable monitoring of land cover and land cover changes. The growing availability of high resolution, regularly collected remote sensing data can support the increasing number of applications that require high spatial resolution products that are frequently updated (e.g., annually). However, large-scale operational mapping requires a highly-automated data processing workflow, which is currently lacking. To address this issue, we developed a methodology for the automated classification of multi-temporal Sentinel-2 imagery. The method uses a random forest classifier and existing land cover/use databases as the source of training samples. In order to demonstrate its operability, the method was implemented on a large part of the European continent, with CORINE Land Cover and High-Resolution Layers as training datasets. A land cover/use map for the year 2017 was produced, composed of 13 classes. An accuracy assessment, based on nearly 52,000 samples, revealed high thematic overall accuracy (86.1\%) on a continental scale, and average overall accuracy of 86.5\% at country level. Only low-frequency classes obtained lower accuracies and we recommend that their mapping should be improved in the future. Additional modifications to the classification legend, notably the fusion of thematically and spectrally similar vegetation classes, increased overall accuracy to 89.0\%, and resulted in ten, general classes. A crucial aspect of the presented approach is that it embraces all of the most important elements of Earth observation data processing, enabling accurate and detailed (10 m spatial resolution) mapping with no manual user involvement. The presented methodology demonstrates possibility for frequent and repetitive operational production of large-scale land cover maps.},
	language = {en},
	number = {21},
	urldate = {2021-10-20},
	journal = {Remote Sensing},
	author = {Malinowski, Radek and Lewiński, Stanisław and Rybicki, Marcin and Gromny, Ewa and Jenerowicz, Małgorzata and Krupiński, Michał and Nowakowski, Artur and Wojtkowski, Cezary and Krupiński, Marcin and Krätzschmar, Elke and Schauer, Peter},
	month = jan,
	year = {2020},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {CORINE land cover, High Resolution Layers (HRL), Sentinel-2, land cover and use, machine learning, multi-temporal, random forest},
	pages = {3523},
}

@article{isola_image--image_2018,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2021-10-15},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{visin_reseg_2016,
	title = {{ReSeg}: {A} {Recurrent} {Neural} {Network}-based {Model} for {Semantic} {Segmentation}},
	shorttitle = {{ReSeg}},
	url = {http://arxiv.org/abs/1511.07053},
	abstract = {We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https://github.com/fvisin/reseg.},
	urldate = {2021-10-15},
	journal = {arXiv:1511.07053 [cs]},
	author = {Visin, Francesco and Ciccone, Marco and Romero, Adriana and Kastner, Kyle and Cho, Kyunghyun and Bengio, Yoshua and Matteucci, Matteo and Courville, Aaron},
	month = may,
	year = {2016},
	note = {arXiv: 1511.07053},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{jegou_one_2017,
	title = {The {One} {Hundred} {Layers} {Tiramisu}: {Fully} {Convolutional} {DenseNets} for {Semantic} {Segmentation}},
	shorttitle = {The {One} {Hundred} {Layers} {Tiramisu}},
	url = {http://arxiv.org/abs/1611.09326},
	abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
	urldate = {2021-10-15},
	journal = {arXiv:1611.09326 [cs]},
	author = {Jégou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
	month = oct,
	year = {2017},
	note = {arXiv: 1611.09326},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2021-10-15},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
	urldate = {2021-10-15},
	journal = {arXiv:1411.4038 [cs]},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = mar,
	year = {2015},
	note = {arXiv: 1411.4038},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ronneberger_u-net_2015-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
}

@inproceedings{albawi_understanding_2017,
	title = {Understanding of a convolutional neural network},
	doi = {10.1109/ICEngTechnol.2017.8308186},
	abstract = {The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.},
	booktitle = {2017 {International} {Conference} on {Engineering} and {Technology} ({ICET})},
	author = {Albawi, Saad and Mohammed, Tareq Abed and Al-Zawi, Saad},
	month = aug,
	year = {2017},
	keywords = {Convolution, Convolutional neural networks, Feature extraction, Image edge detection, Image recognition, Neurons, artificial neural networks, computer vision, convolutional neural networks, deep learning, machine learning},
	pages = {1--6},
}

@incollection{kim_convolutional_2017,
	address = {Berkeley, CA},
	title = {Convolutional {Neural} {Network}},
	isbn = {978-1-4842-2845-6},
	url = {https://doi.org/10.1007/978-1-4842-2845-6_6},
	abstract = {The importance of the deep neural network lies in the fact that it opened the door to the complicated non-linear model and systematic approach for the hierarchical processing of knowledge.},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {{MATLAB} {Deep} {Learning}: {With} {Machine} {Learning}, {Neural} {Networks} and {Artificial} {Intelligence}},
	publisher = {Apress},
	author = {Kim, Phil},
	editor = {Kim, Phil},
	year = {2017},
	doi = {10.1007/978-1-4842-2845-6_6},
	keywords = {Convolutional Neural Network, Deep Neural Network, Image Recognition, Input Image, Neural Network},
	pages = {121--147},
}

@inproceedings{constantin_accurate_2018,
	title = {Accurate {Road} {Detection} from {Satellite} {Images} {Using} {Modified} {U}-net},
	doi = {10.1109/APCCAS.2018.8605652},
	abstract = {In this paper, we present an accurate neural network algorithm to detect roads in satellite images. Based on convolutional neural networks, from a 6-channel image, this model is able to transfer the road structure to the output using both the U-net and the atrous convolution architecture. To train this model, we introduce a new combination of existing loss functions including the binary cross-entropy and the Jaccard distance to avoid false positive detection and increase binary classification accuracy. In terms of precision, recall, the F-score and accuracy, experiments carried out using the Massachusetts roads dataset, provide better results than state-of-the-art road extraction models.},
	booktitle = {2018 {IEEE} {Asia} {Pacific} {Conference} on {Circuits} and {Systems} ({APCCAS})},
	author = {Constantin, Alexandre and Ding, Jian-Jiun and Lee, Yih-Cherng},
	month = oct,
	year = {2018},
	keywords = {Computer architecture, Convolution, Databases, Deep learning, Kernel, Roads, Satellites, Training, convolutional neural network (CNN), road extraction, satellite images},
	pages = {423--426},
}

@article{wagner_u-net-id_2020,
	title = {U-{Net}-{Id}, an {Instance} {Segmentation} {Model} for {Building} {Extraction} from {Satellite} {Images}—{Case} {Study} in the {Joanópolis} {City}, {Brazil}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/12/10/1544},
	doi = {10.3390/rs12101544},
	abstract = {Currently, there exists a growing demand for individual building mapping in regions of rapid urban growth in less-developed countries. Most existing methods can segment buildings but cannot discriminate adjacent buildings. Here, we present a new convolutional neural network architecture (CNN) called U-net-id that performs building instance segmentation. The proposed network is trained with WorldView-3 satellite RGB images (0.3 m) and three different labeled masks. The first is the building mask; the second is the border mask, which is the border of the building segment with 4 pixels added outside and 3 pixels inside; and the third is the inner segment mask, which is the segment of the building diminished by 2 pixels. The architecture consists of three parallel paths, one for each mask, all starting with a U-net model. To accurately capture the overlap between the masks, all activation layers of the U-nets are copied and concatenated on each path and sent to two additional convolutional layers before the output activation layers. The method was tested with a dataset of 7563 manually delineated individual buildings of the city of Joanópolis-SP, Brazil. On this dataset, the semantic segmentation showed an overall accuracy of 97.67\% and an F1-Score of 0.937 and the building individual instance segmentation showed good performance with a mean intersection over union (IoU) of 0.582 (median IoU = 0.694).},
	language = {en},
	number = {10},
	urldate = {2021-10-15},
	journal = {Remote Sensing},
	author = {Wagner, Fabien H. and Dalagnol, Ricardo and Tarabalka, Yuliya and Segantine, Tassiana Y. F. and Thomé, Rogério and Hirye, Mayumi C. M.},
	month = jan,
	year = {2020},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {U-net, building detection, instance segmentation, urban landscape},
	pages = {1544},
}

@inproceedings{ivanovsky_building_2019,
	title = {Building {Detection} on {Aerial} {Images} {Using} {U}-{NET} {Neural} {Networks}},
	doi = {10.23919/FRUCT.2019.8711930},
	abstract = {This article presents research results of two convolutional neural networks for building detection on satellite images of Planet database. To analyze the quality of developed algorithms, there was used Sorensen-Dice coefficient of similarity which compares results of algorithms with tagged masks. The masks were generated from json files and sliced on smaller parts together with respective images before the training of algorithms. This approach allows to cope with the problem of segmentation for aerial high-resolution images efficiently and effectively. The problem of building detection on satellite images can be put into practice for urban planning, building control, etc.},
	booktitle = {2019 24th {Conference} of {Open} {Innovations} {Association} ({FRUCT})},
	author = {Ivanovsky, Leonid and Khryashchev, Vladimir and Pavlov, Vladimir and Ostrovskaya, Anna},
	month = apr,
	year = {2019},
	note = {ISSN: 2305-7254},
	keywords = {Buildings, Databases, Image segmentation, Machine learning algorithms, Planets, Satellites, Training},
	pages = {116--122},
}

@article{wieland_performance_2014,
	title = {Performance {Evaluation} of {Machine} {Learning} {Algorithms} for {Urban} {Pattern} {Recognition} from {Multi}-spectral {Satellite} {Images}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/6/4/2912},
	doi = {10.3390/rs6042912},
	abstract = {In this study, a classification and performance evaluation framework for the recognition of urban patterns in medium (Landsat ETM, TM and MSS) and very high resolution (WorldView-2, Quickbird, Ikonos) multi-spectral satellite images is presented. The study aims at exploring the potential of machine learning algorithms in the context of an object-based image analysis and to thoroughly test the algorithm’s performance under varying conditions to optimize their usage for urban pattern recognition tasks. Four classification algorithms, Normal Bayes, K Nearest Neighbors, Random Trees and Support Vector Machines, which represent different concepts in machine learning (probabilistic, nearest neighbor, tree-based, function-based), have been selected and implemented on a free and open-source basis. Particular focus is given to assess the generalization ability of machine learning algorithms and the transferability of trained learning machines between different image types and image scenes. Moreover, the influence of the number and choice of training data, the influence of the size and composition of the feature vector and the effect of image segmentation on the classification accuracy is evaluated.},
	language = {en},
	number = {4},
	urldate = {2021-10-15},
	journal = {Remote Sensing},
	author = {Wieland, Marc and Pittore, Massimiliano},
	month = apr,
	year = {2014},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {data mining, machine learning, object-based image analysis, urban remote sensing},
	pages = {2912--2939},
}

@article{sharifi_yield_2021,
	title = {Yield prediction with machine learning algorithms and satellite images},
	volume = {101},
	issn = {1097-0010},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jsfa.10696},
	doi = {10.1002/jsfa.10696},
	abstract = {BACKGROUND Barley is one of the strategic agricultural products available in the world, and yield prediction is important for ensuring food security. One way of estimating a product is to use remote sensing data in conjunction with field data and meteorological data. One of the main issues surrounding this comprises the use of machine learning techniques to create a multi-resource data-based estimation model. Many studies have been conducted on barley yield prediction from planting to harvest. Still, the effect of different time intervals on yield prediction has not been investigated. Furthermore, the effect of different periods on yield prediction has not been investigated. RESULTS In the present study, the whole growth period was divided into three parts. Using one of the major barley production areas in Iran, the performance of the proposed model was evaluated. In the first step, a model for integrating field data, remote sensing data and meteorological data was prepared. The results obtained show that, among the four machine learning methods implemented, the gaussian process regression algorithm performed best and estimated yield with r2 = 0.84, root mean square error = 737 kg ha−1 and mean absolute = 650 kg ha−1, 1 month before harvest. CONCLUSION It was found that the estimation results change depending on different agricultural zones and temporal training settings. The findings of the present study provide a powerful potential tool for the yield prediction of barley using multi-source data and machine learning. © 2020 Society of Chemical Industry},
	language = {en},
	number = {3},
	urldate = {2021-10-15},
	journal = {Journal of the Science of Food and Agriculture},
	author = {Sharifi, Alireza},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jsfa.10696},
	keywords = {Gaussian process regression, barley, machine learning, remote sensing, yield prediction},
	pages = {891--896},
}

@inproceedings{wang_understanding_2018,
	title = {Understanding {Convolution} for {Semantic} {Segmentation}},
	doi = {10.1109/WACV.2018.00163},
	abstract = {Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are of both theoretical and practical value. First, we design dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields (RF) of the network to aggregate global information; 2) alleviates what we call the "gridding issue"caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a state-of-art result of 80.1\% mIOU in the test set at the time of submission. We also have achieved state-of-theart overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Our source code can be found at https://github.com/TuSimple/TuSimple-DUC.},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Wang, Panqu and Chen, Pengfei and Yuan, Ye and Liu, Ding and Huang, Zehua and Hou, Xiaodi and Cottrell, Garrison},
	month = mar,
	year = {2018},
	keywords = {Convolution, Decoding, Image segmentation, Kernel, Semantics, Task analysis, Training},
	pages = {1451--1460},
}

@article{garcia-garcia_review_2017,
	title = {A {Review} on {Deep} {Learning} {Techniques} {Applied} to {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1704.06857},
	abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
	urldate = {2021-10-15},
	journal = {arXiv:1704.06857 [cs]},
	author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.06857},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{rebecq_icarus_2016,
	title = {Icarus : un package {R} pour le calage sur marges et ses variantes},
	url = {http://paperssondages16.sfds.asso.fr/submission_54.pdf},
	language = {fr},
	urldate = {2021-08-16},
	author = {Rebecq, Antoine},
	year = {2016},
}

@book{tille_orie_2001,
	address = {Paris},
	title = {Théorie des sondages: échantillonnage et estimation en population finie : cours et exercices avec solutions},
	isbn = {9782100054848},
	shorttitle = {Théorie des sondages},
	abstract = {Ce livre offre un aperçu général et cohérent des méthodes statistiques permettant de réaliser les différentes étapes d'une enquête par sondage. Ces étapes sont essentiellement la planification, l'estimation et le traitement des non-réponses. La théorie des sondages est d'abord située dans son contexte historique puis développée dans le cadre de l'approche s'appuyant sur le plan de sondage. Les différentes techniques de planification sont examinées en détail : plans simples, à probabilités inégales, stratifiés, équilibrés, par grappes, à plusieurs degrés, à plusieurs phases. Un ensemble d'algorithmes de tirage d'échantillon sont intégralement décrits. Les techniques d'estimation classiques sont ensuite appliquées aux plans simples, puis l'estimateur par la régression et les techniques de calage sont appliqués aux plans complexes pour le cas multivarié. Des exercices, dont certains sont corrigés, complètent le cours. Ouvrage pédagogique à destination des étudiants (mathématiques appliquées, sciences économiques ...) et des élèves ingénieurs, ce livre sera utile à tous ceux qui sont confrontés dans leur vie professionnelle ou au cours de leurs études à la réalisation d'enquêtes par sondage. [Ed.].},
	language = {French},
	publisher = {Dunod},
	author = {Tillé, Yves},
	year = {2001},
	note = {OCLC: 999744269},
}

@article{thibault_villages_2019,
	title = {Les villages de {Mayotte} en 2017},
	issn = {2275-4318},
	url = {https://www.insee.fr/fr/statistiques/4223807},
	abstract = {En 2017, les conditions de vie restent toujours aussi difficiles à Mayotte. Elles n’ont guère progressé depuis 2012, dans un contexte de forte croissance démographique et d’une augmentation},
	number = {22},
	urldate = {2021-08-11},
	journal = {Insee Analyses Mayotte},
	author = {Thibault, Pierre},
	month = aug,
	year = {2019},
	keywords = {CAH, Typologie},
}

@techreport{division_emploi_enquete_2021,
	type = {Note méthodologique},
	title = {Enquête {Emploi}, enquête sur l'emploi, le chômage et l'inactivité. {Méthodologie}.},
	shorttitle = {Note méthodologique de l'{EEC}},
	url = {https://www.insee.fr/fr/metadonnees/source/operation/s2022/documentation-methodologique},
	language = {fr},
	urldate = {2021-07-21},
	institution = {Insee},
	author = {{Division Emploi}},
	month = jul,
	year = {2021},
	keywords = {EEC},
	pages = {20},
}

@article{haziza_construction_2007,
	title = {On the {Construction} of {Imputation} {Classes} in {Surveys}},
	volume = {75},
	issn = {1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2006.00002.x},
	doi = {10.1111/j.1751-5823.2006.00002.x},
	abstract = {This paper explores the problem of the construction of imputation classes using the score method, sometimes called predictive mean stratification or response propensity stratification, depending on the context. This method was studied in Thomsen (1973), Little (1986) and Eltinge \& Yansaneh (1997). We use a different framework to evaluate the properties of the resulting imputed estimator of a population mean. In our framework, we condition on the realized sample. This enables us to considerably simplify our theoretical developments in the frequent situation where the boundaries and the number of classes are sample-dependent. We find that the key factor for reducing the non-response bias is to form classes homogeneous with respect to the response probabilities and/or the conditional expectation of the variable of interest. In the latter case, the non-response/imputation variance is also reduced. Finally, we performed a simulation study to fully evaluate various versions of the score method and to compare them with a cross-classification method, which is frequently used in practice. The results showed the superiority of the score method in general.},
	language = {en},
	number = {1},
	urldate = {2021-07-19},
	journal = {International Statistical Review},
	author = {Haziza, David and Beaumont, Jean-François},
	year = {2007},
	keywords = {Classification algorithm, cross-classification method, imputation model, non-response and imputation variance, non-response bias, non-response model, score method},
	pages = {25--43},
}

@inproceedings{deroyon_comment_2016,
	address = {Gatineau, Québec},
	title = {Comment constituer des groupes de réponse homogène ? {Une} comparaison de quelques méthodes appliquées aux {Enquêtes} {Sectorielles} {Annuelles} en {France}},
	shorttitle = {Comment constituer des groupes de réponse homogène ?},
	url = {http://paperssondages16.sfds.asso.fr/submission_67.pdf},
	language = {fr},
	author = {Deroyon, Thomas},
	month = oct,
	year = {2016},
	keywords = {Non-response, Sondage, Survey},
}

@article{haziza_discussion_2016,
	title = {A {Discussion} of {Weighting} {Procedures} for {Unit} {Nonresponse}},
	volume = {32},
	issn = {2001-7367},
	url = {https://www.sciendo.com/article/10.1515/jos-2016-0006},
	doi = {10.1515/jos-2016-0006},
	abstract = {Abstract 
            Weighting procedures are commonly applied in surveys to compensate for nonsampling errors such as nonresponse errors and coverage errors. Two types of weight-adjustment procedures are commonly used in the context of unit nonresponse: (i) nonresponse propensity weighting followed by calibration, also known as the two-step approach and (ii) nonresponse calibration weighting, also known as the one-step approach. In this article, we discuss both approaches and warn against the potential pitfalls of the one-step procedure. Results from a simulation study, evaluating the properties of several point estimators, are presented.},
	language = {en},
	number = {1},
	urldate = {2021-07-19},
	journal = {Journal of Official Statistics},
	author = {Haziza, David and Lesage, Éric},
	month = mar,
	year = {2016},
	pages = {129--145},
}

@article{metropolis_monte_1949,
	title = {The {Monte} {Carlo} {Method}},
	volume = {44},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2280232},
	doi = {10.2307/2280232},
	abstract = {We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.},
	number = {247},
	urldate = {2021-05-17},
	journal = {Journal of the American Statistical Association},
	author = {Metropolis, Nicholas and Ulam, S.},
	year = {1949},
	pages = {335--341},
}

@book{ardilly_techniques_2006,
	address = {Paris},
	edition = {2e édition},
	title = {Les techniques de sondage},
	isbn = {9782710808473},
	language = {fr},
	publisher = {Editions Technip},
	author = {Ardilly, Pascal},
	year = {2006},
}

@techreport{insee_-_dsds_compte-rendu_2021,
	address = {Paris},
	type = {Compte-{Rendu}},
	title = {Compte-rendu de la réunion du {GT} {EEC} {Mayotte} du 23 avril 2021},
	url = {https://www.agora.insee.fr/files/live/sites/dg-dsds/files/shared/F201_DERA/F201_PEEE/M%c3%a9lop%c3%a9e/Projet/GT%20Mayotte/2021_10347_DG75-F201.pdf},
	language = {fr},
	number = {2021\_10347\_DG75-F201},
	urldate = {2021-05-15},
	institution = {Insee},
	author = {{Insee - DSDS}},
	month = may,
	year = {2021},
}

@techreport{insee_-_dsds_compte-rendu_2021-1,
	address = {Paris},
	type = {Compte-{Rendu}},
	title = {Compte-{Rendu} de la réunion du {GT} {EEC} {Mayotte} du 11 mars 2021},
	url = {https://www.agora.insee.fr/files/live/sites/dg-dsds/files/shared/F201_DERA/F201_PEEE/M%c3%a9lop%c3%a9e/Projet/GT%20Mayotte/2021_8733_DG75-F201.pdf},
	language = {fr},
	number = {2021\_8733\_DG75-F201},
	urldate = {2021-05-15},
	institution = {Insee},
	author = {Insee - DSDS},
	month = apr,
	year = {2021},
}

@techreport{insee_-_dsds_precision_2018,
	address = {Paris},
	type = {Note},
	title = {Précision et taille d’échantillon à {Mayotte} en vue de la refonte de l’{EEC}},
	url = {https://www.agora.insee.fr/files/live/sites/dg-dsds/files/shared/F201_DERA/F201_PEEE/M%c3%a9lop%c3%a9e/Projet/GT%20Mayotte/2018_22934_DG75-F201.pdf},
	language = {fr},
	number = {2018\_22934\_DG75-F201},
	urldate = {2021-05-15},
	institution = {Insee},
	author = {{Insee - DSDS}},
	month = oct,
	year = {2018},
}

@techreport{insee_-_dsds_compte-rendu_2018,
	address = {Paris},
	type = {Compte-{Rendu}},
	title = {Compte-{Rendu} de la 1ère réunion du {GT} {Mayotte} du 10 juillet 2018},
	url = {https://www.agora.insee.fr/files/live/users/jj/dc/ji/DEQB4I/files/2018_14295_DG75-F201.pdf},
	language = {fr},
	number = {2018\_14295\_DG75-F201},
	urldate = {2021-05-15},
	institution = {Insee},
	author = {{Insee - DSDS}},
	month = aug,
	year = {2018},
}

@misc{rivest_stratification_2017,
	title = {stratification, package {R}},
	copyright = {License GPL-2},
	url = {https://cran.r-project.org/web/packages/stratification/},
	author = {Rivest, Louis-Paul and Baillargeon, Sophie},
	month = mar,
	year = {2017},
}

@techreport{insee_-_sr_mayotte_synthese_2020,
	address = {Mammoudzou},
	title = {Synthèse démographique, sociale et économique},
	url = {https://www.insee.fr/fr/statistiques/fichier/2018177/tiTEM.pdf},
	language = {fr},
	urldate = {2021-05-14},
	institution = {Insee},
	author = {{Insee - SR Mayotte}},
	month = aug,
	year = {2020},
}

@techreport{eurostat_iess_2014,
	address = {Brussels},
	title = {{IESS} {Framework} regulation: state of play and impact on the {LFS}},
	url = {https://circabc.europa.eu/sd/a/339f3fb8-7004-4fb7-aac3-c3ca5c15f8c6/Doc%2038%20Item%202%201%20IESS%20Framework%20Regulation.pdf},
	language = {en},
	number = {Eurostat/F3/LAMAS/38/14},
	urldate = {2021-05-15},
	institution = {Eurostat},
	author = {{Eurostat}},
	month = dec,
	year = {2014},
	pages = {8},
}

@techreport{insee_dr_la_reunion_-_criem_mise_2013,
	address = {Saint-Denis},
	title = {Mise en place d'un dispositif pérenne de sondage pour les enquêtes auprès des ménages à {Mayotte}},
	number = {797/DR974-DR974-SES},
	author = {{Insee DR La Réunion - Criem}},
	month = sep,
	year = {2013},
}

@techreport{insee_dsds-dmtr_note_2020,
	address = {Paris},
	type = {Note},
	title = {Note de tirage des groupes de rotation des îlots des grandes communes mahoraises},
	number = {2020\_9950\_DG75-F520},
	institution = {Insee},
	author = {{Insee DSDS-DMTR}},
	month = apr,
	year = {2020},
}

@techreport{insee_dr_la_reunion_-_criem_note_2020,
	address = {Saint-Denis},
	type = {Note},
	title = {Note de tirage des groupes de rotation des îlots des petites communes mahoraises},
	number = {2020\_9767\_DR974-SES},
	institution = {Insee},
	author = {{Insee DR La Réunion - Criem}},
	month = apr,
	year = {2020},
}

@article{dalenius_minimum_1959,
	title = {Minimum {Variance} {Stratification}},
	volume = {54},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1959.10501501},
	doi = {10.1080/01621459.1959.10501501},
	language = {en},
	number = {285},
	urldate = {2021-05-05},
	journal = {Journal of the American Statistical Association},
	author = {Dalenius, Tore and Hodges, Joseph L.},
	month = mar,
	year = {1959},
	pages = {88--101},
}

@article{chauvet_stratified_2009,
	series = {Statistics {Canada}},
	title = {Stratified balanced sampling},
	volume = {35},
	url = {https://www.researchgate.net/profile/Guillaume_Chauvet/publication/288704033_Stratified_balanced_sampling/links/58d8e3d592851c44d4ada090/Stratified-balanced-sampling.pdf},
	language = {en},
	number = {1},
	journal = {Survey Methodology},
	author = {Chauvet, Guillaume},
	month = jun,
	year = {2009},
	pages = {115--119},
}

@article{chauvet_fast_2006,
	title = {A fast algorithm for balanced sampling},
	volume = {21},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-006-0250-2},
	doi = {10.1007/s00180-006-0250-2},
	abstract = {The cube method (Deville \& Tillé 2004) is a large family of algorithms that allows selecting balanced samples with equal or unequal inclusion probabilities. In this paper, we propose a very fast implementation of the cube method. The execution time does not depend on the square of the population size anymore, but only on the population size. Balanced samples can thus be selected in very large populations of several hundreds of thousands of units.},
	language = {en},
	number = {1},
	urldate = {2021-05-05},
	journal = {Computational Statistics},
	author = {Chauvet, Guillaume and Tillé, Yves},
	month = mar,
	year = {2006},
	pages = {53--62},
}

@inproceedings{chauvet_simplified_2018,
	address = {Paris},
	title = {Simplified {Variance} {Estimation} {For} {Multistage} {Sample} {Surveys}},
	volume = {13},
	url = {http://jms-insee.fr/jms2018s13_1/},
	language = {en},
	booktitle = {Journées de {Méthodologie} {Statistique}},
	author = {Chauvet, Guillaume},
	year = {2018},
}

@article{grafstrom_doubly_2013,
	title = {Doubly balanced spatial sampling with spreading and restitution of auxiliary totals},
	volume = {24},
	issn = {1099-095X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2194},
	doi = {https://doi.org/10.1002/env.2194},
	abstract = {A new spatial sampling method is proposed in order to achieve a double property of balancing. The sample is spatially balanced or well spread so as to avoid selecting neighbouring units. Moreover, the method also enables to satisfy balancing equations on auxiliary variables available on all the sampling units because the Horvitz–Thompson estimator is almost equal to the population totals for these variables. The method works with any definition of distance in a multidimensional space and supports the use of unequal inclusion probabilities. The algorithm is simple and fast. Examples show that the method succeeds in using more information than the local pivotal method, the cube method and the Generalized Random-Tessellation Stratified sampling method, and thus performs better. An estimator of the variance for this sampling design is proposed in order to lead to an inference that takes the effect of the sampling design into account. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {2},
	urldate = {2021-04-29},
	journal = {Environmetrics},
	author = {Grafström, Anton and Tillé, Yves},
	year = {2013},
	keywords = {balanced sampling, pivotal method, spatial correlation, spatially balanced sampling},
	pages = {120--131},
}

@article{deville_variance_2005,
	title = {Variance approximation under balanced sampling},
	volume = {128},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/abs/pii/S0378375804000035},
	doi = {10.1016/j.jspi.2003.11.011},
	abstract = {A balanced sampling design has the interesting property that Horvitz–Thompson estimators of totals for a set of balancing variables are equal to the t…},
	language = {en},
	number = {2},
	urldate = {2021-04-29},
	journal = {Journal of Statistical Planning and Inference},
	author = {Deville, Jean-Claude and Tillé, Yves},
	month = feb,
	year = {2005},
	pages = {569--591},
}

@article{benedetti_spatially_2017,
	title = {Spatially {Balanced} {Sampling}: {A} {Review} and {A} {Reappraisal}},
	volume = {85},
	issn = {1751-5823},
	shorttitle = {Spatially {Balanced} {Sampling}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12216},
	doi = {https://doi.org/10.1111/insr.12216},
	abstract = {Spatially distributed data exhibit particular characteristics that should be considered when designing a survey of spatial units. Unfortunately, traditional sampling designs generally do not allow for spatial features, even though it is usually desirable to use information concerning spatial dependence in a sampling design. This paper reviews and compares some recently developed randomised spatial sampling procedures, using simple random sampling without replacement as a benchmark for comparison. The approach taken is design-based and serves to corroborate intuitive arguments about the need to explicitly integrate spatial dependence into sampling survey theory. Some guidance for choosing an appropriate spatial sampling design is provided, and some empirical evidence for the gains from using these designs with spatial populations is presented, using two datasets as illustrations.},
	language = {en},
	number = {3},
	urldate = {2021-04-29},
	journal = {International Statistical Review},
	author = {Benedetti, Roberto and Piersimoni, Federica and Postiglione, Paolo},
	year = {2017},
	keywords = {design-based inference, finite populations, spatial dependence, spatial statistics, spatial units},
	pages = {439--454},
}

@article{jr_spatially_2004,
	title = {Spatially {Balanced} {Sampling} of {Natural} {Resources}},
	volume = {99},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214504000000250},
	doi = {10.1198/016214504000000250},
	abstract = {The spatial distribution of a natural resource is an important consideration in designing an efficient survey or monitoring program for the resource. Generally, sample sites that are spatially balanced, that is, more or less evenly dispersed over the extent of the resource, are more efficient than simple random sampling. We review a unified strategy for selecting spatially balanced probability samples of natural resources. The technique is based on creating a function that maps two-dimensional space into one-dimensional space, thereby defining an ordered spatial address. We use a restricted randomization to randomly order the addresses, so that systematic sampling along the randomly ordered linear structure results in a spatially well-balanced random sample. Variable inclusion probability, proportional to an arbitrary positive ancillary variable, is easily accommodated. The basic technique selects points in a two-dimensional continuum, but is also applicable to sampling finite populations or one-dimensional continua embedded in two-dimensional space. An extension of the basic technique gives a way to order the sample points so that any set of consecutively numbered points is in itself a spatially well-balanced sample. This latter property is extremely useful in adjusting the sample for the frame imperfections common in environmental sampling.},
	number = {465},
	urldate = {2021-04-29},
	journal = {Journal of the American Statistical Association},
	author = {Jr, Don L. Stevens and Olsen, Anthony R.},
	month = mar,
	year = {2004},
	keywords = {Environmental sampling, Imperfect sampling frame, Monitoring, Non-response, Spatial sampling, Survey design, Systematic sampling},
	pages = {262--278},
}

@article{grafstrom_spatially_2012,
	title = {Spatially {Balanced} {Sampling} through the {Pivotal} {Method}},
	volume = {68},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2011.01699.x},
	doi = {https://doi.org/10.1111/j.1541-0420.2011.01699.x},
	abstract = {A simple method to select a spatially balanced sample using equal or unequal inclusion probabilities is presented. For populations with spatial trends in the variables of interest, the estimation can be much improved by selecting samples that are well spread over the population. The method can be used for any number of dimensions and can hence also select spatially balanced samples in a space spanned by several auxiliary variables. Analysis and examples indicate that the suggested method achieves a high degree of spatial balance and is therefore efficient for populations with trends.},
	language = {en},
	number = {2},
	urldate = {2021-04-29},
	journal = {Biometrics},
	author = {Grafström, Anton and Lundström, Niklas L. P. and Schelin, Lina},
	year = {2012},
	keywords = {Generalized random-tessellation stratified, Pivotal method, Spatial sampling, Unequal probability sampling},
	pages = {514--520},
}

@article{tille_ten_2011,
	title = {Ten years of balanced sampling with the cube method: {An} appraisal},
	shorttitle = {Ten years of balanced sampling with the cube method},
	abstract = {This paper presents a review and assessment of the use of balanced sampling by means of the cube method. After defining the notion of balanced sample and balanced sampling, a short history of the concept of balancing is presented. The theory of the cube method is briefly presented. Emphasis is placed on the practical problems posed by balanced sampling: the interest of the method with respect to other sampling methods and calibration, the field of application, the accuracy of balancing, the choice of auxiliary variables and ways to implement the method},
	language = {eng},
	journal = {Survey Methodology},
	author = {Tillé, Yves},
	year = {2011},
	pages = {215--226},
}

@article{deville_efficient_2004,
	title = {Efficient balanced sampling: {The} cube method},
	volume = {91},
	issn = {0006-3444},
	shorttitle = {Efficient balanced sampling},
	url = {https://doi.org/10.1093/biomet/91.4.893},
	doi = {10.1093/biomet/91.4.893},
	abstract = {A balanced sampling design is defined by the property that the Horvitz–Thompson estimators of the population totals of a set of auxiliary variables equal the known totals of these variables. Therefore the variances of estimators of totals of all the variables of interest are reduced, depending on the correlations of these variables with the controlled variables. In this paper, we develop a general method, called the cube method, for selecting approximately balanced samples with equal or unequal inclusion probabilities and any number of auxiliary variables.},
	number = {4},
	urldate = {2021-04-29},
	journal = {Biometrika},
	author = {Deville, Jean-Claude and Tillé, Yves},
	month = dec,
	year = {2004},
	pages = {893--912},
}

@article{madow_theory_1944,
	title = {On the {Theory} of {Systematic} {Sampling}, {I}},
	volume = {15},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2236209},
	number = {1},
	urldate = {2021-04-29},
	journal = {The Annals of Mathematical Statistics},
	author = {Madow, William G. and Madow, Lillian H.},
	year = {1944},
	pages = {1--24},
}

@article{iachan_systematic_1982,
	title = {Systematic {Sampling}: {A} {Critical} {Review}},
	volume = {50},
	issn = {0306-7734},
	shorttitle = {Systematic {Sampling}},
	url = {https://www.jstor.org/stable/1402499},
	doi = {10.2307/1402499},
	abstract = {The main purpose of this paper is to present the developments in systematic sampling posterior to the review by Buckland (1951). Emphasis is given to recent asymptotic and optimality results in the framework of superpopulation models /// Le but de cet article est de présenter les développements récents en la théorie de sondage systématique et ses applications. Les travaux des Madows (1944, 1946, 1949), de Cochran (1946) et de Yates (1948), déja couverts par le revue de Buckland (1951), forment le terrain sur lequel toute la théorie subséquente est édifiée. Les résultats asymptotiques et d'optimalité sont ici spécialement accentués. Enfin, quelques applications sont revues.},
	number = {3},
	urldate = {2021-04-29},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Iachan, Ronaldo},
	year = {1982},
	pages = {293--303},
}

@article{yates_systematic_1948,
	title = {Systematic sampling},
	volume = {241},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.1948.0023},
	doi = {10.1098/rsta.1948.0023},
	abstract = {This paper gives an account of the results of an investigation into one-dimensional systematic sampling, i.e. the sampling of sequences of quantitative values by the use of sampling points equally spaced along the sequence. New methods, using what are termed partial systematic samples, are evolved for estimating the systematic sampling error from short sections of sequences of completely enumerated numerical material. This gets over the difficulty, which previously existed, that the only estimates of the systematic sampling error of a numerical sequence, even when completely enumerated, were those provided by the actual deviations of the systematic samples of the whole sequence. Such deviations are few in number and by no means independent. Simple end-corrections are proposed for eliminating the errors, due to trend, which are otherwise inherent in randomly located systematic samples. It is demonstrated that it is impossible to make any fully reliable estimate of the sampling error from the systematic sampling results themselves, though if the continuous components of variation are not too marked, the sum of sets of terms taken alternately positive and negative, with suitable end adjustments, will provide a moderately satisfactory estimate, which will always be an overestimate provided there are no periodicities. This estimate is substantially better than the customary estimate based on successive differences. In other cases supplementary sampling is required to furnish an estimate of error, and methods are described whereby estimates can be derived from supplementary samples at half-spacing, or at half and quarter spacing. The performance of systematic sampling is investigated theoretically for certain mathematical functions, and also by the numerical analysis of certain numerical sequences. The mathematical functions investigated are (1) the two-valued function,/ ( a?) = 0 or 1, corresponding to sampling for attributes, (2) the normal error function, which corresponds to sampling for density with material normally distributed about a point in a line, and (3) the one-term autoregressive function yr+1=by?+a?? In the case of the two-valued function the relative performance of systematic and random samples is shown to depend on the lengths of the intervals of the function relative to the sampling interval. If these are small all forms of sampling are about of equal accuracy, but if they are large, systematic sampling is on the average twice as accurate as random sampling with one point per block, which is again twice as accurate as random sampling with two points per block. Similar results hold for the autoregressive function when b-*■ 1. In the case of the normal function, numerical analysis shows that systematic sampling over the whole of the curve is remarkably accurate in determining the integral of the curve. Mathematical reasons why this should be so are put forward. The sampling of part of the curve by systematic sampling is also investigated, and is used to demonstrate the value of end-corrections. The effect on the sampling errors of departures of actual density distributions from the normal form due to random variations in the material are evaluated. Numerical analyses are made of five numerical sequences: (1) 288 altitudes at 0-1 mile intervals along a grid line of a 1 in. O.S. map, (2) yields of 96 rows of potatoes, (3) 192 daily maximum screen temperature readings, (4) 192 soil temperature readings (9 a.m.) at 4 in., (5) 192 similar readings at 12 in. These analyses confirm the findings of the theoretical part of the investigation, and show that for these types of material the gain in precision with systematic sampling over stratified random sampling of the same intensity with one point per block is of the same order as the gain in precision with stratified random sampling with one point per block over stratified random sampling of the same intensity with two points per block, though the former tends to be larger in material of the more continuous type. The actual average ratios of the variances for the five sequences range from 1.26 to 2.99 in the first case, and T31 to T90 in the second. The relation between the gain in precision and the gain in efficiency is evaluated. The latter is always smaller owing to decrease in accuracy per point for a given method of sampling with decrease in intensity. Consideration of the relation between sampling costs and the losses due to errors in the sampling results shows, however, that with a more precise method of sampling greater accuracy should be demanded in the results. The danger of using systematic sampling in material about which nothing is known, or on material which may be subject to periodicities, is stressed, as is the importance in large-scale sampling investigations of making a preliminary investigation before instituting systematic sampling and of arranging for adequate control of error in the form of error estimates, with supplementary observations if necessary, in systematic sampling or stratified random sampling with one point per block. Control of this type should of course also be employed in stratified random sampling with two or more points per block, but in this case no special provisions are necessary, since valid estimates of error are always available from the sampling results themselves.},
	number = {834},
	urldate = {2021-04-29},
	journal = {Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences},
	author = {Yates, Frank and Thornton, Henry Gerard},
	month = sep,
	year = {1948},
	pages = {345--377},
}

@book{tille_sampling_2006,
	address = {New York, NY},
	title = {Sampling {Algorithms}},
	isbn = {9780387342405 9780387308142},
	language = {English},
	publisher = {Springer Science+Business Media, Inc.},
	author = {Tillé, Yves},
	year = {2006},
	note = {OCLC: 804629481},
}

@book{wolter_introduction_2007,
	address = {New York},
	edition = {2nd ed},
	series = {Statistics for social and behavioral sciences},
	title = {Introduction to variance estimation},
	isbn = {9780387329178},
	publisher = {Springer},
	author = {Wolter, Kirk M.},
	year = {2007},
	note = {OCLC: ocm86071306},
	keywords = {Analysis of variance, Estimation theory},
}

@article{hartley_sampling_1962,
	title = {Sampling with {Unequal} {Probabilities} and without {Replacement}},
	volume = {33},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177704564},
	doi = {10.1214/aoms/1177704564},
	language = {en},
	number = {2},
	urldate = {2021-04-29},
	journal = {The Annals of Mathematical Statistics},
	author = {Hartley, H. O. and Rao, J. N. K.},
	month = jun,
	year = {1962},
	pages = {350--374},
}

@article{sampford_sampling_1967,
	title = {On sampling without replacement with unequal probabilities of selection},
	volume = {54},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/54.3-4.499},
	doi = {10.1093/biomet/54.3-4.499},
	abstract = {A sample of n different units is to be drawn from a population or stratum in such a way that unit i has probability npi, assumed less than 1, of appearing in the sample. A mathematical solution of this problem is given by a formula from which the required probability of selection of any possible sample can be calculated: this formula is an extension of one, due to Durbin, for n = 2. The required npican be achieved in practice in three ways: (a) by evaluating the required probabilities for all possible samples, and selecting one; (b) selecting units without replacement, with probabilities of selection that must be recalculated after each drawing; and (c) by selecting up to n units with replacement, the first drawing being made with probabilities pi, and all subsequent ones with probabilities proportional to pi/(1−npi), and rejecting completely any sample that does not contain n different units. Method (c) seems likely to be the most convenient in practice. The probability of the simultaneous appearance in the sample of any pair of units is relatively easily calculated, so that unbiased variance estimates can be obtained without undue labour.},
	number = {3-4},
	urldate = {2021-04-29},
	journal = {Biometrika},
	author = {SAMPFORD, M.R.},
	month = dec,
	year = {1967},
	pages = {499--513},
}

@article{yates_selection_1953,
	title = {Selection {Without} {Replacement} from {Within} {Strata} with {Probability} {Proportional} to {Size}},
	volume = {15},
	issn = {2517-6161},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1953.tb00140.x},
	doi = {https://doi.org/10.1111/j.2517-6161.1953.tb00140.x},
	abstract = {In selection with probability proportional to size x from within strata without replacement, the usual method of selection gives rise to bias in the estimate of the total of a variate y derived by weighting the units by weights proportional to 1/x. By means of numerical examples it is shown that the amount of this bias is usually quite trivial. If, however, unbiased estimates are required, the true total probabilities of selection of the different units can be calculated easily for samples of 2, and with considerably more labour for samples of 3. The bias in the ordinary formula for the estimation of error is also investigated, and the formula is shown to be reasonably accurate. Horvitz and Thompson have given an unbiased estimator of the error variance, but this is shown to be inefficient and a new unbiased estimator is given. A method of revising the size measures so that with the usual method of selection the true total probabilities of selection are proportional to the original size measures is given for samples of 2. Horvitz and Thompson's solution of this problem does not appear to give satisfactory approximations in the cases met with in practice. The selection of successive members of a sample with arbitrary sets of probabilities chosen solely so that the total probabilities shall be proportional to the original size measures, which has been advocated in various quarters, is criticized.},
	language = {en},
	number = {2},
	urldate = {2021-04-29},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Yates, F. and Grundy, P. M.},
	year = {1953},
	pages = {253--261},
}

@article{hajek_asymptotic_1964,
	title = {Asymptotic {Theory} of {Rejective} {Sampling} with {Varying} {Probabilities} from a {Finite} {Population}},
	volume = {35},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2238287},
	abstract = {In [3] the author established necessary and sufficient conditions for asymptotic normality of estimates based on simple random sampling without replacement from a finite population, and thus solved a comparatively old problem initiated by W. G. Madow [8]. The solution was obtained by approximating simple random sampling by so called Poisson sampling, which may be decomposed into independent subexperiments, each associated with a single unit in the population. In the present paper the same method is used for deriving asymptotic normality conditions for a special kind of sampling with varying probabilities called here rejective sampling. Rejective sampling may be realized by n independent draws of one unit with fixed probabilities, generally varying from unit to unit, given the condition that samples in which all units are not distinct are rejected. If the drawing probabilities are constant, rejective sampling coincides, with simple random sampling without replacement, and so the present paper is a generalization of [3]. Basic facts about rejective sampling are exposed in Section 2. To obtain more refined results, Poisson sampling is introduced and analyzed (Section 3) and then related to rejective sampling (Section 4). Next three sections deal with probabilities of inclusion, variance formulas and asymptotic normality of estimators for rejective sampling. In Section 8 asymptotic formulas are tested numerically and applications to sample surveys are indicated. The paper is concluded by short-cuts in practical performance of rejective sampling. The readers interested in applications only may concentrate upon Sections 1, 8 and 9. Those interested in the theory of mean values and variances only, may omit Lemma 4.3 and Section 7.},
	number = {4},
	urldate = {2021-04-29},
	journal = {The Annals of Mathematical Statistics},
	author = {Hájek, Jaroslav},
	year = {1964},
	pages = {1491--1523},
}

@inproceedings{planchat_articulation_2018,
	address = {Paris},
	title = {Articulation des tirages des enquêtes ménages dans les {DOM}},
	volume = {9},
	url = {http://www.jms-insee.fr/2018/S09_5_ACTE_PLANCHAT_JMS2018.pdf},
	abstract = {In this article, we will focus on the survey methodology adaptation to a fast evolving survey plan: the extension of the rents and charges survey to the four overseas departments and the generalisation of small sample in order to represent the national level. In addition to this evolution we will discuss a new opportunity : drawing sample in tax sources.},
	language = {fr},
	urldate = {2021-03-18},
	booktitle = {Journées de {Méthodologie} {Statistique}},
	author = {Planchat, Cédric and Martinoz, Cyril FAVRE and Sampietro, Oriane LAFUENTE},
	year = {2018},
	pages = {1--27},
}

@inproceedings{fleuret_quinze_2015,
	title = {Quinze ans d'enquêtes auprès des ménages dans les départements d'{Outre}-{Mer}},
	volume = {13},
	url = {http://jms-insee.fr/jms2015s13_1/},
	abstract = {In the past 15 years, household surveys in French overseas territories of Guadeloupe, Martinique, Guyane, La Réunion and Mayotte achieved a revolution, jumping from an antic stage to a modern one. This paper aim to provide a comprehensive overview of this period, in terms of surveys and methods, and to present the new sampling system used since 2013.},
	language = {fr},
	booktitle = {Journées de {Méthodologie} {Statistique}},
	author = {Fleuret, Aurore and Torterat, Jérémie},
	year = {2015},
	pages = {1--86},
}

@article{bring_how_1994,
	title = {How to {Standardize} {Regression} {Coefficients}},
	volume = {48},
	issn = {00031305},
	url = {https://www.jstor.org/stable/2684719?origin=crossref},
	doi = {10.2307/2684719},
	number = {3},
	urldate = {2021-02-03},
	journal = {The American Statistician},
	author = {Bring, Johan},
	month = aug,
	year = {1994},
	pages = {209},
}

@article{pfeffermann_role_1993,
	title = {The {Role} of {Sampling} {Weights} {When} {Modeling} {Survey} {Data}},
	volume = {61},
	url = {http://www.jstor.org/stable/1403631},
	abstract = {The purposeof this paperis to providea criticalsurveyof the literature,directedat answering two mainquestions.i) Canthe use of the samplingweightsbe justifiedfor analyticinferenceabout modelparametersand if so, underwhatcircumstances?ii) Can guidelinesbe developedfor how to incorporatethe weightsin the analysis?The generalconclusionof this studyis that the weightscan be used to test and protectagainstinformativesamplingdesignsand againstmisspecificationof the model holding in the population. Six approachesfor incorporatingthe weights in the inference processare considered.The firstfour approachesare intendedto yield designconsistentestimators for correspondingdescriptive population quantities of the model parameters. The other two approachesattemptto incorporatethe weightsinto the model.},
	language = {en},
	number = {2},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Pfeffermann, Danny},
	year = {1993},
	pages = {317--337},
}

@article{elhorst_applied_2010,
	title = {Applied {Spatial} {Econometrics}: {Raising} the {Bar}},
	volume = {5},
	issn = {1742-1772, 1742-1780},
	shorttitle = {Applied {Spatial} {Econometrics}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/17421770903541772},
	doi = {10.1080/17421770903541772},
	abstract = {This paper places the key issues and implications of the new ‘introductory’ book on spatial econometrics by James LeSage \& Kelley Pace (2009) in a broader perspective: the argument in favour of the spatial Durbin model, the use of indirect effects as a more valid basis for testing whether spatial spillovers are significant, the use of Bayesian posterior model probabilities to determine which spatial weights matrix best describes the data, and the book’s contribution to the literature on spatiotemporal models. The main conclusion is that the state of the art of applied spatial econometrics has taken a step change with the publication of this book.},
	language = {en},
	number = {1},
	urldate = {2020-11-02},
	journal = {Spatial Economic Analysis},
	author = {Elhorst, J. Paul},
	month = mar,
	year = {2010},
	keywords = {Durbin, Spatial Econometrics, Tests},
	pages = {9--28},
}

@article{gobillon_determinants_2007,
	title = {Les déterminants locaux du chômage en région parisienne},
	volume = {180-181},
	url = {https://www.cairn.info/revue-economie-et-prevision-1-2007-4-page-19.htm},
	doi = {10.3917/ecop.180.0019},
	abstract = {Cet article étudie l'effet de la ségrégation résidentielle et de la déconnexion physique aux lieux d'emploi sur le chômage. 
Nous présentons tout d'abord une brève synthèse de la littérature économique portant sur ces questions. Nous proposons
ensuite des statistiques descriptives caractérisant l'ampleur de la ségrégation résidentielle et de la déconnexion physique
aux emplois en Île-de-France en utilisant les données du recensement de 1999 et des matrices de temps de déplacement
intercommunal de2000. Nousestimons finalementl'effet ducontexte local (ségrégationetdéconnexion aux emplois) sur
les transitions chômage/emploi en Île-de-France en utilisant les données de l'Enquête Emploi de 1990 à 2002. Nos
résultats montrent que les chômeurs des quartiers ségrégués ont plus de mal à retrouver un emploi.},
	language = {FR},
	number = {4-5},
	journal = {Économie \& prévision},
	author = {Gobillon, Laurent and Selod, Harris},
	year = {2007},
	note = {Place: Paris
Publisher: La Documentation française},
	pages = {19--38},
}

@article{lemoine_alain_2009,
	title = {Alain {Desrosières}, {L}'{Argument} statistique. {Pour} une sociologie historique de la quantification (tome {I}) et {Gouverner} par les nombres (tome {II}). {Paris}, {Presses} de l'école des {Mines}, 2008},
	volume = {3,  2},
	url = {https://www.cairn.info/revue-anthropologie-des-connaissances-2009-2-page-359.htm},
	doi = {10.3917/rac.007.0359},
	language = {FR},
	number = {2},
	journal = {Revue d'anthropologie des connaissances},
	author = {Lemoine, Benjamin},
	year = {2009},
	note = {Place: Grenoble
Publisher: S.A.C.},
	pages = {359--365},
}

@misc{insee_documentation_2019,
	title = {Documentation sur la méthodologie − {Enquête} emploi en continu 2019},
	urldate = {2020-09-13},
	author = {{Insee}},
	month = aug,
	year = {2019},
}

@misc{noauthor_nouvelle_nodate,
	title = {Une nouvelle {Enquête} {Emploi} − Économie et {Statistique} n° 362 - 2003 {\textbar} {Insee}},
	url = {https://www.insee.fr/fr/statistiques/1376196?sommaire=1376203},
	urldate = {2020-09-13},
}

@article{bunel_conjoints_2004,
	title = {Les conjoints des salariés passés à 35 heures travaillent-ils davantage ?{Une} analyse de l'offre de travail familiale sur données françaises},
	volume = {164-165},
	shorttitle = {Les conjoints des salariés passés à 35 heures travaillent-ils davantage ?},
	url = {https://www.cairn.info/revue-economie-et-prevision-1-2004-3-4-page-165.htm},
	doi = {10.3917/ecop.164.0165},
	abstract = {Cet article étudie l’évolution de l’offre de travail familiale consécutive à la diffusion des 35 heures en s’appuyant sur un  échantillon original de plus de 10 000 couples issu de l’enquête Emploi 2000. Deux dimensions de l’offre de travail de  l’individu sont étudiées successivement lorsque son conjoint est affecté par la réduction du temps de travail : l’intensité de  la participation au marché du travail et la décision de passer du statut d’inactif à celui d’actif. Les résultats obtenus  montrent que les salariés dont le conjoint passe à 35 heures ont une probabilité plus élevée de passer du statut d’inactif à  celui d’actif. Ainsi, la diffusion des 35 heures peut permettre d’accroître le taux d’activité dans l’économie française.},
	language = {FR},
	number = {3-4},
	journal = {Économie \& prévision},
	author = {Bunel, Matthieu},
	year = {2004},
	note = {Place: Paris
Publisher: La Documentation française},
	pages = {165--188},
}

@article{zilloniz_lactivite_2017,
	title = {‪{L}’activité rémunérée des étudiants et ses liens avec la réussite des études‪. {Les} enseignements des enquêtes {Emploi} 2013-2015},
	volume = {152},
	shorttitle = {‪{L}’activité rémunérée des étudiants et ses liens avec la réussite des études‪},
	url = {https://www.cairn.info/revue-travail-et-emploi-2017-4-page-89.htm},
	doi = {10.4000/travailemploi.7776},
	abstract = {Les emplois étudiants revêtent de nombreuses formes. L’un des critères centraux pour les différencier est leur lien ou non avec les études suivies. Parmi les étudiants qui travaillent, plus de la moitié exerce une activité prévue par leurs études (stage, apprentissage, internat de médecine, etc.) d’après les enquêtes Emploi 2013-2015 de l’Insee. Les autres exercent une activité sans lien avec leurs études, occasionnellement ou régulièrement tout au long de l’année. Celle-ci correspond à des emplois moins qualifiés et plus souvent à temps partiel que lorsque le lien avec les études est avéré, ce qui n’exclut pas qu’elle puisse représenter une charge horaire lourde et contraignante vis-à-vis de l’emploi du temps studieux et freiner la réussite des études. Les enfants d’ouvriers, ayant plus souvent que les autres un emploi étudiant non lié aux études et dont la réussite dans l’enseignement supérieur est souvent présentée comme fragile, seraient ainsi particulièrement pénalisés.},
	language = {FR},
	number = {4},
	journal = {Travail et emploi},
	author = {Zilloniz, Sandra},
	year = {2017},
	note = {Place: Paris
Publisher: DARES},
	pages = {89--117},
}

@article{rathelot_origine_2010,
	title = {Origine et quartier. {Expliquer} le salaire et l'emploi des descendants d'immigrés},
	volume = {mars},
	issn = {9782200926083},
	url = {https://www.cairn.info/revue-d-economie-regionale-et-urbaine-2010-1-page-27.htm},
	doi = {10.3917/reru.101.0027},
	abstract = {Ce numéro spécial de la Revue d’Économie Régionale et Urbaine est issu du colloque« Ségrégation urbaine et accès à l’emploi », qui a été organisé à l’Université de Paris-Est Marne-la-Vallée les 15 et 16 septembre 2008 par la fédération de recherche du CNRS « Travail, Emploi et Politiques publiques » (FR n! 3126). Au-delà des 7 unités de recherche qui composent la fédération TEPP (Centre d’Études de l’Emploi, ERMES-UP2, EPEE et Centre Pierre NAVILLE-UEVE, OEP-UPEMLV, ERUDITE-UPEC, GAINS-U du Maine), cette manifestation a eu comme partenaire scientifique les laboratoires LATTS de l’ENPC, CRETEIL de l’UPEC, Ville Mobilité Transport (ENPC, UPEMLV, INRETS) et l’InstitutFrançais d’Urbanisme (UP VIII). Les partenaires institutionnels de la manifestation étaient le CNRS, l’Agence Nationale pour la Cohésion Sociale et l’égalité des chances (ACSé), la Région Ile-de-France, la Préfecture de la Région Ile de France, et la Direction régionale du travail, de l’Emploi, et de la formation Professionnelle d’Ile de France. Le colloque a réuni près de 150 participants et les articles qui composent ce numéro ont été sélectionnés parmi la trentaine de contributions présentées},
	language = {FR},
	number = {1},
	journal = {Revue d’Économie Régionale \& Urbaine},
	author = {Rathelot, Roland},
	year = {2010},
	note = {Place: Paris
Publisher: Armand Colin},
	pages = {27--55},
}

@article{aeberhardt_differences_2013,
	title = {Les différences liées à l'origine nationale sur le marché du travail français},
	volume = {XXVIII},
	url = {https://www.cairn.info/revue-francaise-d-economie-2013-1-page-43.htm},
	doi = {10.3917/rfe.131.0043},
	abstract = {Cet article propose une synthèse de la question des écarts ethniques de salaires et de taux d’emploi sur le marché du travail français en utilisant un cadre empirique homogène et l’enquête Emploi 2005-2011. Les écarts de salaires, même quand ils sont élevés, sont essentiellement dus à des différences d’âge et de diplôme tandis que les écarts de taux d’emploi ne peuvent s’expliquer de cette manière. Ce résultat est en particulier robuste à la prise en compte du lieu de résidence à un niveau très fin. Par ailleurs, les écarts sont hétérogènes suivant les caractéristiques individuelles (diplôme, âge, sexe...).},
	language = {FR},
	number = {1},
	journal = {Revue française d'économie},
	author = {Aeberhardt, Romain and Rathelot, Roland},
	year = {2013},
	note = {Place: Paris
Publisher: Revue française d’économie},
	pages = {43--71},
}

@article{devetter_travailler_2008,
	title = {Travailler au-delà de 48 heures par semaine},
	volume = {114},
	url = {https://www.cairn.info/revue-travail-et-emploi-2008-2-page-59.htm},
	doi = {10.4000/travailemploi.1955},
	abstract = {Bien que moins fréquentes que dans les pays anglo-saxons, les très longues durées de travail perdurent en France. En 2005, près de 9 \% des salariés travaillent 48 heures ou plus par semaine. Loin d’être homogène, la population concernée regroupe au moins deux catégories très différentes et qui seront au cœur de l’analyse. D’un côté, il s’agit principalement de cadres et professions intermédiaires d’entreprise qui offrent de longues durées de travail mais qui en retour obtiennent certaines compensations. De l’autre, on retrouve des employés et ouvriers peu ou non qualifiés dont la disponibilité ne semble pas véritablement reconnue.},
	language = {FR},
	number = {2},
	journal = {Travail et emploi},
	author = {Devetter, François-Xavier},
	year = {2008},
	note = {Place: Paris
Publisher: DARES},
	pages = {59--70},
}

@article{gouyon_vingt_2014,
	title = {Vingt ans d'évolution de l'emploi dans les professions culturelles (1991-2011)},
	volume = {6},
	url = {https://www.cairn.info/revue-culture-chiffres-2014-6-page-1.htm},
	doi = {10.3917/culc.146.0001},
	abstract = {Au cours des vingt dernières années, les effectifs en emploi des professions
culturelles ont connu une expansion exceptionnelle, bien plus importante
que celle de l’ensemble des actifs.
Le profil des personnes qui exercent ces professions et celui des emplois
qu’elles occupent présentaient, au début des années 1990 tout comme
aujourd’hui, certaines caractéristiques d’ensemble qui distinguent cette
population professionnelle du reste des actifs, au-delà de la grande diversité
des métiers qu’elle rassemble. C’est ainsi une population plutôt plus jeune
que la moyenne des actifs, plus masculine, nettement plus diplômée et plus
francilienne. L’emploi y est globalement marqué à la fois par le poids important,
constant depuis vingt ans, du non-salariat (trois fois plus que dans
l’ensemble de la population active) et par une plus grande flexibilité du
salariat (davantage de contrats courts, de temps partiel et de pluriactivité).
Pour autant, les professions culturelles ne sont pas restées imperméables à
certaines évolutions qui ont marqué l’ensemble du monde du travail depuis
le début des années 1990. Le mouvement continu de féminisation des
emplois, par exemple, s’y est opéré dans les mêmes proportions que dans
l’ensemble de la population active, amenant certaines professions
culturelles traditionnellement très masculines à connaître un doublement
de la part de leurs effectifs féminins en vingt ans.
En outre, les efforts des politiques publiques de décentralisation ont
conduit à une légère atténuation de la concentration francilienne de ces
professions. D’autres évolutions communes à l’ensemble des actifs comme
l’élévation continue du niveau de diplôme et le développement de la flexibilité
de l’emploi – deux caractéristiques attachées de longue date au monde
de l’emploi culturel – ont continué à s’y développer, dans des proportions
toujours supérieures à celles observées dans le reste de la population active.},
	language = {FR},
	number = {6},
	journal = {Culture chiffres},
	author = {Gouyon, Marie and Patureau, Frédérique},
	year = {2014},
	note = {Place: Paris
Publisher: Ministère de la Culture - DEPS},
	pages = {1--24},
}

@article{gouyon_lente_2016,
	title = {La lente féminisation des professions culturelles},
	volume = {2},
	url = {https://www.cairn.info/revue-culture-etudes-2016-2-page-1.htm},
	doi = {10.3917/cule.162.0001},
	abstract = {Depuis les années 1960, la croissance du taux d’activité féminine est l’une des transformations majeures du monde du travail, et les professions culturelles n’y font pas exception : la part des femmes au sein de ces professions est ainsi passée de 39\% à 43\% de 1991 à 2013. Presque tous les métiers fortement masculins au début des années 1990 comme les métiers d’art, les architectes et les photographes, par exemple, se sont ouverts aux femmes. Pour autant, la part des femmes demeure inférieure à la moyenne nationale, où l’activité féminine atteint 48\% de l’ensemble des professions, un constat qui peut surprendre au regard de la surreprésentation des femmes en termes de participation culturelle.
En se développant, l’emploi féminin a largement épousé les spécificités de l’emploi propres aux professions culturelles, notamment leur plus grande flexibilité : les professionnelles de la culture sont, plus souvent que l’ensemble des actives, non salariées ou salariées sur contrats à durée limitée. Elles travaillent plus souvent à temps partiel, avec des horaires variables et atypiques, en soirée et le week-end.
Surtout, leur accès aux professions artistiques et culturelles conforte la règle de la surqualification des femmes en termes de niveau de diplôme : en 2013, plus d’une active de ces professions sur deux est titulaire d’un diplôme égal ou supérieur à bac + 3, contre 40\% de leurs homologues masculins – une surqualification d’autant plus forte qu’il s’agit de métiers traditionnellement exercés par des hommes.},
	language = {FR},
	number = {2},
	journal = {Culture études},
	author = {Gouyon, Marie and Patureau, Frédérique and Volat, Gwendoline},
	year = {2016},
	note = {Place: Paris
Publisher: Ministère de la Culture - DEPS},
	pages = {1--20},
}

@article{afsa_roconditions_2009,
	title = {Le rôle des conditions de travail dans les absences pour maladie : le cas des horaires irréguliers},
	volume = {187},
	url = {https://www.cairn.info/revue-economie-et-prevision-1-2009-1-page-83.htm},
	doi = {10.3917/ecop.187.0083},
	abstract = {Souvent négligées par la littérature économique, les conditions de travail sont susceptibles d’influencer les comportements d’activité. Cette étude s’intéresse en particulier à leur impact sur les absences pour maladie. Le modèle théorique développé dans une première partie suggère deux effets contradictoires : de mauvaises conditions de travail dégradent l’état de santé et accroissent les absences pour maladie, mais l’absentéisme peut être inversement freiné par un effet salaire, si les conditions de travail défavorables sont compensées par un salaire plus élevé. L’évaluation empirique, dans le cas spécifique des horaires irréguliers pour les ouvriers travaillant dans le secteur privé, montre que le premier effet prédomine, particulièrement aux âges élevés.},
	language = {FR},
	number = {1},
	journal = {Économie \& prévision},
	author = {Afsa, Cédric and Givord, Pauline},
	year = {2009},
	note = {Place: Paris
Publisher: La Documentation française},
	pages = {83--103},
}

@article{narcy_quel_2020,
	title = {Quel est l’effet d’une réduction de la durée d’indemnisation du congé parental sur l’activité des mères ? {Une} évaluation de la réforme de 2015},
	volume = {Prépublication},
	url = {https://www.cairn.info/revue-economique-2020-7-page-553.htm},
	abstract = {L’objectif de cet article est d’étudier dans quelle mesure la réforme du congé parental, entrée en vigueur en janvier 2015, a pu modifier la probabilité des mères d’avoir recours à ce dispositif, l’indemnisation ne couvrant plus désormais l’intégralité de la période allant de la naissance jusqu’à la scolarisation de l’enfant. Pour identifier l’effet causal de cette réforme, nous exploitons les données de l’enquête Emploi et mettons en œuvre une méthodologie combinant régression sur discontinuité et doubles différences. Cette réforme a eu pour conséquence de réduire la probabilité de recourir au congé parental à temps plein d’environ 10 points de pourcentage. En outre, les mères les plus particulièrement affectées par cette réforme sont les moins diplômées et les salariées du secteur privé.Classification JEL : J13, J16, J18, J22.},
	language = {FR},
	number = {7},
	journal = {Revue économique},
	author = {Narcy, Mathieu and Sari, Florent},
	year = {2020},
	note = {Place: Paris
Publisher: Presses de Sciences Po},
	pages = {553--583},
}

@book{desrosieres_politique_2014,
	address = {Paris},
	title = {La politique des grands nombres: histoire de la raison statistique},
	isbn = {978-2-7071-6504-6},
	shorttitle = {La politique des grand nombres},
	language = {French},
	publisher = {La Découverte},
	author = {Desrosières, Alain},
	year = {2014},
	note = {OCLC: 921231811},
}

@book{desrosieres_politique_2014-1,
	address = {Paris},
	title = {La politique des grand nombres: histoire de la raison statistique},
	isbn = {978-2-7071-6504-6},
	shorttitle = {La politique des grand nombres},
	language = {French},
	publisher = {La Découverte},
	author = {Desrosières, Alain},
	year = {2014},
	note = {OCLC: 921231811},
}

@book{frege_ecrits_1994,
	address = {Paris},
	series = {Collection points série essais},
	title = {Ecrits logiques et philosophiques},
	isbn = {978-2-02-022966-1},
	language = {fre},
	number = {296},
	publisher = {Seuil, 1994},
	author = {Frege, Gottlob},
	year = {1994},
	note = {OCLC: 247966694},
	keywords = {Philosophie},
}

@book{jouanna_saint-barthelemy_nodate,
	series = {Collection {Folio} {Histoire}},
	title = {La {Saint}-{Barthélemy}: les mystères d'un crime d'État (24 août 1572)},
	isbn = {978-2-07-274886-8},
	shorttitle = {La {Saint}-{Barthélemy}},
	language = {fre},
	number = {268},
	author = {Jouanna, Arlette},
	note = {OCLC: 1011670421},
	keywords = {Histoire, France, Histoire, Guerre des religions, Histoire, XVIe},
}

@book{supiot_homo_2009,
	address = {Paris},
	title = {Homo juridicus essai sur la fonction anthropologique du droit},
	isbn = {978-2-7578-1520-5},
	language = {French},
	publisher = {Éditions du Seuil},
	author = {Supiot, Alain},
	year = {2009},
	note = {OCLC: 495350645},
	keywords = {Anthropologie, Droit, Droits de l'Homme},
}

@article{courtois_linstitution_2007,
	title = {L’institution du lien social : À propos des ouvrages d’{Alain} {Supiot}, {Homo} {Juridicus}. {Essai} sur la fonction anthropologique du droit et {Tisser} le lien social, {Alain} {Supiot} (éd). {Paris}, Édition du {Seuil}, 2005, 333p. et {Paris}, Édition de la {Maison} des sciences de l’homme, 2004, vi + 370 pages},
	copyright = {Droits et Culture est mis à disposition selon les termes de la licence Creative Commons Attribution - Pas d'Utilisation Commerciale - Pas de Modification 4.0 International.},
	issn = {0247-9788},
	shorttitle = {L’institution du lien social},
	url = {http://journals.openedition.org/droitcultures/1909},
	abstract = {Alain Supiot, à côté de nombreuses publications en droit social, poursuit une réflexion qui concerne le droit en tant que tel, comme en témoignent ses ouvrages : « Homo Juridicus » et « Tisser le lien social ». Le second présente des contributions issues du séminaire pluri-disciplinaire tenu depuis de nombreuses années à la Maison des sciences de l’Homme Ange-Guépin, le premier –auquel nous nous tiendrons- expose en deux parties « nos croyances fondatrices » et trois études présentées comme d...},
	language = {fr},
	number = {54},
	urldate = {2020-08-24},
	journal = {Droit et cultures. Revue internationale interdisciplinaire},
	author = {Courtois, Gérard},
	month = dec,
	year = {2007},
	note = {ISBN: 9782296049109
Number: 54
Publisher: L’Harmattan},
	keywords = {Anthropologie, Droit, Institutions},
	pages = {243--249},
}

@book{braudel_mediterranee_2010,
	address = {Paris},
	edition = {9. éd., 1990},
	series = {Le livre de poche {Références}},
	title = {La {Méditerranée} et le monde méditerranéen à l'époque de {Philippe} {II}  -  1: {La} part du milieu},
	volume = {1},
	isbn = {978-2-253-06168-7},
	shorttitle = {La méditerranée et le monde méditerranéen à l'époque de {Philippe} {II}. 1},
	language = {fre},
	publisher = {Colin},
	author = {Braudel, Fernand},
	year = {2010},
	note = {OCLC: 837525712},
	keywords = {Histoire, Espagne, Histoire, Méditerranée, Histoire, XVIe},
}

@book{braudel_mediterranee_2006,
	address = {Paris},
	edition = {9. éd., 1990},
	title = {La {Méditerranée} et le monde méditerranéen à l'époque de {Philippe} {II}  -  3: {Les} événements, la politique et les hommes},
	volume = {3},
	isbn = {978-2-253-06170-0},
	language = {fre},
	publisher = {Armand Colin Livre de poche},
	author = {Braudel, Fernand},
	year = {2006},
	note = {OCLC: 837192532},
	keywords = {Histoire, Espagne, Histoire, Europe, Histoire, Méditerranée, Histoire, XVIe},
}

@book{braudel_mediterranee_2006-1,
	address = {Paris},
	edition = {9. éd., 1990},
	title = {La {Méditerranée} et le monde méditerranéen à l'époque de {Philippe} {II}  -  2. {Destins} collectifs et mouvements d'ensemble},
	volume = {2},
	isbn = {978-2-253-06169-4},
	language = {fre},
	publisher = {Armand Colin Livre de poche},
	author = {Braudel, Fernand},
	year = {2006},
	note = {OCLC: 837192497},
	keywords = {Histoire, Europe, Histoire, Méditerranée, Histoire, XVIe},
}

@book{hopkirk_grand_2013,
	address = {Bruxelles},
	title = {Le grand jeu officiers et espions en {Asie} centrale},
	isbn = {978-2-87523-046-1},
	language = {French},
	publisher = {Editions Nevicata},
	author = {Hopkirk, Peter},
	editor = {Weber, Olivier},
	translator = {Hemptinne, Gerald de},
	year = {2013},
	note = {OCLC: 863605048},
	keywords = {Histoire, Grande-Bretagne, Histoire, Indes coloniales, Histoire, Iran, Histoire, Russie, Histoire, XIXe, Histoire, espionnage},
}

@book{hobsbawm_lage_2017,
	title = {L'âge des extrêmes: histoire du court {XXe} siècle 1914-1991},
	isbn = {978-2-87495-011-7},
	shorttitle = {L'âge des extrêmes},
	language = {French},
	author = {Hobsbawm, Eric J and Leasa, André},
	year = {2017},
	note = {OCLC: 1089441611},
	keywords = {Histoire, XXe},
}

@book{hobsbawm_bandits_2008,
	address = {Paris},
	title = {Les bandits},
	isbn = {978-2-35522-013-5},
	abstract = {En historien et sociologue, l'auteur retrace l'histoire mouvementée du "banditisme social" et s'efforce d'inscrire le destin de ces hors-la-loi dans une étude plus large des structures économiques et sociales qui conditionnent leur apparition.},
	language = {French},
	publisher = {Zones-la Découverte},
	author = {Hobsbawm, Eric John and Rospars, Jean-Pierre and Guilhot, Nicolas},
	year = {2008},
	note = {OCLC: 470947933},
	keywords = {Histoire},
}

@book{camus_premier_2012,
	address = {Paris},
	edition = {Réimpr.},
	series = {Folio},
	title = {Le premier homme},
	isbn = {978-2-07-040101-7},
	language = {fre},
	number = {3320},
	publisher = {Gallimard},
	author = {Camus, Albert},
	year = {2012},
	note = {OCLC: 951286395},
	keywords = {Littérature, XXe},
}

@book{hugo_quatrevingt-treize_2001,
	address = {Paris},
	title = {Quatrevingt-treize},
	isbn = {978-2-253-16078-6},
	language = {French},
	publisher = {Librairie Générale Française},
	author = {Hugo, Victor and Leuilliot, Bernard},
	year = {2001},
	note = {OCLC: 1166833369},
	keywords = {Littérature, XIXe},
}

@book{boorstin_histoire_1981,
	address = {Paris},
	title = {Histoire des {Américains}  - 2. {Naissaince} d'une {Nation}},
	volume = {2},
	publisher = {Armand Colin Editeur},
	author = {Boorstin, Daniel},
	translator = {Lemeunier, Yves and Blanc, Marcel and Christol, Hélène and Crapoulet, Jean-Claude and Gervaud, Michel and Guieu, Yves},
	year = {1981},
	keywords = {Histoire, Etats-Unis, Histoire, USA},
}

@book{boorstin_histoire_1981-1,
	address = {Paris},
	title = {Histoire des {Américains} - 3. {L}'{Expérience} démocratique},
	volume = {3},
	publisher = {Armand Colin Editeur},
	author = {Boorstin, Daniel},
	translator = {Lemeunier, Yves and Blanc, Marcel and Christol, Hélène and Crapoulet, Jean-Claude and Gervaud, Michel and Guieu, Yves},
	year = {1981},
}

@book{braudel_civilisation_1993,
	address = {Paris},
	title = {Civilisation, économie et capitalisme: {XVe}-{XVIIIe} siècle},
	volume = {2},
	isbn = {978-2-253-06455-8 978-2-253-06456-5 978-2-253-06457-2},
	shorttitle = {Civilisation, économie et capitalisme},
	language = {French},
	publisher = {Armand Colin Livre de poche},
	author = {Braudel, Fernand},
	year = {1993},
	note = {OCLC: 492174037},
}

@book{bairoch_mythes_1999,
	address = {Paris},
	title = {Mythes et paradoxes de l' histoire économique},
	isbn = {978-2-7071-2926-0 978-2-7071-4840-7},
	language = {French},
	publisher = {La Découverte/Poche},
	author = {Bairoch, Paul},
	year = {1999},
	note = {OCLC: 318373975},
	keywords = {Histoire économique, Krach de 1929, Libre-échange, Protectionnisme},
}

@book{hervieu_au_2005,
	address = {La Tour-d'Aigues},
	title = {Au bonheur des campagnes},
	isbn = {978-2-87678-547-2},
	language = {French},
	publisher = {Editions de l'Aube},
	author = {Hervieu, Bertrand and Viard, Jean},
	year = {2005},
	note = {OCLC: 949083553},
	keywords = {Sociologie, urbain/rural},
}

@book{soboul_revolution_2010,
	address = {Paris},
	edition = {3. éd},
	series = {Quadrige {Grands} textes},
	title = {La {Révolution} française},
	isbn = {978-2-13-058067-6},
	language = {fre},
	publisher = {PUF},
	author = {Soboul, Albert},
	year = {2010},
	note = {OCLC: 845873180},
	keywords = {Histoire, Révolution Française},
}

@book{lordon_imperium_2015,
	address = {Paris},
	title = {Imperium: structures et affects des corps politiques},
	isbn = {978-2-35872-070-0},
	shorttitle = {Imperium},
	publisher = {La fabrique éditions},
	author = {Lordon, Frédéric},
	year = {2015},
	keywords = {Economic aspects, Philosophy, Political science, Power (Philosophy), Social contract, State, The},
}

@book{beaufret_parmenide_1996,
	address = {Paris},
	series = {Quadrige},
	title = {Parménide: le poème},
	isbn = {978-2-13-047755-6},
	shorttitle = {Parménide},
	language = {French},
	number = {214},
	publisher = {Presses universitaires de France},
	author = {Beaufret, Jean},
	year = {1996},
	note = {OCLC: 611503163},
	keywords = {Philosophy, Ancient},
}

@book{zweig_montaigne_2012,
	address = {Paris},
	edition = {6. édition},
	series = {Quadrige},
	title = {Montaigne},
	isbn = {978-2-13-060765-6},
	language = {fre},
	publisher = {PUF},
	author = {Zweig, Stefan},
	translator = {Jaccard, Roland},
	year = {2012},
	note = {OCLC: 844035094},
	keywords = {Biographie, Littérature, XXe, Montaigne, Michel de},
}

@book{girard_choses_2009,
	address = {Paris},
	series = {Livre de poche {Biblio} {Essais}},
	title = {Des choses cachées depuis la fondation du monde: recherches avec {Jean}-{Michel} {Oughourlian} et {Guy} {Fort}},
	isbn = {978-2-253-03244-1},
	shorttitle = {Des choses cachées depuis la fondation du monde},
	language = {fre},
	number = {40001},
	publisher = {Grasset},
	author = {Girard, René},
	collaborator = {Oughourlian, Jean-Michel and Fort, Guy},
	year = {2009},
	note = {OCLC: 838516638},
	keywords = {Anthropologie, Philosophie},
}

@book{montesquieu_esprit_2005,
	address = {Paris},
	edition = {Nachdr.},
	series = {{GF} {Flammarion}},
	title = {De l'esprit des lois. 1},
	isbn = {978-2-08-070325-5},
	shorttitle = {De l'esprit des lois. 1},
	language = {fre},
	number = {325},
	publisher = {Flammarion},
	author = {Montesquieu, Charles Louis de Secondat de},
	editor = {Goldschmidt, Victor},
	year = {2005},
	note = {OCLC: 836678517},
	keywords = {Philosophie, Politique, Philosophie, XVIIIe},
}

@book{bloom_amour_2003,
	address = {Paris},
	title = {L'amour et l'amitié},
	isbn = {978-2-253-94352-5},
	language = {French},
	publisher = {Libr. Générale Francaise},
	author = {Bloom, Allan David},
	year = {2003},
	note = {OCLC: 177309401},
	keywords = {Austen, Jane, Essai, Flaubert, Gustave, Montaigne, Michel de, Romantisme, Rousseau, Jean-Jacques, Shakespeare, William, Stendhal, Tolstoï},
}

@book{bernard_introduction_2001,
	address = {Paris},
	series = {Champs},
	title = {Introduction à l'étude de la médecine expérimentale},
	isbn = {978-2-08-081137-0},
	language = {French},
	number = {137},
	publisher = {Flammarion},
	author = {Bernard, Claude},
	editor = {Dagognet, François},
	year = {2001},
	note = {OCLC: 716737977},
	keywords = {Biologie, Epistémologie, Médecine},
}

@book{gould_herisson_1994,
	address = {Paris},
	edition = {Le Livre de Poche},
	series = {Biblio {Essais}},
	title = {Un {Hérisson} dans la tempête: essais sur des livres et des idées/ {Stephan} {Jay} {Gould}. {Traduit} de l'américain par {Gilbert}-{Xavier} {Fayon} et {Aurélie} {Guillain}},
	isbn = {978-2-253-94227-6},
	shorttitle = {Un {Hérisson} dans la tempête},
	language = {fre},
	number = {4227},
	publisher = {Grasset},
	author = {Gould, Stephen Jay},
	translator = {Fayon, Gilbert-Xavier},
	year = {1994},
	note = {OCLC: 935164215},
	keywords = {Biologie, Epistémologie, Evolution (Biology)},
}

@book{behaghel_lire_2012,
	series = {Repères},
	title = {Lire l'économétrie},
	isbn = {978-2-7071-7311-9},
	abstract = {Résumé en 4ème de couverture: "Discipline au croisement de l'économie et de la statistique, l'économétrie reste pour beaucoup mystérieuse, alors qu'elle est de plus en plus souvent invoquée dans les débats politiques. L'objectif de ce guide est de rendre la lecture des textes économétriques accessible à des non-spécialistes. Le parcours, inspiré d'un enseignement auprès d'étudiants non économistes, comporte trois entrées. L'entrée par l'histoire montre les controverses suscitées par la discipline: l'économétrie permet-elle, conformément à son projet initial, d'expliquer les phénomènes économiques, de tester les théories et de faire des prédictions? L'entrée par la méthodologie présente les principaux outils en privilégiant l'intuition sur la technique. Enfin et surtout, l'entrée par la lecture guidée d'articles scientifiques illustre l'apport de l'économétrie dans deux débats contemporains: la générosité du RMI est-elle une des causes du chômage? Les baisses de charges patronales sur les bas salaires permettent-elles de créer davantage d'emplois ?"},
	language = {French},
	number = {460},
	author = {Behaghel, Luc},
	year = {2012},
	note = {OCLC: 801082492},
}

@book{valery_oeuvres_1957,
	edition = {Gallimard},
	series = {Bibliothèque de la {Pléiade}},
	title = {Oeuvres},
	volume = {1},
	language = {fre},
	number = {127},
	author = {Valéry, Paul},
	year = {1957},
	keywords = {Essai, Pléiade, Poésie},
}

@book{foucault_hermeneutique_2001,
	address = {Paris},
	series = {Hautes études},
	title = {L'herméneutique du sujet: cours au {Collège} de {France}, 1981-1982},
	isbn = {978-2-02-030800-7},
	shorttitle = {L'herméneutique du sujet},
	publisher = {Gallimard : Seuil},
	author = {Foucault, Michel},
	editor = {Ewald, François and Fontana, Alessandro and Gros, Frédéric},
	collaborator = {Collège de France},
	year = {2001},
	keywords = {History, Philosophy, Ancient, Self (Philosophy)},
}

@book{sober_philosophy_2000,
	address = {Boulder, Colo},
	edition = {2nd ed},
	series = {Dimensions of philosophy series},
	title = {Philosophy of biology},
	isbn = {978-0-8133-9126-7},
	publisher = {Westview Press},
	author = {Sober, Elliott},
	year = {2000},
	keywords = {Creationism, Evolution (Biology), Natural selection, Philosophy, Religious aspects Christianity},
}

@book{brehier_histoire_2004,
	address = {Paris},
	edition = {Nouvelle éd},
	series = {Quadrige},
	title = {Histoire de la philosophie},
	isbn = {978-2-13-054396-1},
	language = {fre},
	publisher = {PUF},
	author = {Bréhier, Emile},
	year = {2004},
	note = {OCLC: 254532030},
	keywords = {Histoire, Philosophie},
}

@book{fanon_damnes_2010,
	address = {Paris},
	edition = {1. tirage, 9. [Nachdr.]},
	series = {La découverte poche},
	title = {Les damnés de la terre},
	isbn = {978-2-7071-4281-8},
	language = {fre},
	number = {134},
	publisher = {La Découverte},
	author = {Fanon, Frantz and Sartre, Jean-Paul and Cherki, Alice and Harbi, Mohammed},
	year = {2010},
	note = {OCLC: 838678586},
	keywords = {Colonalisme, Essai},
}

@book{michea_complexe_2011,
	address = {Paris},
	title = {Le complexe d'{Orphée}: la gauche, les gens ordinaires et la religion du progrès},
	isbn = {978-2-08-126047-4},
	shorttitle = {Le complexe d'{Orphée}},
	publisher = {Climats},
	author = {Michéa, Jean Claude},
	year = {2011},
	note = {OCLC: ocn769741720},
	keywords = {Capitalism, Liberalism, Progress},
}

@book{favier_guerre_1980,
	address = {Paris},
	title = {La {Guerre} de {Cent} ans},
	isbn = {978-2-7242-0912-9},
	language = {French},
	publisher = {France loisirs},
	author = {Favier, Jean},
	year = {1980},
	note = {OCLC: 461859557},
	keywords = {France, Guerre de cent ans, Histoire},
}

@book{levy-leblond_aux_1996,
	address = {Paris},
	series = {{NRF} essais},
	title = {Aux contraires: l'exercise de la pensée et la pratique de la science},
	isbn = {978-2-07-074534-0},
	shorttitle = {Aux contraires},
	publisher = {Gallimard},
	author = {Lévy-Leblond, Jean Marc},
	year = {1996},
	keywords = {Philosophy, Science},
}

@book{luo_three_1995,
	address = {Beijing},
	series = {Chinese classics},
	title = {Three kingdoms},
	isbn = {978-7-119-00590-4},
	language = {eng},
	publisher = {Foreign Languages Press},
	author = {Luo, Guanzhong},
	translator = {Roberts, Moss},
	year = {1995},
	keywords = {China, Fiction, History, Three kingdoms, 220-265},
}

@book{barthes_mythologies_2005,
	address = {Paris},
	edition = {Nachdr.},
	series = {Collection {Points} {Essais}},
	title = {Mythologies},
	isbn = {978-2-02-000585-2 978-2-02-002582-9},
	language = {fre},
	number = {10},
	publisher = {Éd. du Seuil},
	author = {Barthes, Roland},
	year = {2005},
	note = {OCLC: 255205039},
	keywords = {Essai},
}

@book{gernet_monde_2006,
	series = {Agora},
	title = {Le monde chinois - 2. {L}'époque moderne {Xe} siècle - {XIXe} siècle},
	volume = {2},
	isbn = {978-2-266-16133-6},
	language = {French},
	number = {302},
	publisher = {Pocket},
	author = {Gernet, Jacques},
	year = {2006},
	note = {OCLC: 979418356},
	keywords = {Histoire, Chine},
}

@book{paules_republique_2019,
	address = {Paris},
	series = {Histoire},
	title = {La {République} de {Chine}: histoire générale de la {Chine} (1912-1949)},
	isbn = {978-2-251-44945-6},
	shorttitle = {La {République} de {Chine}},
	abstract = {"En 1912, avec la proclamation de la République, la Chine bascule dans une ère foncièrement nouvelle. La révolution de 1911-1912 n'a pas pour seule conséquence de renverser les Qing. Elle vient mettre fin à la succession des dynasties qui scandait l'histoire chinoise depuis plus de deux millénaires. Le modèle politique et intellectuel de l'empire, qu'une décennie de réformes radicales (les Nouvelles politiques) avait commencé à remettre en cause à partir de 1901, est définitivement enterré. En se fondant sur l'historiographie la plus récente, l'auteur réfute la grille de lecture d'une période républicaine dominée par l'épopée révolutionnaire du Parti communiste chinois. La victoire finale de ce dernier ne se dessine que très tardivement. Elle doit au moins autant à sa stratégie et sa faculté à organiser et mobiliser la population qu'à une série de concours de circonstances particulièrement favorables qui aboutissent à affaiblir son principal ennemi, le Guomindang."--Page 4 of cover},
	number = {146},
	publisher = {Les Belles Lettres},
	author = {Paulès, Xavier and Chaussende, Damien},
	year = {2019},
	keywords = {China, Histoire, Chine, History, Republic, 1912-1949},
}

@book{didier_en_2009,
	address = {Paris},
	series = {Textes à l'appui. {Série} "{Anthropologie} des sciences et des techniques"},
	title = {En quoi consiste l'{Amérique}? les statistiques, le {New} {Deal} et la démocratie},
	isbn = {978-2-7071-5708-9},
	shorttitle = {En quoi consiste l'{Amérique}?},
	urldate = {2020-07-02},
	publisher = {Découverte},
	author = {Didier, Emmanuel},
	year = {2009},
	keywords = {1918-1945, 1929, Depressions, Economic conditions, Histoire, statistiques, New Deal, 1933-1939, Statistiques, Sondages, US political and economic history 20th century New Deal, United States},
}

@book{lefranc_juin_1966,
	address = {Paris},
	series = {Archives},
	title = {Juin 36 "l'explosion sociale" du {Front} {Populaire}},
	language = {fre},
	number = {22},
	publisher = {Julliard},
	editor = {Lefranc, Georges},
	year = {1966},
}

@book{soboul_proces_1973,
	address = {Paris},
	series = {Archives},
	title = {Le proces de {Louis} {XVI}},
	isbn = {978-2-07-028900-4},
	language = {French},
	number = {19},
	publisher = {Gallimard},
	editor = {Soboul, Albert},
	year = {1973},
	note = {OCLC: 931298442},
	keywords = {Histoire, Histoire, Louis XVI, Histoire, Procès, Histoire, Révolution Française, Soboul, Albert},
}

@book{compagnon_ete_2013,
	address = {Paris},
	title = {Un été avec {Montaigne}},
	isbn = {978-2-84990-244-8},
	publisher = {Éditions des Équateurs},
	author = {Compagnon, Antoine},
	year = {2013},
	keywords = {Criticism and interpretation, Montaigne, Michel de},
}

@book{noauthor_france_2013,
	address = {Paris},
	title = {France 2014, les données clés},
	isbn = {978-2-11-009434-6},
	language = {French},
	publisher = {La Documentation française, Direction de l'information légale et administrative},
	year = {2013},
	note = {OCLC: 863262673},
	keywords = {Economie},
}

@book{ardant_histoire_1976,
	address = {Paris},
	edition = {Gallimard},
	series = {Idées},
	title = {Histoire financière de l'antiquité à nos jours},
	language = {fre},
	number = {364},
	author = {Ardant, Gabriel},
	year = {1976},
	keywords = {Histoire, Histoire, économie},
}

@book{cohen_prosperite_2013,
	title = {La prospérité du vice: une introduction (inquiète) à l'économie},
	isbn = {978-2-253-15965-0},
	shorttitle = {La prospérité du vice},
	language = {French},
	author = {Cohen, Daniel},
	year = {2013},
	note = {OCLC: 1040987161},
	keywords = {Economie, Essai},
}

@book{kapuscinski_ebene_2000,
	address = {Paris},
	title = {Ébène: aventures africaines /{cRyszard} {Kapuscinski} ; traduit du polonais par {Véronique} {Patte}.},
	isbn = {978-2-266-11458-5},
	shorttitle = {Ébène},
	language = {French},
	publisher = {Plon},
	author = {Kapuscinski, Ryszard},
	year = {2000},
	note = {OCLC: 424261336},
	keywords = {Afrique, Journalisme},
}

@misc{istituto_geografico_de_agostini_grand_1990,
	address = {Paris},
	title = {Le grand atlas du monde.},
	isbn = {978-2-7312-0861-0},
	language = {French},
	publisher = {Editions Atlas},
	author = {{Istituto geografico De Agostini} and {Editions Atlas}},
	year = {1990},
	note = {OCLC: 23527001},
	keywords = {Atlas},
}

@book{winckelmann_histoire_2005,
	address = {Paris},
	series = {Le livre de poche},
	title = {Histoire de l'art dans l'{Antiquité}},
	isbn = {978-2-253-13127-4},
	language = {fre},
	publisher = {Libr. Générale Française},
	author = {Winckelmann, Johann Joachim},
	editor = {Gallo, Daniela},
	year = {2005},
	note = {OCLC: 180905410},
	keywords = {Histoire de l'art, Histoire de l'art, Antiquité},
}

@misc{lacoste_geopolitique_2012,
	address = {Paris},
	title = {Géopolitique: la longue histoire d'aujourd'hui},
	isbn = {978-2-03-587653-9},
	shorttitle = {Géopolitique},
	language = {French},
	publisher = {Larousse},
	author = {Lacoste, Yves and {Larousse}},
	year = {2012},
	note = {OCLC: 910898657},
	keywords = {Atlas, Géopolitique},
}

@misc{du_atlas_2007,
	address = {Beijing},
	edition = {1st. ed},
	title = {Atlas of {China}},
	isbn = {978-7-5031-4178-2},
	publisher = {SinoMaps Press},
	author = {Du, Xiurong},
	year = {2007},
	note = {Num Pages: 1},
	keywords = {Administrative and political divisions, Atlases, China, Maps, Topographic maps},
}

@book{le_bras_peuplement_1996,
	address = {Paris},
	series = {Recherches},
	title = {Le peuplement de l'{Europe}},
	isbn = {978-2-11-003580-6},
	publisher = {Documentation française},
	author = {Le Bras, Hervé},
	year = {1996},
	keywords = {Europe, Géographie, Géographie, peuplement, Population},
}

@book{levy_comprendre_1979,
	address = {Paris},
	title = {Comprendre les statistiques},
	isbn = {978-2-02-005344-0},
	language = {French},
	publisher = {Éditions du Seuil},
	author = {Lévy, Michel},
	year = {1979},
	note = {OCLC: 462681412},
	keywords = {Statistics, Statistiques, Indices, Statistiques, Séries chronologiques},
}

@book{asanuma-brice_siecle_2019,
	title = {Un siècle de banlieue japonaise: au paroxysme de la société de consommation},
	isbn = {978-2-940563-43-2},
	shorttitle = {Un siècle de banlieue japonaise},
	language = {French},
	author = {Asanuma-Brice, Cécile},
	year = {2019},
	note = {OCLC: 1088661953},
	keywords = {Géographie, urbaine, Histoire, urbaine, Japon, Sociologie, urbaine, Urbanisation},
}

@book{charmes_ville_2011,
	address = {Paris},
	series = {La ville en débat},
	title = {La ville émiettée: essai sur la clubbisation de la vie urbaine},
	isbn = {978-2-13-058776-7},
	shorttitle = {La ville émiettée},
	publisher = {Presses universitaires de France},
	author = {Charmes, Éric},
	year = {2011},
	keywords = {City and town life, City planning, France, Sociologie, Sociology, Urban, Urban renewal, Urbanization},
}

@book{berque_raisons_1995,
	address = {Paris},
	title = {Les raisons du paysage: de la {Chine} antique aux environnements de synthèse},
	isbn = {978-2-85025-390-4},
	shorttitle = {Les raisons du paysage},
	publisher = {Hazan},
	author = {Berque, Augustin},
	year = {1995},
	keywords = {Géographie, Landscapes, Landscapes in art, Nature (Aesthetics), Paysages},
}

@book{castel_metamorphoses_2007,
	address = {Paris},
	edition = {Impr},
	series = {Collection {Folio}/{Essais}; 349},
	title = {Les métamorphoses de la question sociale: une chronique du salariat},
	isbn = {978-2-07-040994-5},
	shorttitle = {Les métamorphoses de la question sociale},
	language = {fre},
	publisher = {Gallimard},
	author = {Castel, Robert},
	year = {2007},
	note = {OCLC: 255674769},
	keywords = {Salariat, Sociologie, Travail},
}

@book{daston_objectivite_2012,
	address = {Dijon},
	series = {Fabula},
	title = {Objectivité},
	isbn = {978-2-84066-334-8},
	language = {fre},
	publisher = {Les Presses du réel},
	author = {Daston, Lorraine and Galison, Peter and Latour, Bruno},
	translator = {Quiniou, Hélène and Renaut, Sophie},
	year = {2012},
	keywords = {Epistémologie, Histoire des sciences},
}

@book{yang-drocourt_parlons_2007,
	address = {Paris},
	series = {Parlons},
	title = {Parlons chinois},
	isbn = {978-2-296-04035-9},
	language = {fre},
	publisher = {L'Harmattan},
	author = {Yang-Drocourt, Zhitang},
	year = {2007},
	note = {OCLC: 254760046},
	keywords = {Chine, Langage},
}

@book{lao_pousse-pousse_1990,
	address = {Arles},
	title = {Le pousse-pousse},
	isbn = {978-2-87730-033-9},
	publisher = {Picquier},
	author = {Lao, She and Cheng, François},
	year = {1990},
	note = {OCLC: 246539899},
	keywords = {Chine, Roman},
}

@book{hawking_breve_2018,
	title = {Une brève histoire du temps: du {Big} {Bang} aux trous noirs : essai},
	isbn = {978-2-290-00645-0},
	shorttitle = {Une brève histoire du temps},
	language = {French},
	author = {Hawking, Stephen William},
	year = {2018},
	note = {OCLC: 1043310860},
	keywords = {Physique},
}

@book{hosmer_applied_2000,
	address = {New York},
	edition = {2nd ed},
	series = {Wiley series in probability and statistics},
	title = {Applied logistic regression},
	isbn = {978-0-471-35632-5},
	publisher = {Wiley},
	author = {Hosmer, David W. and Lemeshow, Stanley},
	year = {2000},
	keywords = {Regression analysis, Statistics},
}

@book{angrist_mostly_2009,
	address = {Princeton},
	title = {Mostly harmless econometrics: an empiricist's companion},
	isbn = {978-0-691-12034-8 978-0-691-12035-5},
	shorttitle = {Mostly harmless econometrics},
	publisher = {Princeton University Press},
	author = {Angrist, Joshua David and Pischke, Jörn-Steffen},
	year = {2009},
	note = {OCLC: ocn231586808},
	keywords = {Econometrics, Regression analysis},
}

@book{tibshirani_elements_2017,
	edition = {2nd edition (version corrigée n°12)},
	title = {The {Elements} of {Statistical} {Learning}},
	url = {https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf},
	language = {en},
	publisher = {Springer},
	author = {Tibshirani, Robert and Friedman, Jerome and Hastie, Trevor},
	month = jan,
	year = {2017},
	keywords = {Apprentissage statistique, R (Computer program language), Statistical learning},
}

@book{pinheiro_mixed-effects_2000,
	address = {New York},
	series = {Statistics and computing},
	title = {Mixed-effects models in {S} and {S}-{PLUS}},
	isbn = {978-0-387-98957-0},
	publisher = {Springer},
	author = {Pinheiro, José C. and Bates, Douglas M.},
	year = {2000},
	keywords = {effets mixtes, multilevel analysis},
}

@article{hanley_students_2008,
	title = {Student's \textit{z} , \textit{t} , and \textit{s}: {What} if {Gosset} had {R}?},
	volume = {62},
	issn = {0003-1305, 1537-2731},
	shorttitle = {Student's \textit{z} , \textit{t} , and \textit{s}},
	url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X269602},
	doi = {10.1198/000313008X269602},
	language = {en},
	number = {1},
	urldate = {2020-06-25},
	journal = {The American Statistician},
	author = {Hanley, James A and Julien, Marilyse and Moodie, Erica E. M},
	month = feb,
	year = {2008},
	keywords = {R (Computer program language), Student, Tests},
	pages = {64--69},
}

@misc{noauthor_modesemi-parametriques_nodate,
	title = {Modèles semi-paramétriques de survie en temps continu sous {R} - {Documents} de travail - {M2018}/02 {\textbar} {Insee}},
	url = {https://insee.fr/fr/statistiques/3695681},
	urldate = {2020-06-25},
	keywords = {Modèle de survie, R (Computer program language), Survival analysis},
}

@book{davison_bootstrap_1997,
	address = {Cambridge ; New York, NY, USA},
	title = {Bootstrap methods and their application},
	isbn = {978-0-521-57391-7 978-0-521-57471-6},
	publisher = {Cambridge University Press},
	author = {Davison, A. C. and Hinkley, D. V.},
	year = {1997},
	keywords = {Bootstrap (Statistics)},
}

@book{taeger_statistical_2014,
	address = {Chichester, West Sussex},
	title = {Statistical hypothesis testing with {SAS} and {R}},
	isbn = {978-1-119-95021-9},
	publisher = {Wiley},
	author = {Taeger, Dirk and Kuhnt, Sonja},
	year = {2014},
	keywords = {MATHEMATICS / Probability \& Statistics / General, R (Computer program language), SAS (Computer program language), Statistical hypothesis testing, Tests},
}

@book{hosmer_applied_2000-1,
	address = {New York},
	edition = {2nd ed},
	series = {Wiley series in probability and statistics},
	title = {Applied logistic regression},
	isbn = {978-0-471-35632-5},
	publisher = {Wiley},
	author = {Hosmer, David W. and Lemeshow, Stanley},
	year = {2000},
	keywords = {Regression analysis, Regression logistique},
}

@misc{noauthor_analyse_nodate,
	title = {Analyse multi-niveaux en sciences sociales on {JSTOR}},
	url = {https://www.jstor.org/preview-page/10.2307/1534616?seq=1},
	abstract = {L'approche multi-niveaux permet d'aborder les comportements humains, en tenant compte non seulement des caractéristiques individuelles, mais également d...},
	language = {en},
	urldate = {2020-06-25},
	note = {Library Catalog: www.jstor.org},
	keywords = {multilevel analysis, sciences sociales},
}

@book{angrist_mostly_2009-1,
	address = {Princeton},
	title = {Mostly harmless econometrics: an empiricist's companion},
	isbn = {978-0-691-12034-8 978-0-691-12035-5},
	shorttitle = {Mostly harmless econometrics},
	publisher = {Princeton University Press},
	author = {Angrist, Joshua David and Pischke, Jörn-Steffen},
	year = {2009},
	note = {OCLC: ocn231586808},
	keywords = {Econometrics, Regression analysis},
}

@misc{noauthor_hommage_nodate,
	title = {Hommage à {Jacques} {Dars} (1941-2010) - {Persée}},
	url = {https://www.persee.fr/doc/etchi_0755-5857_2011_num_30_1_952},
	urldate = {2020-06-25},
}
