---
title: Disclosure risks
subtitle: |
  **[Define, Assess, Decide]{.orange}**
order: 1
href: theorie/supports/arbitrer-risque-utilite.html
image: ../../images/theorie.png
slide-number: true
header: |
  [Back home](https://inseefrlab.github.io/formation_protection_donnees)
footer: |
  Disclosure risks
# uncomment for French presentations:
lang: en-US
# for blind readers:
slide-tone: false
# for @olevitt:
# chalkboard: # press the B key to toggle chalkboard
#   theme: whiteboard
# uncomment to use the multiplex mode:
#multiplex: true
format:
  onyxia-revealjs:
    number-sections: true
    number-depth: 1
    toc: true
    toc-depth: 1
    toc-title: Table of contents
tbl-cap-location: bottom
include-in-header: 
  text: |
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
  # pick the light mode (onyxia-revealjs) or the dark mode (onyxia-dark-revealjs)
  #onyxia-dark-revealjs:
controls: true
css: custom.css
# from: markdown+emoji
# listing:
#   id: sample-listings
#   contents: teaching
#   sort: "date desc"
#   type: table
bibliography: ../../references-SDC.bib
# csl: ../style.csl  # Décommentez besoin d'un style de biblio spécifique
---


# Introduction {.unnumbered}

## Context

We will place ourselves in the following context:

- A statistical institute collects individual data.
- It is assumed that it complies with the regulations in force regarding the protection of individual data (for example, the GDPR in Europe).
- The institute seeks to disseminate aggregated statistics or individual data files for statistical purposes.

## Problem Statement

::: {.callout-tip icon=true}

### Workshop Overall Problem Statement

How can statistical information be disseminated without harming the individuals or organizations from whom the information was collected?
:::

## Protecting: a challenge for official statistics

_The missions [of official statistics] do not depend solely on its ability to master the tools or methods necessary for producing quality information, but also on [__its ability to protect and guarantee the confidentiality of the data entrusted to it__]{style="color:green;"}. This protection is the condition for continuing to have access to this data._ [@redor_confidentialite_2023]

## In Search of a Definition

::: {.callout-note icon=false}

## A maximalist definition of disclosure...

T. Dalenius proposes in 1977 the following definition:

_"If the publication of the statistics $T(D)$ allows determining the value of confidential statistical data more precisely than would be possible without access to $T(D)$, then a disclosure has occurred."_ [@dalenius_privacy_1977].
:::

::: {.callout-important icon=false}

## ... impossible to hold

This definition does not take into account the auxiliary information already available
:::

::: {.callout-tip icon=false}

## An example of disclosure impossible to protect

Studies show a correlation between smoking and the occurrence of lung cancer $\Rightarrow$ if, we (an insurance company) know that an individual is a smoker, then we can attribute to them a significant risk of having cancer, information which will very likely cause harm to that person.

:::

## In Search of a Definition

Two realities must be taken into account:

- the user has **auxiliary information**.
- **statistical inference** is one of the means of knowledge provided by the publication of statistics.

$\Rightarrow$ **unconditional and total protection is not possible.**

## Need to Master Risks

Data protection is defined through:  

- mastery of disclosure risks
- and the need to continue disseminating statistical information.

$\Rightarrow$ **Need to arbitrate between the two components.**

# A Necessary Trade-off

## Arbitrate

Protecting confidential data means **arbitrating** between:

-   their **utility** for knowledge and public debate;

-   their **intrinsic risk**: any data disseminated can disclose information about an individual or a group of individuals.

::: {style="text-align:center;"}
![Arbitrate](img/trade_off_balance.png){width=40% fig-alt="Risk-Utility Trade-off" fig-align="center"}
:::

## Two Pitfalls

-   Not disseminating would make the public statistic useless.

-   Disseminating everything would make the public statistic dangerous.


::: {style="text-align:center;"}
![Arbitrer](img/trade_off_graph.png){width=80% fig-alt="Arbitrage Risque-Utilité" fig-align="center"}
:::

## Issues

- What are the disclosure risks?
- How to measure these risks?
- How to address them?

$\Rightarrow$ How to address them without *too much* deteriorating the statistical information?

## Main issue of arbitration:

::: {.callout-note icon=true}

### Finding a balance between protection and information

This involves achieving a **compromise** between minimizing the risks of disclosing confidential information and minimizing the loss of information due to data protection processes.
:::

::: {.callout-warning icon=true}

### No Magic Method 
There is no method that minimizes risk and information loss at the same time!
:::

## An example of arbitrage

The population census (RP) in France:

-   Specific publication decree;

-   Utility of RP data is very strong:
    -   a number of laws depend on it
    -   financial allocation to municipalities

-   Risk of using disseminated data against individuals is judged low:
    -   information not very prejudicial
    -   except for sensitive variables

## An example of assumed disclosure

INSEE disseminates municipal data even for very small municipalities:

[See the example of the municipality of Rochefourchat](https://www.insee.fr/fr/statistiques/2011101?geo=COM-26274):

- A municipality of \(2\) inhabitants
- We know their sex, their marital status, their age, a rough description of their housing, the occupancy status of their housing, their education level, their employment status, etc.

## The Risk/Utility Trade-off: a Paradigm

-   There is no such thing as zero risk

-   No method completely eliminates risk $\Rightarrow$

    -   Trade-offs are therefore inherent to data protection.

    -   This trade-off is considered **the paradigm** of the discipline.

## A Criticizable Paradigm

[Voir @cox_risk-utility_2011]

-   If it helps to know **how to think**...

-   ...it helps less to know **how to act**.

## In practice, another trade-off takes place

A **Costs/Benefits** trade-off is performed:

-   By the data producer: 
    -   What resources to deploy for what level of protection (time, financial means)?

-   By the attacker: 
    -   Is the hoped-for confidential information worth the resources necessary for its disclosure? 

$\Longrightarrow$ Implement methods that have a cost adapted to the identified risk and to the sensitivity of the data.

## Objective: define and measure

To arbitrate, one must be able to

-   Define the terms of the arbitration 

-   Measure the phenomena.


# Need for governance

## Data Governance

Before defining the risks:

- What are the dissemination objectives?
- Who accesses what with which rights?

::: {.callout-important}

### Objective
To better manage risks, become aware of the context in which they are likely to appear.

:::

## Securing Access

- Four main types of users can be distinguished:

  - **General public and public actors**: aggregated statistics
  - **Researchers**: detailed statistics or even individual data
  - **Public statistics study officers**: individual data without direct identifiers
  - **Data collection officers**: individual data with identifiers

## Adapt access based on user needs

  - **Minimization principle**: each user must access only the data strictly necessary for their missions.
  

## Adapt access based on user needs

  - **Access differentiation** : 
    - **general public and public actors**: free access without conditions.
    - **researchers**: access under strict conditions (contracts, public utility projects, secure working environment, etc.).
    - **study managers**: access to data on internal servers subject to prior authorization.
    - **data collectors**:  access to collection data for the sole survey on which they are working.
  

## Adapt access based on user needs

  - **Ensure monitoring and control of access**: each access must be able to be controlled and revoked if necessary.

## Adjust the Necessary Anonymization Level

The looser the access, the higher the anonymization must be able to be:  

  - **general public**: 
    - open access $\iff$ strong statistical protection

## Adjust the Necessary Anonymization Level

The looser the access, the higher the anonymization must be able to be:  

  - **researchers and study managers**: 
    - secure access $\iff$ weak statistical protection on the inputs
    - But need to process/verify the public outputs (**output checking**)

## Adjust the Necessary Anonymization Level

The looser the access, the higher the anonymization must be able to be:  

  - **data collectors**: 
    - very secure access $\iff$ no statistical protection of the data
    - But need to comply with regulations on the protection of individual data (GDPR in Europe).
## An example

::: {style="text-align:center;"}
![Adapt files to audience type (Source: INSEE)](img/types_fichiers_confidentialite.png){#fig-types-fichiers width=90% fig-alt="Adapt files to audiences" fig-align="center"}
:::

## Governance, Ethics, Trust

- Mastery of access security and the best possible data governance are not infallible shields.
- Strong **Ethics** of public statisticians.
- **Trust** in the various actors (researchers) to whom data access is granted.

## Hold Accountable

A necessary accountability:

- Public statisticians (in France, 1951 law on statistical secrecy)
- Researchers (Risk of contract revocation and consequences for the entire laboratory)




# A Multi-Step Approach

## Key Steps in the Data Protection Process

::: {style="font-size: 80%;"}
1.  Is it necessary to protect the data?

2.  What are the main characteristics and uses of the data?

3.  Definition and measurement of disclosure risks

4.  Selection of data protection methods

5.  Implementation of the methods

6.  Controls and documentation

:::

Source:  [@hundepool_handbook_2024]

## Step 1: Is it necessary to protect the data?

::: {style="font-size: 80%;"}
-   Analysis of the units considered and variables present in the microdata file; if they are not sensitive, there is no need to perform processing for data protection

-   What type of dissemination? (data tables, maps, microdata \...)
:::

## Step 2: Main Characteristics and Uses of the Data (I)

::: {style="font-size: 80%;"}
-   Analysis of the type and structure of the data to determine the variables / units that require protection

-   Analysis of the survey methodology

-   Definition of the institute's objectives: type of publication (PUF, MFR), dissemination policies, consistency between multiple simultaneous disseminations, consistency with what is already published
:::

## Step 2: Main Characteristics and Uses of the Data (II)

::: {style="font-size: 80%;"}
-   Analysis of user needs (priority variables, types of analyses that will be performed)

-   Analysis of the questionnaire for surveys (variables to remove / to include, what level of detail for structural indicators such as socio-demographic variables?)
:::

## Step 3: Definition and Measurement of Disclosure Risks

::: {style="font-size: 80%;"}
-   Identify the different possible scenarios leading to data disclosure

    -   Depending on the type of data considered (exhaustive data, surveys) 
    -   Depending on the target audience (researchers, decision-makers, general public)

-   Choose the disclosure risk measure(s)
-   Risk tolerance threshold to set
:::

## Step 4: Selection of Methods

::: {style="font-size: 80%;"}
-   Choose one / several protection method(s)

-   Compare the methods: risk level vs loss of utility
:::

::: {style="text-align:center;"}
![Risk-Utility Trade-off](img/trade_off_graph_smiley.png){#fig-trade-off-sm width=80% fig-alt="Risk-Utility Trade-off" fig-align="center"}
:::

## Step 5: Implementation of the methods

::: {style="font-size: 80%;"}
1.  Choose software

2.  Perform the measurement of disclosure risks

3.  Protect the data

4.  Quantification of information loss
:::

## Step 6: Controls and documentation

::: {style="font-size: 80%;"}
- **Process protection control**

    -   verify that the methods implemented have effectively reduced the disclosure risk to the level considered acceptable

- Preparation of a document summarizing the methods used and providing an overview of the information loss

    -   if possible, transmit it to the users of the published data
    -   may contain warnings about precautions to take when using an anonymized file
:::

# Define the risk

## The risk of disclosure

::: {.callout-note icon=true}

## General Definition

Risk of disclosing confidential information by publishing aggregated or individual data.
:::

## Four types of disclosure risk

-   Identity disclosure risk

-   Attribute disclosure risk

-   Disclosure risk by inference

-   Disclosure risk by differentiation

    -   by "nesting"

    -   by "cross-referencing"

## Risk of Identity Disclosure

::: {.callout-tip}

## Definition

Risk of recognizing a specific individual in the published data: an attacker can identify a unit from the publication.

:::

Examples: 

-   Certain variables such as name, address that identify *directly* individuals or households.

-   All people with very rare characteristics (e.g., very elderly people).

-   87% of the U.S. population is unique solely from ZIP code, gender, and date of birth [@sweeney_simple_2000].

## Remarks

- Direct identifiers (name, first name, address) are useful for collection but removed from statistical databases.

- Other variables have a strong re-identifying power (place of residence, age, gender, profession, education level, etc.).

- Re-identifying does not always allow obtaining more information about people.

## Risk of Attribute Disclosure

::: {.callout-tip}

## Definition

Risk of disclosing **sensitive information** about one or more individuals from the disseminated data.

:::

Examples:

-   Re-identification often leads to attribute disclosure.
-   It is possible to disclose an attribute about entire groups.

## Disclosure of a Group Attribute

 - Disclosure of sensitive information for an entire group of people.
 - Without necessarily needing to re-identify beforehand.



```{r}
#| label: tbl-div-attr
#| tbl-cap: An example of group attribute disclosure
#| tbl-cap-location: bottom
#| echo: false
library(kableExtra)

df <- data.frame(
  Femmes = c(30, 12),
  Hommes = c(40, 0)
) 
row.names(df) <- c("Diabétiques", "Non diabétiques")

df |> 
  kableExtra::kbl(
    booktabs = T,
    col.names = c("Femmes", "Hommes"),
    row.names = TRUE
  ) |>
  kableExtra::kable_styling(
    bootstrap_options = c("hover", "striped"),
    full_width = F
  )
  # kableExtra::kable_classic_2() 
```

## Risk of Disclosure by Inference

::: {.callout-tip}

### Definition

Risk of being able to deduce with high certainty sensitive information about individuals from published data.

:::

-   Strong correlation of information with a sensitive characteristic.
-   Very high proportion within a group.

```{r}
#| label: tbl-div-infer
#| tbl-cap: An example of attribute disclosure by inference
#| tbl-cap-location: bottom
#| echo: false
library(kableExtra)

df <- data.frame(
  Women = c(30, 12),
  Men = c(38, 2)
) 
row.names(df) <- c("Diabetic", "Non-diabetic")

df |> 
  kableExtra::kbl(
    booktabs = T,
    col.names = c("Women", "Men"),
    row.names = TRUE
  ) |>
  kableExtra::kable_styling(
    bootstrap_options = c("hover","striped"),
    full_width = F
  )
  # kableExtra::kable_classic_2() 
```

## Risk of Disclosure by Differentiation 

::: {.callout-tip}

### Definition
When aggregated information is disseminated for various cross-tabulations, it is sometimes possible to infer additional information by differentiating the various results.

:::

::: {style="font-size: 80%;"}

Examples:

- Marginal differentiation: consists of using the margins of tabulated data
- Differentiation by geographic nesting: consists of using aggregates disseminated over nested zonings (variant of the first)
- Differentiation by cross-tabulation: consists of using aggregates disseminated over imperfectly nested zonings

:::

## Differentiation by Overlap


::: {style="text-align:center;"}
![Example of differentiation by overlap in France between a department and a community of communes (EPCI)](img/diff_geo_recoupment1.png){#fig-diff-rec1 height=80% fig-alt="Example of differentiation by overlap" fig-align="center"}
:::

## Differentiation by Cross-Referencing


::: {style="text-align:center;"}
![Zoom on the differentiation problem](img/diff_geo_recoupement2.png){#fig-diff-rec2 height=80% fig-alt="Example of differentiation by cross-referencing" fig-align="center"}
:::


# Measuring the Risk

## Distinguishing Variables

In an individual dataset, we distinguish:

-   **Identifiers**: Variables that allow direct identification of an individual.

-   **Quasi-identifiers**: Variables that can lead to re-identifying an individual using auxiliary information.

-   **Sensitive variables**: Variables for which specific protection measures may be necessary.

-   Other variables

## Identifiers

-   Identifiers are removed very early in the production process to comply with data protection regulations.
  
- Thereafter, it will be assumed that all direct identifiers have been removed.

## Quasi-identifiers


-   For individual/household data: sex, age, place of residence, diploma, marital status, etc.

-   For company data: sector of activity, location of headquarters, etc.

-   List to be determined each time

-   What variables does an attacker already have?

## An example

```{r}
#| label: tbl-type-var
#| tbl-cap: Different types of variables
#| tbl-cap-location: bottom
#| echo: false
library(kableExtra)

df <- data.frame(
  Nom = c("Johan", "Jeanne", "Johnny","Jeannette"),
  Adresse = c("3 rue...", "11 bd...", "12 pl...", "8 rue..."),
  Commune = c("Paris", "Malakoff", "Pithiviers", "Belval"),
  Age = c(36, 41, 23, 85),
  Diplôme = c("Bac", "Bac+3", "Bac Pro", ""),
  Revenus = c(150000, 60000, 25000, 10000)
)

df |> 
  kableExtra::kbl(
    booktabs = T,
    # col.names = c("Femmes", "Hommes"),
    row.names = FALSE
  ) |>
  kableExtra::add_header_above(
    c("Identifiants" = 2, 
      "Quasi Id."=3, 
      "Sensible"=1), 
    align = "c"
  ) |>
  kableExtra::kable_styling(
    bootstrap_options = c("hover", "striped", "bordered"),
    full_width = F
  )
  # kableExtra::kable_classic_2() 
```

## Quantifying the risk (1): **k**-anonymity

::: {.callout-note}

### $k$-Anonymity [@sweeney_k-anonymity_2002]

A dataset is considered *k-anonymous* if the least frequent combination of modalities of the quasi-identifying variables includes at least *k* units.
:::

::: {style="font-size: 80%;"}

-   This measure ensures that all individuals are similar to at least $k-1$ others.

-   **Overall risk measure** that focuses on the individuals most at risk of re-identification.

-   The probability associated with the risk for an individual in the file of being re-identified is at minimum $1/k$

-   Choice of $k$ taking into account existing rules and/or risk-utility trade-offs.
  
:::

## The Envisioned Attack Scenario

::: {.callout-important}



The YAML, code block syntax, and callout structure have been preserved exactly as in the original French version.

### Characteristics of the Attack Scenario

**k-anonymity** protects data from a re-identification attack when the attacker has auxiliary information about the same individuals (at least one):

- If the intruder knows that a specific individual is in the dataset, k-anonymity is a **fair way to assess the risk of re-identification**.
- The attacker performs a **matching** between the two datasets.
- **Quasi-identifiers (QI)** serve as matching keys.
- The rarer an individual's characteristics are on the QI, the better the re-identification will be.
  - **The unique ones** on the QI: re-identification will be certain
  - The more common an individual is, the lower the probability of re-identification will be.

:::

## The Importance of the Script

::: {.callout-warning}

### An effectiveness that depends on the realism of the scenario

Ensuring a certain level of anonymity helps reduce the risk of re-identification, but:  

- If the attacker has more auxiliary information $\Rightarrow$ [**underestimation**]{style="color:red;"} of the risk with *$k$-anonymity*.
- Over time, the attacker may have more information. However, the scenario is set once $\Rightarrow$ [**underestimation**]{style="color:red;"} of the risk with *$k$-anonymity*.
- If the attacker has less auxiliary information $\Rightarrow$ [**overestimation**]{style="color:green;"} of the risk with *$k*-anonymity*.
- The quality of matches depends on many other factors (vintages, quality of variables, consistency of fields, etc.) $\Rightarrow$ [**overestimation**]{style="color:green;"} of the risk with *$k$-anonymity*.

:::

## An example

::: {#tbl-1-anonyme .table .table-striped .table-hover .table-bordered .table-sm}

   Id       Age     Gender     Disease
  ---- ----------- ------- -------------------------
  1    [45;55[   M          Diabetes
  2    [45;55[   M          Arterial hypertension
  3    [45;55[   F          Cancer
  4    [45;55[   F          Flu
  5    [70;75[   M          Diabetes
  6    [45;55[   M          Diabetes

  : Example of a 1-anonymous individual file, with age and gender considered as quasi-identifiers
:::

## Limits of $k$-anonymity

::: {style="font-size: 80%;"}

- Strong **dependence on the quality/truthfulness of the scenario**   
  $\Rightarrow$ choice of QIs is crucial.  
  - $k$-anonymity overestimates the risk if the attacker has less information than assumed 
  - $k$-anonymity underestimates the risk if the attacker has more information than assumed
  - Difficult to measure the plausibility of the attack.

- $k$-anonymity **does not take sampling weights into account**.
    $\Rightarrow$ Applied to a sample, it will overestimate the re-identification risk.
- Does not reduce risks of **disclosure of sensitive attributes**.
  
:::

## Quantifying the Risk (2): l-Diversity

::: {.callout-note}

### The l-diversity [@machanavajjhalaDiversityPrivacyAnonymity2007]

It ensures *sufficient diversity of the values* of a sensitive variable taken by individuals within the same combination of quasi-identifiers.

:::

-   Refinement of $k$-anonymity.

-   Protection against the disclosure of sensitive attributes.

-   Each group must contain at least $l$ different values of the studied sensitive variable (or $l$ of the most frequent values).
   
-   Choice of the target $l$ based on existing rules and/or a risk-utility trade-off.

## An example

<!-- Review example => make an aggregated table (frequencies on IQ and breakdown by modalities of the sensitive variable) -->

::: {#tbl-ex-l-divers .table .table-striped .table-hover .table-bordered .table-sm}

     Age        Sex     Disease
  ----------- ------- ---------
   [50;55[       M      Diabetes
   [50;55[       M      Diabetes
   [50;55[       F       Cancer
   [50;55[       F       Flu
   [50;55[       M      Diabetes

  : A 2-anonymous file but not diversified enough
:::

## Envisaged Attack Scenarios

[@machanavajjhalaDiversityPrivacyAnonymity2007]

- Lack of homogeneity
- Use of auxiliary information

## Limits

- Suitable for categorical sensitive variables only
- => poorly suited for continuous variables (e.g. CA, standard of living, etc.)


# Probabilistic Measures of Risk

## Some notations for going further

::: {style="font-size: 70%;"}


- $N$: the population size
- $n$: the size of the possible sample
- $w_i$: the weight of individual $i$ in the sample ($\sum_i{w_i}=N$)
- $c$: the group formed by a type of crossing of the quasi-identifying variables.
  - For example, if the QIs are age and sex, $c$ may correspond to the crossing $[25; 35] \times$ Women, or $[55; 95] \times$ Men, etc.
- $N_c$: the number of individuals in the population sharing the characteristics $c$
- $n_c$: the number of individuals in the (possible) sample sharing the characteristics $c$


$\Rightarrow$ If a comprehensive file is $k$-anonymous, then $\forall c, N_c \geq k$.

:::

## Individuals most at risk of re-identification

-   **Unique in the population**: $N_c = 1$

-   **Unique in the sample**: $n_c = 1$

-   **Unique in the sample who are also unique in the population**: $n_c = 1$ et $\sum\limits_{i \in c}{w_i} = 1$

## Measuring individual risk in a comprehensive file

- **k**-anonymity and **l**-diversity are global measures.  
  
- We can move to the individual level:   
  
  - by considering as at risk each individual in a group of modalities with fewer than $k$ individuals
  - by associating with each individual in the database an **individual measure of re-identification risk**: 
  
  $$r_c = \frac{1}{N_c}$$

## Measuring individual risk in a sample

::: {.callout-note}

### Probability of re-identifying an individual in a sample

- *At the sample level*: 
  $$r_c = \frac{1}{n_c}$$

  - If the presence of an individual in the sample is known to the attacker, this is an adequate measure.
  - Otherwise, this measure overestimates the re-identification risk.

- *At the population level*, the re-identification risk can be estimated by:

 $$\hat r_c = \frac{1}{\sum\limits_{i \in c}{w_i}}$$

 where $\sum\limits_{i \in c}{w_i}$ is an estimate of $\hat N_c$.

:::

## A global measure of risk

To perform the arbitrage, a global risk measure is more practical. One can consider:

- **The number of uniques** (in the sample or the population, depending on the case);
- **The average individual risk** defined, in the context of a sample, by:
    $$\tau = \frac{\sum\limits_{c}{n_c \times r_c}}{n}$$

## Probabilistic Risk Measures 

More refined measures are implemented in classic tools such as $\mu$-Argus or the `R` package `sdcMicro`.

-   When a sample is available (thus the $n_k$), the $N_k$ are generally unknown.

-   Measure of individual risk conditional on the sample: $r_k = \mathbb{E}(\frac{1}{N_k}|n_k)$.

-   Measure depending on a modeling of the (posterior) distribution of $N_k | n_k$

    -   Modeling of key frequencies in the population conditional on their frequency in the sample.

    -   By a negative binomial, for example, in @Benedetti_Franconi_1998.

# Other Measures

## Record Linkage

  -   *A posteriori* measure of the distance between individuals in the protected dataset and those in the original dataset.

  -   Allows evaluating the number of exact matches between perturbed data and original data.

## Outliers

  -   High risk of re-identification of individuals with values in the tail of the distribution (e.g., very high incomes of footballers)

  -   A perturbation is not always sufficient (A perturbed outlier often remains an outlier).

  -   Detection of outliers based on the quantiles of the distribution.

# Attack Scenarios

## Define an Attack Scenario

::: {.callout-tip}

### Attack Scenario
Defining attack scenarios involves considering the means used by the attacker and objectifying the fraudulent uses that we seek to prevent.
:::

-   **Cost/Risk Trade-off (INS)**

-   **Cost/Benefit Trade-off (Attacker)**

## Attacks on k-anonymous and/or l-diverse data

@cohen_attacks_2022


# Conclusion

## In conclusion {.smaller}

-   There are many ways to evaluate information loss.

-   Strongly linked to the level of protection.

-   Many methods to evaluate information loss; the choice of measure depends entirely on the end users of the published data.

-   Difficult to anticipate all uses of a dataset and therefore all associated measures of information loss.

-   Need to make trade-offs on certain characteristics of a table to relax constraints elsewhere.

-   It is impossible to preserve all characteristics of a dataset.


# For further reading

## Bibliography 
::: {#refs}
:::
